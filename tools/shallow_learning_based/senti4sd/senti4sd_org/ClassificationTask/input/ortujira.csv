Jimmy this ready for review/commit? If so please 'submit'. Thanks.
I'm backporting this to 3.4 and 3.3 branches. --Michi
+1 overall. Here are the results of testing the latest attachment http://issues.apache.org/jira/secure/attachment/12508207/ZOOKEEPER-1263.patch against trunk revision 1214571. +1 @author. The patch does not contain any @author tags. +1 tests included. The
Bug Flavio more? Seriously
Integrated in ZooKeeper-trunk #1266 (See https://builds.apache.org/job/ZooKeeper-trunk/1266/) ZOOKEEPER-1104. CLONE - In QuorumTest
The patch no longer applies. Abmar
Integrated in ZooKeeper-trunk #271 (See http://hudson.zones.apache.org/hudson/job/ZooKeeper-trunk/271/)
3.0.0 has been released
Edited these issues to set the fix version.
Integrated in Hadoop-Mapreduce-trunk #1386 (See https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1386/) YARN-209. Fix CapacityScheduler to trigger application-activation when the cluster capacity changes. Contributed by Zhijie Shen. (Revision 1461773) Result = SUCCESS vinodkv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1461773 Files : /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRM.java /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java
Thanks for the patch
Changed MULTI_TEXTVALUE_SEPARATOR into a property configurable by injection of constant named xwork.validatorfileparser.multi_textvalue_separator. This makes it possible to have empty String as 'separator' if it apllies to your particular platform
That is my understanding. Looking at the dtd
No reply
Updated in 1.2 branch. David
As Wing Yew said
See comments above
Patch Applied. Good Work Adrian!
After re-compiling the applcation and re-deploying everything worked properly
I mailed jsr-206-comments@jcp.org and got the following response from Jeff Suttor: 'hi Daniel
reg_name has been accepted as valid URI syntax since Xerces 2.6.0. This may change in the future if XML Schema 1.0 moves up to the RFC 3986 syntax which excludes this production. Xerces CVS currently supports the XML Schema 1.0
Venu committed your patch. It's in Xerces 2.6.0. Please verify.
Venu committed your patch for Xerces 2.6.0. Please verify.
All of the components in the configuration get reset before parsing begins. This gives each component an opportunity to read features and properties from the configuration
This bug is 3 years old. Anybody looking to fix it?
The problem is not in the SAX parser. The problem occurs if you give the -server flag to the jvm (jdk 1.4/1.3) e.g. java -server <sax parser class> on a really big file. Will cause a stackOverflow. Greg Meagher
See bug 5694 for the DOMException problem.
Reporter took back the issue.
Hi George
As a work around
I verified the fix using the xml-xerces_20040107172932.tar.gz CVS snapshot. Thanks!
patch in cvs. Could you please verify?
Verified
*** Bug 9236 has been marked as a duplicate of this bug. ***
*** Bug 8704 has been marked as a duplicate of this bug. ***
I tried using your testcase on Win95/IE4
I think you want the XMLBEANS project
checked in
Added 'onRowInserted' callback to TreeTable
This appears to be a duplicate of XALANJ-2301. That bug report contains a one-line patch (supplied inline in the text of the bug report).
closing this issue.
Xalan-Java appears to be operating correctly.
I checked that this bug was fixed in version 2.5.2.
First I want to verify that Gretchen's fix >Additional Comments From Gretchen Chiaramonte 2003-09-22 20:04 ------- for this bug works for me as well using Xalan 2.6.0. Second
This bug has been fixed in Xalan Java 2.5.2. Please verify.
Closing this issue. Please re-open if you have a testcase.
I think with the latest code
I wasn't able to reproduce the problem with the attachments. I saw appropriate error messages
Retest shows Running xalan on lre02 <?xml version='1.0' encoding='UTF-8'?> <out xmlns='www.lotus.com' xmlns:ped='http://tester.com' english='to leave'/> Running XSLTC with Xerces Parser on lre02 <?xml version='1.0' encoding='UTF-8' ?> <out xmlns:ped='http://tester.com' xmlns='www.lotus.com' english='to leave'/>
Original reporter is not available. This report has now 'timed out'.
*** This bug has been marked as a duplicate of 6155 ***
Still happens to me. Steps to duplicate: 1. Start IE5.5 on Win2k - open java console from the 'view' menu. 2. Download xalan-j_2_2_D11.tar.gz to server. 3. tar xzf xalan-j_2_2_D11.tar.gz (extract to location on a webserver - for me this was /home/httpd/html) 4. navigate to http://SERVER/xalan- j_2_2_D11/samples/AppletXMLtoHTML/appletXMLtoHTML.html
AFAIK
Unfortunatly
Very cool. Thank you.
You are mixing runtimes. You must link _all_ executables with the same run- time library. For release
Integrated in Struts2 #551 (See https://builds.apache.org/job/Struts2/551/) WW-3914 solves problem with returning always system implementation of FileManager (Revision 1405930) Result = SUCCESS lukaszlenart : Files : /struts/struts2/trunk/xwork-core/src/main/java/com/opensymphony/xwork2/util/fs/DefaultFileManagerFactory.java /struts/struts2/trunk/xwork-core/src/test/java/com/opensymphony/xwork2/util/fs/DefaultFileManagerFactoryTest.java
Integrated in Struts2 #545 (See https://builds.apache.org/job/Struts2/545/) WW-3881 allows new html input types (Revision 1397712) Result = SUCCESS lukaszlenart : Files : /struts/struts2/trunk/core/src/site/resources/tags/ajax/autocompleter.html
Already resolved with WW-3414
I am also having the same issue with a java.sql.Timestamp based attribute. The date I entered into the field was 'not a date'. Of course
> Would it have an impact to remove this 'name' attribute from the generated 'form' tag ? YES! If you need XHTML strict compliance
Assigning this to future until the dependent XWork enhancement is made.
Integrated in Struts2-JDK6 #573 (See https://builds.apache.org/job/Struts2-JDK6/573/) WW-3928 Rollbacks changes introduced with WW-2749 to allow work with mod_jk and so on (Revision 1415770) Result = SUCCESS lukaszlenart : Files : /struts/struts2/trunk/plugins/tiles/src/main/java/org/apache/struts2/tiles/StrutsTilesRequestContext.java
Ported the solution of WW-2551 to the Struts 2.0 branch.
Resolving this issue as the original problem has been fixed. Please open a new ticket if you feel session invalidation should also be addressed.
This is by design
we're moving away from dojo integration in struts2
yay! it should be really really easy (not including the test
This has been committed in XWork and will be part of the XWork 2.1 release. See: http://jira.opensymphony.com/browse/XW-541
Reporter asked for this to be closed.
Makes sense to me.
duplicate of WW-1598
Hellow
As of r438174
I believe this is an issue. I have an action saveAction which I want to chain to editAction whenever there is a validation error. This is often necessary because a JSP will be displayed which has dynamic content such as a selectable list of items. However
This has been fixed in CVS. You can get the latest xwork.jar (which fixes the problem) from http://ivyrep.opensymphony.com
Given the dependencies
You are only flushing the writer
This was fixed by having all the items in the action chain having their properties copied over to the new action.
Property editors may throw a PropertyException with a bundle key and this will then be localized in ActionSupport.
Thanks. The problem is with deferred node expansion
I'll update the javadoc accordingly.
Marking this issue as resolved. Reasons: - Not reproducible. - No feedback from reporter. - If there was a dependency issue
Verified. I added the info to the RELEASE_NOTES file.
Verified. I added the info to the RELEASE_NOTES file.
Fixed.
r563181 pom.xml
r499213. The API changed as described in previous comments
This was probably fixed by WODEN-86 which introduced support for the curly brace syntax in the http location template. This JIRA can now be closed.
Resolved with no further action
Issue is resolved. Closing per Bryant's approval to do so.
Ah
Thanks Sven!
Attached QuickStart. Access URL is https://localhost:8443/test/ajaxtest - click on the buttons several times - refresh page: the counters should now reflect the clicks - click on the buttons more times - refresh page: the counters do not reflect click after first refresh
Just an idea: how about treating a jar as versioned resource? For example in my case that would work beautifully: my star rating component was in its own jar file
Bummer! -- The solution too this problem was that my JVM had too little memory (64MB) for allocating all the byte buffers required for writing / reading serialized pages.
Is there any documentation such as 'getting started with swarm' available? If there is
Fixed with r1097472.
See my comment in WICKET-3093.
By the way
Integrated in Apache Wicket 1.4.x #436 (See [https://hudson.apache.org/hudson/job/Apache%20Wicket%201.4.x/436/]) ?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤WICKET-3413 FLAG_INHERITABLE_MODEL and default model change reset 'inherited model' flag if model changed and a new one is not IComponentInheritedModel
Integrated in Apache Wicket 1.4.x #200 (See [https://hudson.apache.org/hudson/job/Apache%20Wicket%201.4.x/200/]) ?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤
Sorry
Integrated in Apache Wicket 1.4.x #58 (See [http://hudson.zones.apache.org/hudson/job/Apache%20Wicket%201.4.x/58/]) ?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤
this will be tackled when we improve ajax
1.3.x is no longer maintained outside of security-related issues
woops
Integrated in Apache Wicket 1.5.x #54 (See [http://hudson.zones.apache.org/hudson/job/Apache%20Wicket%201.5.x/54/]) ?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤
thanks
How about my comment at the end? 'Also
seems like all subtask as resolved
A RangeValidator exists by now which looks very similar to yours.
Fixed and I've tested it.
Fixed in trunk (r555005).
It is just resolved in 2.0 and the changes for 1.x have been committed at the same moment. The dot: you'll find that the xml parser is not a general purpose xml parser complying to all details of xml. It doesn't have to. It just implements what is sufficient for Wicket. If only if there is a real need to implement an additional detail
behvaiors:ajax calls is not always 1:1. some behaviors are purely clientside. also
Rolled back the change to Resource and included a FIXME that notes we don't cover all cases. But that doesn't lead to very serious problems anymore as it least the exception isn't thrown up in ComponentResourceRequestTarget anymore.
I just put this in branch-0.8 too. In fact
Closing since Whirr was updated to jclouds 1.5.0 in WHIRR-659.
Changed the site to include the incubator disclaimer and incubator logo.
Updated version of the patch. Removed the setter for the log storage provider in the stanza relay
Committed in rev 983396
Sorry for the link 'block'
patch applied.
Close all resolved issues for Engine 1.5 release.
Looks solid. I'm okay with resolving this one.
We'll look at the issue for 1.1
Closing as FIXED
Code delivered to SVN.
Change the element type of 'attributes' to FSAttribute.
Thanks for that comment Richard! I was laboring under the (false) impression that I could not change the docs that went with our previous releases
added fixVersion
documentation needs updating
Fixed a prompt that asks user to enter either 's' or 'q'.
applied patch
This issue is maybe related to UIMA-1408.
I think this should be fixed/changed for uima-base too. Is there a reason it wasn't?
To test this
Patch applied Eric.
Closing resolved issues
Fixed. Sorry folks
Dave has let me know that this has resoved the issue
Closing because this has been in RESOLVED state for over one year
Closing because this has been in RESOLVED state for over one year
Closing because this has been in RESOLVED state for over one year
This change is now in trunk as part of r679774.
Closing because this has been in RESOLVED state for over one year
Closing because this has been in RESOLVED state for over one year
This is now fixed!
I've just made some observations on the tuscany-user mailing list which should appear in the mailing list archives shortly
Assign to 1.0 during tidy up prior to 1.1
Closing because this has been in RESOLVED state for over one year
Here is a complete patch removing Composite* from the getImplementation() calls
Assign to 1.0 during tidy up prior to 1.1
reattaching with grant of license
Patch applied. Thanks!
Fixed in revision 406707.
Lazy session associated has been implemented in the TuscanyValve The spec people are still discussing Scope so I am going to close this and we can open a new issue when the spec clearly defines how scopes operate.
Right now ET_CALL
Moving these all to a 'Doc 3.x' release version.
as for V2.1.4 and later
Proposed patch.
Reviewed and tested. Looks good so far
UIXListView also provides selection functionality through 'selectedRowKeys'
This is on top of http://svn.apache.org/repos/asf/myfaces/trinidad/branches/2.0.0.1-branch
Patch over 1.2.12.3 branch.
This issue is already comitted
More done: -absolute-ordering faces-config/factory/view-declaration-language-factory faces-config/render-kit/renderer/client-behavior-renderer faces-config/application/partial-traversal
The call to launchDialog must supply a component for delivery of the returnListener event. This bug is invalid.
Attention: - Please refer https://issues.apache.org/jira/browse/TRINIDAD-1145 for a possible fix
The issue as described by (resolved) issue TRINIDAD-1171 occurs consistently in our environment using 1.2.10
Appled patch with some changes to compile with the new public rendering API.
in branch
home page fixed in svn.
I've re-worked my code and the problem has now 'gone away'. The original code
Reopening issues to set version.
Looks like a user issue to me
Thanks to Hazem Saleh for provide us this enhancement fixed at revision 655653
The above code is a part of a dataTable of course ...
I have fixed that issue by checking for a null before the line 95 of HtmlInputTextRenderer: writer.writeAttribute(HTML.VALUE_ATTR
Thanks to Simon Kitching.
add separate handling for webkit browser
I think you have enable ajax. f:loadBundle has only request scope. If you request the table sort the ajax response didn't know anything about the resource bundle. Please use tc:loadBundle it has session scope. But tc:loadBundle has a different semantic. Please consult the demo or other examples how to use it.
Closed due to the release of Tiles 2.1.1
Closed due to the release of Tiles 2.1.0.
As suggested above
TagSoup 1.2.1 is finally available
Actually I think I already applied similar changes when doing the POI 3.6 upgrade in TIKA-353. Resolving as fixed.
Integrated in Thrift #633 (See https://builds.apache.org/job/Thrift/633/) THRIFT-1879 Add support for GObject-Introspection (Revision a51186b7f2bd388d95485404017f05eb3b861074) Result = ABORTED roger : https://git-wip-us.apache.org/repos/asf?p=thrift.git&a=commit&h=a51186b7f2bd388d95485404017f05eb3b861074 Files : lib/c_glib/src/thrift/c_glib/transport/thrift_transport.h lib/c_glib/src/thrift/c_glib/server/thrift_simple_server.h lib/c_glib/src/thrift/c_glib/transport/thrift_buffered_transport.h lib/c_glib/src/thrift/c_glib/transport/thrift_framed_transport.h lib/c_glib/src/thrift/c_glib/protocol/thrift_protocol_factory.h lib/c_glib/src/thrift/c_glib/protocol/thrift_binary_protocol_factory.h lib/c_glib/src/thrift/c_glib/transport/thrift_server_socket.h lib/c_glib/src/thrift/c_glib/protocol/thrift_binary_protocol.h lib/c_glib/src/thrift/c_glib/server/thrift_server.h lib/c_glib/src/thrift/c_glib/transport/thrift_transport_factory.h lib/c_glib/src/thrift/c_glib/transport/thrift_server_transport.h lib/c_glib/src/thrift/c_glib/thrift_application_exception.h lib/c_glib/src/thrift/c_glib/transport/thrift_memory_buffer.h lib/c_glib/src/thrift/c_glib/transport/thrift_socket.h lib/c_glib/src/thrift/c_glib/protocol/thrift_protocol.h lib/c_glib/src/thrift/c_glib/thrift_struct.h
Integrated in Thrift #533 (See https://builds.apache.org/job/Thrift/533/) Thrift-1701:node.js TBufferedTransport buffer corruption Patch: Marshall Roch fix buffer copy method calls (Revision 1389517) Result = SUCCESS
Integrated in Thrift #407 (See https://builds.apache.org/job/Thrift/407/) THRIFT-1517 TTransport.ReadAll() should set exception type to EndOfFile Patch: Stefan Gmeiner (Revision 1291039) Result = SUCCESS roger : http://svn.apache.org/viewvc/?view=rev&rev=1291039 Files : /thrift/trunk/lib/csharp/src/Transport/TTransport.cs
Integrated in Thrift #426 (See https://builds.apache.org/job/Thrift/426/) THRIFT-1532/ THRIFT-1475 - fix record generation for erlang (Revision 1303663) Result = SUCCESS molinaro : http://svn.apache.org/viewvc/?view=rev&rev=1303663 Files : /thrift/trunk/compiler/cpp/src/generate/t_erl_generator.cc /thrift/trunk/lib/erl/Makefile.am /thrift/trunk/lib/erl/test/Thrift1475.thrift
Integrated in Thrift #230 (See https://builds.apache.org/job/Thrift/230/) THRIFT-1284. cpp: fix processor inheritance Don't make processors that have a proper parent class also inherit from TProcessor. Patch: Adam Simpkins bryanduxbury : http://svn.apache.org/viewvc/?view=rev&rev=1160933 Files : /thrift/trunk/compiler/cpp/src/generate/t_cpp_generator.cc
#NOME?
was introduced with THRIFT-1267
committed
Bryan
Second shot at this one
Ah
In r681863
Great! I'm glad you found it helpful.
Very curious as to where you saw that exception. Because digging through my log files
Duplicate of TAPESTRY-1988.
Thanks for the issue / steps to resolve. Pretty much did what you said verbatim. (as it worked out on the other regexp issue)
patches applied
Much cleared exception now .... org.apache.hivemind.ApplicationRuntimeException Component BadInjectComponent/border is not assignable to type org.apache.tapestry.html.Body. location: Annotation @org.apache.tapestry.annotations.InjectComponent(value=border) of public abstract org.apache.tapestry.html.Body pages.BadInjectComponent.getBorder() Stack Trace: ?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤* org.apache.tapestry.TapestryUtils.getComponent(TapestryUtils.java:301) ?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤* $BadInjectComponent_0.finishLoad($BadInjectComponent_0.java) ?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤* org.apache.tapestry.pageload.PageLoader.constructComponent(PageLoader.java:470) ?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤* org.apache.tapestry.pageload.PageLoader.loadPage(PageLoader.java:639) ?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤* $IPageLoader_1066542252f.loadPage($IPageLoader_1066542252f.java) ?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤* $IPageLoader_10665422530.loadPage($IPageLoader_10665422530.java) ?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤* org.apache.tapestry.pageload.PageSource.getPage(PageSource.java:118)
It's actually been in there (in jakarta.apache.org/tapestry/current and in CVS) for a while.
Fixed - thanks!
Does not occurs on Firefox or Curl. Probably a Chrome problem.
Integrated in tapestry-trunk-freestyle #515 (See [https://builds.apache.org/job/tapestry-trunk-freestyle/515/]) ?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤TAP5-1621 - Fixed test joshcanfield : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1167447 Files : * /tapestry/tapestry5/trunk/tapestry-ioc/src/test/java/org/apache/tapestry5/ioc/internal/services/TypeCoercerImplTest.java
Seems like this has all been done.
Thanks for the sample app
Integrated in Syncope-ROLE_PROVISIONING #4 (See [https://builds.apache.org/job/Syncope-ROLE_PROVISIONING/4/]) ?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤[SYNCOPE-173][SYNCOPE-219] (Additional) Moving some classes to new packages as preparation for role-related workflow classes (Revision 1408258) [SYNCOPE-173][SYNCOPE-219] Moving some classes to new packages as preparation for role-related workflow classes (Revision 1408249) ?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤?â¢?Ã¤Result = SUCCESS ilgrosso : Files : * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/init/ContentLoader.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/init/SpringContextInitializer.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/policy/AccountPolicyEnforcer.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/rest/controller/UserController.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/rest/controller/WorkflowController.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/sync/SyncopeSyncResultHanlder.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/resources/workflow.properties * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/resources/workflowContext.xml * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/test/resources/noopworkflow/workflow.properties * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/test/resources/noopworkflow/workflowContext.xml * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/test/resources/workflow.properties ilgrosso : Files : * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/init/ActivitiWorkflowLoader.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/init/SpringContextInitializer.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/policy/AccountPolicyEnforcer.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/rest/controller/UserController.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/rest/controller/WorkflowController.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/sync/SyncopeSyncResultHanlder.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/AbstractUserWorkflowAdapter.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/ActivitiUserWorkflowAdapter.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/NoOpUserWorkflowAdapter.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/UserWorkflowAdapter.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/activiti * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/role * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/user * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/user/AbstractUserWorkflowAdapter.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/user/NoOpUserWorkflowAdapter.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/user/UserWorkflowAdapter.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/user/activiti * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/user/activiti/ActivitiUserWorkflowAdapter.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/user/activiti/ActivitiWorkflowLoader.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/user/activiti/SyncopeEntitiesVariableType.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/user/activiti/SyncopeGroupManager.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/user/activiti/SyncopeGroupQueryImpl.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/user/activiti/SyncopeSession.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/user/activiti/SyncopeSessionFactory.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/user/activiti/SyncopeUserManager.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/user/activiti/SyncopeUserQueryImpl.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/user/activiti/task * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/user/activiti/task/AbstractActivitiDelegate.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/user/activiti/task/AutoActivate.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/user/activiti/task/Create.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/user/activiti/task/Delete.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/user/activiti/task/GenerateToken.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/user/activiti/task/Reactivate.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/user/activiti/task/Suspend.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/java/org/apache/syncope/core/workflow/user/activiti/task/Update.java * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/resources/userWorkflow.bpmn20.xml * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/resources/workflow.properties * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/main/resources/workflowContext.xml * /incubator/syncope/branches/DEV_ROLE_PROVISIONING/core/src/test/resources/workflow.properties
thanks Malith for your contribution...patch committed under rev. 1326522
Fixed in the trunk and branch
Thanks for the patch
Resolved in Version 1.2_QA_B1 . Therefore will be closing the issue
Fixed in both the current trunk and 1.2.x branch
Fixed.
Fixed in nightly build 20010612. Will be fixed in Struts 1.0 final.
Each instance in SQL Server gets its own port number. As long as the port number for the SQLExpress instance is accurate it should work okay.
Closing as fixed.
After the changes the error no longer appears in nightly build results. Closing as Fixed.
The tests have been added
Fixed with the referenced change.
Marking as Resolved in 4.2.0 ?Ãµ?Ã­ I couldn't reproduce it even there. Will close once we create a regression test
All third party repos are removed for released artifacts. The only exception is the metaxa engine which will be excluded from the release.
rules/
Build on Windows looks also okay but there are still other problem loading the model data for the entity hub.
Looks like STANBOL-258 describes the same Issue.
Fixed in revision 1073288
Example RequestDocumentor output for the StatelessEngineTest
Committing to https://svn.apache.org/repos/asf/mina/sshd/trunk ... M sshd-core/src/main/java/org/apache/sshd/client/UserAuth.java M sshd-core/src/main/java/org/apache/sshd/client/auth/UserAuthAgent.java M sshd-core/src/main/java/org/apache/sshd/client/auth/UserAuthPassword.java M sshd-core/src/main/java/org/apache/sshd/client/auth/UserAuthPublicKey.java M sshd-core/src/main/java/org/apache/sshd/client/session/ClientSessionImpl.java M sshd-core/src/main/java/org/apache/sshd/common/Session.java M sshd-core/src/main/java/org/apache/sshd/common/session/AbstractSession.java M sshd-core/src/main/java/org/apache/sshd/server/FileSystemFactory.java M sshd-core/src/main/java/org/apache/sshd/server/FileSystemView.java M sshd-core/src/main/java/org/apache/sshd/server/channel/ChannelSession.java M sshd-core/src/main/java/org/apache/sshd/server/command/ScpCommand.java M sshd-core/src/main/java/org/apache/sshd/server/filesystem/NativeFileSystemFactory.java M sshd-core/src/main/java/org/apache/sshd/server/filesystem/NativeFileSystemView.java M sshd-core/src/main/java/org/apache/sshd/server/session/ServerSession.java Committed r1039541
I haven't seen any bad behavior. I was using open ssh to test this. I used the escape character
The aborted build for hadoop 100 profile seems to be some sort of Jenkins issue.
Failure on profile 200 is expected and will be handled by SQOOP-731.
[branch_4x commit] Shalin Shekhar Mangar http://svn.apache.org/viewvc?view=revision&revision=1444786 SOLR-4426: NRTCachingDirectoryFactory does not initialize maxCachedMB and maxMergeSizeMB if <directoryFactory> is not present in solrconfig.xml
[branch_4x commit] Mark Robert Miller http://svn.apache.org/viewvc?view=revision&revision=1427873 SOLR-4254: Harden the 'leader requests replica to recover' code path.
The commit tag bot is neat!
Fix committed as part of SOLR-4134.
[branch_4x commit] Michael McCandless http://svn.apache.org/viewvc?view=revision&revision=1390137 SOLR-3879: don't ship servlet-api*.jar
committed to 4.x and 5.x
build.xml cleanup... Committed revision 1368286. - trunk Committed revision 1368287. - 4x
removing fixVersion=4.0 since there is no evidence that anyone is currently working on this issue. (this can certainly be revisited if volunteers step forward)
Hoss
I hope someone can fix this
I do have some interest in working on this
Thanks Alexey
The other precedences are OK
Variables specified in data config are also resolved with system properties so there is no reason anymore to configure DIH via solrconfig. Hence
bulk close for 3.4
bulk close for 3.4
@Noble Paul I cooked something resembling a backport here SOLR-3079
Bulk close for Solr 1.4
Ah
commited a while back
This bug was modified as part of a bulk update using the criteria... Marked 'Resolved' and 'Fixed' Had no 'Fix Version' versions Was listed in the CHANGES.txt for 1.3 as of today 2008-03-15 The Fix Version for all 29 issues found was set to 1.3
closing.
This bug was modified as part of a bulk update using the criteria... Marked ('Resolved' or 'Closed') and 'Fixed' Had no 'Fix Version' versions Was listed in the CHANGES.txt for 1.1 The Fix Version for all 38 issues found was set to 1.1
Thanks for the patch J?Ã¥_rgen. Revision 1028760.
The fix for SMXCOMP-759 also resolve this issue
Fixed in http://svn.apache.org/viewvc?view=revision&revision=1210845
please apply these two patches in the $SMX4.0/runtime/trunk & $SMX4.0/nmr/trunk respectively. As Guillaume's mentioned above: Define a deploymentListener in the filemonitor module. Define a JBI artifactDeployment Listener for dealing with JBI artifact
resolved
Very nice
Fixed. Sorry for long waiting. I didn't find the time. Author: tterm Date: Mon Jul 21 01:53:35 2008 New Revision: 678365 URL: http://svn.apache.org/viewvc?rev=678365&view=rev Log: Preserving or configuring content-type of http-header Modified: servicemix/smx3/branches/servicemix-3.2/deployables/bindingcomponents/servicemix-http/src/main/java/org/apache/servicemix/http/HttpEndpoint.java servicemix/smx3/branches/servicemix-3.2/deployables/bindingcomponents/servicemix-http/src/main/java/org/apache/servicemix/http/processors/ProviderProcessor.java
Committed to the 3.2 branch via revision 634707 and to the trunk (3.3) via revision 634714.
I forgot servicemix-soap on the 3.2 branch: http://svn.apache.org/viewvc?diff_format=h&view=rev&revision=657762 And also added it to the trunk http://svn.apache.org/viewvc?diff_format=h&view=rev&revision=657764 http://svn.apache.org/viewvc?diff_format=h&view=rev&revision=657765
Updated the binding components' parent pom.xml http://svn.apache.org/viewvc?view=rev&revision=557983
Please be informed that I am no longer accessible through this email address. Pls refrain from sending further messages. I will just personally contact you with my new email address. For any concerns / issues / inputs relating to your transactions with Exist
The fix currently transform the input source to a DOMSource. We could also handle differently a SAXSource.
I really meant to remove this functionality: What is the use for it anyway ? It doesn't hurt today (except for a few bytes eaten) but it may be confusing. So
If I see it correctly
Close after release
Linked the API docs from the main navigation which completes this task for now.
Additional fixes in Rev. 737663 and fixing the jcrapp module in Rev. 737664 completes this issue.
Tested this fix with two servlet containers (Jetty and Day Servlet Engine) as well as with Firefox 2 on Linux and Windows and Firefox 3 (Windows)
It seems that this enhancement is atm out of scope for Sling - therefore closing this bug.
Closing all resolved issues due to a successful 1.0.0-incubating release
resolved.
resolved -- changed DEFAULT_TTL to defaultTtl to reflect it not being a constant any more..
Applied patch from Jasvir. Also corrected case error in Enum type in Java
Awesome. I actually did find two more non-code files - the readme docs in java/social-api/ and have added them as deletes to my client. Good to know it checks out with you though. Unless anybody objects I'll submit this relatively soon.
Looks good
Applied in r648154. Thanks!
Closing this one since it doesn't fit in with the current caja integration anymore.
Chain 1.2 is now available in the m2 repo. Updated patches to include logging 1.1.1 and scxml 0.8
[[ Old comment
Ok
Hi Dave
Refactoring out common code into a parent processing class for ack request/close/terminate
merged into piper commit d5b96f571c6c43c9e42dfdc78fd906b6b5254e67
Cool. Thanks for considering my bug report!
This was done long ago and is now in trunk and ready for release
Closed issues related to Roller 5.0 release.
Closing resolved/won't fix issues.
Closed issues related to Roller 5.0 release.
Closing all resolved/fixed issues of already released versions of Roller.
The macro is also deprecated now
Closing resolved but not fixed (i.e.
Confirmed with Dave that this can be closed.
Also fixed in main 0.9.7 branch
Issue fixed.
Fixed
Thanks Marijan
Closing old resolved issues
Test log
Rejected for 0.20. Allow me to clarify my position. I was prepared to accept this for 0.20. Right or wrong
Now merged to the 0.20 release branch.
Committed to 0.18 branch r1366206
The above commits were approved by Justin on the dev list
Changes merged to the 0.16 branch
Looks like patch failed QA due to lack of tests. Are there existing tests for the shell? At a glance I couldn't find any.
This patch just removes the lines of code that send the second NEWLEADER quorum packet. The first packet is sent with: leaderLastZxid = leader.startForwarding(this updates)
Committed to trunk. Thanks Edward.
Rakesh  The patch looks good to me. Ill wait for hudson to check this in. We are good to go for 3.4 RC now! Thanks Rakesh!
Just committed this to trunk and 3.4 branch. Thanks Rakesh!
Sorry I meant ZOOKEEPER-1239.
Added license headers.
I just committed this. Thanks pat!
No this was on 3.3.3 (sorry I should have mentioned that before).  That's great to know thanks!
"""Vishal Kher to Benjamin Camille  9:38 AM (54 minutes ago)"
Trying again.
zoo_multi and zoo_amulti crash for the same reason.  Made some local fixes here.
I am not sure how the build system check for ant tar failed.  It appears to be working from the console output.
Thanks for the clarification. I will reintroduce separate data structure for LEADING/FOLLOWING notifications.
same path as ZOOKEEPER-794_4.patch.txt but strip the path prefix added by Git.
You're right I didn't check what the warning was about and assumed it was due to your patch. Resubmitting it.  And yes I'll create a jira for the findbugs warning.
Henry you do need an instance of QCM to get the listener but you don't need the listener because you're using LE.   I'm uploading a patch in which I have commented out some lines of the new test you wrote. This is how I think it should look like. If you agree please remove those lines and submit a new patch.  Note that I have also changed the leader election parameter in the new instance of QuorumPeer in mockServer to 0.
bq. 1)  Sorry didn't create a patch for the README since it was a small change it is included in this patch with the comment revision.
I just committed this. thanks steven!
Hi Benjamin. Did you actually take a look at the attached MANIFEST.MF? Are the import and export statements in there correct? What is in the XML above is just the maven-bundle-plugin/BND instructions to create the exact list by introspection :) If you think it would be better to explicitly list everything when creating the manifest please state what you would like to see in there.
Committed revision 777662.
I just committed this. thanks flavio.
I just committed this. Thanks pat.
Patch looks good to me. Reuploading patch with one corrected code-comment.
Looks good. +1. Checking this in.
+1 looks good. Will check this in once Jenkins gives its blessings.
Thanks Omkar!  I committed this to branch-0.23 as well.
Committed to trunk branch-2 and branch-2.1-beta. Thanks Jian.
Thanks [~ajisakaa].  +1 pending jenkins.
planning to fix this.. I am planning to remember completed containers (only id) at node manager for predefined time (10min). Does that time sounds reasonable or we should make it configurable? ... I don't really think adding a new configuration parameter will be a good idea but I am open for any different approach / adding conf .. thoughts? This will have a similar implementation like YARN-62 but only difference is that YARN-62 only tracks container for a time after it starts to avoid duplicate launch..where as this tries to avoid logging errors for valid stop attempts...
+1. Patch looks good. Thanks Jian and Zhijie Bikas for the reviews.
Fixed the minor nits. Thanks Sid!
Make patch against the latest trunk and add the comment suggested by Bikas.
I am perfectly fine with that.  It seems like more overhead but I am fine either way.
Why not use getSystemLoadAvg() to make it portable?
bq. I just committed this to trunk and branch-0.23. Thanks Derek! Also branch-2.
"""The patch looks good and so do the tests.  Thanks.  My only question is why were the following lines added to setup? {code} conf.set(YarnConfiguration.NM_LOCAL_DIRS """"file:///bin/echo"""")"""
Thanks Radim  I put this into trunk branch-2 and branch-0.23
Jaigak - Its nice to see another application on top of Yarn.  Since its a lot of new code could you please add some notes on the high level components and suggest some pointers on navigating the code. It will help review the changes. Thanks!
Looks good to me.
+1 23 patch changes look trivial.  Thanks!
Will review by EOD today. Thanks for the tip.
Thanks for the patch applied for 2.0.1/2.1
"""Gabe yes this (<param name=""""blocked"""">p1p2</param> ) would be the way I would expect it to work but perhaps we can add OGNL expression evaluation as well. This could be usefull in other cases as stated in XW-340.  What do you think?"""
Hi Jochen  It's not about building successfully (which I have no issue with that as well) it's about resolving the right dependencies of the dependency. Use mvn dependency:tree to see if you really have them all correctly if you are getting them right too:  mvn dependency:tree -Dxmlrpc.version=3.1.1  (Yeap apparently I uploaded the pom after testing it against 3.1 not 3.1.1)  yc
Closing issues which have been released.
Thanks John for bringing this up and also for providing the patch.
Oops sorry. Certainly right.
Hi Thiwanka  I assume you're referring to this code which checks the character offset and produces progress events if the XMLLocator has advanced at least 2K:      public void startElement (QName element XMLAttributes attributes Augmentations augs) throws XNIException {         if(locator.getCharacterOffset() == 0) {             reportProgressEvents()
The patches on XERCESJ-1365 supersede the ones attached here.
Thanks Wei. I've committed your latest patch to SVN.
Created an attachment (id=7238) 2nd repro.
Fixed in CVS.  (Indeed the fix was in CVS slightly before release publication
If and when Xerces moves to SAX 2.0.1 this patch will satisfy the behaviour of  SAX1 and SAX2.
Duplicate of XERCESC-1810
Hi Neil Thanks for the advice ...  > On the other hand if you can design a handler that knows how to > make appropriate calls to the scanner's sendChars() method so that > the buffer gets flushed when a maximum buffer size is reached then > perhaps a pluggable handler wouldn't be necessary since the default > behaviour would always work when an application has chosen to set > this limit.  The code I have as written does have the pluggable handler notion since I didn't want to arbitrarily couple the XMLBuffer class implementation (or worse it's interface) to the scanner. I didn't go so overboard as to allow multiple registered handlers or anything like that
Looks great thanks Tinny!
Created an attachment (id=1417) xercesc/framework/XMLValidator.hpp
As of 05/25/07 no error is thrown in IE7 when title bar is clicked in window.xal example.
On 2008/Apr/08 04:09 pm Brian Minchau wrote: > > Hi Helge. > > I'm afraid you can't assign someone to review.  However I will have a look > at your patch.  Any progress in reviewing my patch?  -- Helge Schulz - http://OpenSHORE.org
This task was done before the 2.7.1 branch was created.  This task is now resolved.
This bug is fixed in 2.7 closing it down per the request by the reporter Anuj Gupta.
Created an attachment (id=8670) javadoc patch to org.apache.xalan.lib and org.apache.xalan.lib.sql
Created an attachment (id=2473) Patch to MANIFEST.MF
Created an attachment (id=1333) xml/xsl testcase for normalize-space-bug
Created an attachment (id=356) XML parsing tool.
Created an attachment (id=228) XML source with one entry that can trigger error
I checked out the patch.  It looks good to me.
I just did an AMD Linux build and all I did was remove /usr/lib from the list  of libraries.  Since we don't really need /usr/lib at all don't you think a  better patch would be to remove that directory all together?
My oops then. I hadn't defined it I was assuming that it would be handled like the other special characters... ...sorry for the trouble.
John it would be great!
Thanks it works fine!
This bug was introduced by https://issues.apache.org/struts/browse/WW-2171
this looks good but a bunch of tests are now broken because of the new hidden input I will fix the tests do some more testing and commit in the next few days. thanks for the patch!
Did you set:  struts.devMode=false struts.configuration.xml.reload=false   This did the trick for me :)
Completed: At revision: 614842  -- Thanks Maja!
"""If you ment this  <s:div id=""""t4"""" showLoadingText=""""false""""> <s:form id=""""form"""" action=""""AjaxTest"""">   <input type=textbox name=""""data"""">   <s:submit type=""""button"""" theme=""""ajax"""" label=""""Update Content"""" targets=""""t4"""" id=""""ajaxbtn""""/> </s:form> </s:div>  it still doesn't post the form values in IE but works fine in FF  Thanks a lot for your quick response. I really really appreciate  -ravi      --  Ravi Mangalagiri www.bluepitch.org (My personal website. Coming soon!) 703-505-4240 (c) """
Patch applied thanks!
It still need a lot of work but there's an initial version checked into the sandbox now.    * http://svn.apache.org/viewvc/struts/sandbox/trunk/jpa-mailreader/  See STATUS.txt for design notes and such.
DWR validation is working fine for me with FF 1.5 but the submit action for lesson1 is still not working. IE 5.5/6.0 still throws some really disturbing error messages which I could not yet resolve.
"""I won't get to it so let's go ahead. In general we should push things to a """"2.0.x"""" version if we want to get it out in time for 2.0 GA."""
Make AMP constants refer to &amp
Lets fix this post wss4j-1.5.3
Reassign to fix in future release.
Sorry clicked the wrong button. Its intended for inclusion.
well if the user uploaded a file in the first FileUploadField then it should be handled by the first nested Form and if he uploaded a file in the second then it should be handled by the second and if both ... then both !  If only outer Form process all the submit then I understand that it can be difficult to implement what I expect are nested Form informed of the submition or not ? In fact FileUploadException is raised only for size limit exceeded can't we handle this thru an IFormValidator instead of a hardcoded check ?
A Quickstart for the bug as Eclipse/Maven Project. Sorry for the delay.
I didn't have time to work on this yet ...
"""Sorry forgot to mention this. Try to switch the tabs a second time. The first click on """"Tab B"""" will correctly render """"Content B"""". But switching back to """"Tab A"""" and then again back to """"Tab B"""" fails to render """"Content B""""."""
Thank you!
Fix applied thanks.
Ah great thanks!
thanks Attila. I fixed the test/header already
I see no gain to make it singleton (Scala object) too. No need to change it without reason.
Sorry I narrowed down the issue to Chrome's over-zealous caching. I was able to reproduce the problem on two separate computers in a simple project where I had upgraded from Wicket 1.4.0 to 1.4.15. But due to testing before and after Chrome had cached the buggy version of wicket-ajax.js from 1.4.0 and did not detect that it changed with the update. Clearing the browser cache resolved the issue.  Sorry about the bogus bug report.
this was released in 1.4.9 but is now being reverted in 1.4.10 due to overwhelming community feedback
thanks
model objects were not serializable
Quickstart to demonstrate that I can't reproduce this.
Sounds like a good idea but better to only fix in 1.5 to keep 1.4 more 1.3-compatible.
Builds fine for me and the code looks good. Great work! I did not run the integration tests.   [~abayer] what do you think?
This looks good to me. +1 pending beta-8.
You are right its the wrong one (an older attempt).
I agree. I'm planning to look into WHIRR-370 later this month but I can't provide a timeline for it.
Ok. I will do that. Adrian thanks for taking a look at this. :)
Forgot to grant license
+1 Looks good to me.
This patch adds some getting started documentation and also a new page about configuration. I've removed the individual pages for services in the absence of something to write there but we can add back as needed. I've also commented out some broken links that won't be live until we do a release.  I think we could make this live now. Thoughts?  BTW before applying the patch run:  svn mv site/src/site/confluence/getting-started.confluence site/src/site/confluence/quick-start-guide.confluence
Attached image showing blip padding in the Google Wave client.
Sorry forgot to mention it in issue description. I use commons-net-2.0.jar i.e. version 2.0.  It's because Scott Bjerstedt wrote in VFS-264 that for his patch to work we need to use commons-net 2.0.
"""It took me a while to figure out where the problem is coming from sorry.  So your way is not a bad way of doing it though let me suggest an alternative...  In AbstractFileObject match the constructor's """"fs.fileObjectHanded(this)"""
Hmm.  Breaking #if($query.foo) compatibility isn't ok if we can't find a reasonable way to keep that and the sub then i'll ditch the sub.   But i much prefer $query.foo.bar over $query.get('foo.bar') so i'm not going to give that up quickly.  The type conversion suffixes are more than just nice since the main point of the ValueParser is to parse values.  If you don't care about type conversion then you may as well just put the source map into the context.  Anyway i'm going to see if i can make both work somehow.  If i can't then expect me to resurrect the sub in a later version where complete backwards compatibility isn't necessary.
Nice!  Thanks for adding nice debug logging too :-)
reopened for fixVersion
reopened for fixVersion
Oops my bad sorry. I indeed did not 'svn add' the new classes since I don't have write access to the repository anyway. But this way 'svn diff' goes wrong of course. I will submit a new patch on Monday when I'm back at my working machine. I'm really sorry for the incovencience!
Fixed typo in title
The fix causes test to fail in other circumstances.  Apparently File.delete() can't be relied upon to actually delete the file.
Thanks for the patch Greg. Applied at r1153789
Thanks for the patch applied to 1.x and 1.5.1
Committed to 1.3.3 at revision: 705591
Committed to 1.3.1 branch at revision: 685854
fix also applied to the Native SDO repo on revision 671633
Fixed in SCA Java 1.2 rc4.
The diffs look good to me but it sounds like this only solves part of hte problems so we should probably file another bug to get the mgmt GUI 100% functional (for 2.1 maybe?).
Patch Commited closing bug
1.2.x patch attached
"""Have a look at current svn head i have changed the styling stuff of the component with this approach it should work now. Just specify the """"style"""" attribute and give it a width value.   I also have had a look at how we can influence the default Dojo styling stuff and have updated the wiki on this:  http://wiki.apache.org/myfaces/InputSuggestAjax  Can you test it if it works for you?    """
Thanks again to Claudio Tasso
patch by Dennis Byrne. Thanks a lot!
Hi  I was going to join in on reporting this problem. I had recently downloaded a build from 10/17/2005 in order to fix a problem I was experiencing with jscookmenu. It was not until today that I ran a test in IE and found inputcalendar didn't work. I had the same JavaScript error: dateFormatSymbols. The calendar worked fine in Firefox.  Today I downloaded MyFaces 1.1.1RC3 and can confirm that this problem no longer seems to exist for me.   Cheers Kevin
patches applied revision 434193
Good advice this works...
Patch applied. Thanks Matt!
Thanks. Yes when that fragment is part of an RTF file it provokes the exception so if you could put it into a valid RTF file it should throw the exception.
Setting to midnight UTC after parsing sounds fine to me.  yyyy-mm-dd looks good too.
bq. {{ catch (InterruptedException ignore) { Thread.currentThread().interrupt()
one additional example: http://www.who.int/entity/tb/publications/global_report/gtbr12_execsummary_ar.pdf
Fixed in revision 903352. The change should get published in a few hours.
Keith -  This is done.  Thanks
thanks Randy!  committed
Thanks Avi!
Excellent thanks a lot!
Committed thanks Dave
Hi [~wikimore] [~michaelstockton] or any other Ruby user: I'd really like to commit that but I need someone of you to review and confirm before. So if you want this being part of the next release .... thank you!
I just committed this patch. Thanks Richard!
patch that replaces the numeric constant value with the symbolic name of the enumerated constant
Any feedback on the updated patch?  I'm guessing it won't cleanly apply to trunk anymore since it's been two weeks but if the general idea looks correct I can update it to apply cleanly to the current trunk.
I'm still not in love with the sarcasm or excessive negativity.  I'll just point a few things out.  1/ I didn't veto anything and this has nothing to do with C++.  I think it is a bad idea for us to generate code in a way that causes semantics to silently change when a new optional field is added.  This in dangerous in the long term.  2/ I have no problem reverting this change.  3/ Restoring the old constructor and marking in deprecated is probably a good idea.
"""> David Reiss writes: > As I recall Jonathan was the only person who really seemed to > care about this issue and he wasn't satisfied with my > suggestion so I put it aside. Chad also requested some changes > to my diff for the JSON protocol. I'll try to reevaluate the > status some time soon but I am away from a computer today.  Jonathan - are you still in the loop on this one?  What do you think?  Given the fundamental differences in what a """"string"""" is across different languages there's unlikely to be a clean solution that suits everyone. Having a backwards-compatible compromise that works is much better than having nothing though. """
Sorry about that.
Any reason not to commit this patch? It's been hanging around a long time.
Committed revision 700277.
David lgtm  Bryan I believe assignee should be Erik as the author of initial version of patch but I can't find him in drop box.
Hi   I have re-based the patch based on the latest master branch.  Please review and let me know if any changes are required.
Committed. Thanks Hitesh
"""[~mikelid] Please take a look at the patch when you get a chance. If it looks fine you can go ahead and commit using """"git am <patch file>""""."""
commit 8193b5fddbff65e9e747f08ad36bc0ad41015ab0 Author: Bikas Saha <bikas@apache.org> Date:   Mon May 13 17:35:57 2013 -0700      TEZ-123. Cleanup package dependencies. (bikas)
patch applied - thanks!
For quick reference....  autoPagingContent is the usage in question.
"""I've tried patching the current SVN version of tapestry-core but the patch did not work for me either though no failure messages just silently does nothing when trying to get a page (404). This is a homebrewed version of the """"quickstart"""" app.  Directory structure is  exploded/x.ear/x.war/WEB-INF/classes/... JBoss is set to deploy the exploded/ directory.  Packing the JAR in WEB-INF/lib works but JBoss/Tomcat can't do reloading or hot deployment in this case which is very annoying.  It would be nice if I could get this to work :)  Used JBoss 4.0.5"""
Sorry but your testcase doesn't fail on my machine it gives me two counters in two windows - the second one with a ten second start delay.  WinXP + JDK 1.5.0_09 + Tomcat 5.5.20 + T4.0.2
"""Oops - the hivemodule.xml snippet from the previous post should read:  <implementation service-id=""""tapestry.multipart.ServletMultipartDecoder"""">     <create-instance class=""""org.apache.tapestry.multipart.MultipartDecoderImplsizeMax=-1"""" model=""""threaded"""" /> </implementation>  Sorry for any potential confusion..."""
This appears in MS-Windows environment too.  The following monkey-patch resolves the problem:   DIV.datePicker td .topLabel {      color: WindowText
T5 requires Java 1.5+
great idea
Great job. I'll review this patch today's night.
Thanks guys. :)  I also agree with Hyunsik's suggestion. I updated hadoop dependency to follow hadoop version of 'tajo-project'.  Check the patch again. please.
"""Thank You very much for Your positive review and for improving the patch!  Also a small correction to Your previous comment it should be:  """"unit tests included in her patch"""" as You know  I'm one of the girls in this year's GSoC program.  It was a great experience to collaborate with You and I Thank You for everything!    """
I'm sorry for frequently updating the issue title and contents.
Changes committed with revision: d3a9688d00bd520dc1c9b3a40b229aa9b10c87f0
Created an attachment (id=16836) struts-config
Sorry I miss. This bug is same STR-2372. I change resolution to INVALID.
Created an attachment (id=3493) Patch to validator-rules.xml for bug
If this is deemed to be a valid bug you can use my upcoming patch which simply doesn't pass the calls  to getLoginTimeout and setLoginTimeout through to the BasicDataSource.  I guess it could be a  good thing to let the BasicDataSource throw the UnsupportedOperationException notifying the  programmer that their call to the method really isn't doing anything.    OTOH they probably  care more about successfully migrating their 1.0.2 app to 1.1 and this is just getting in the way.   If they really want to know why their call to setLoginTimeout isn't working they can check the  included javadoc.  So if you decide it's a bug then there's a patch coming...
"""Now the following tag:  <html:javascript dynamicJavascript=""""false"""" staticJavascript=""""true"""" />  Always adds  <script .....>  ... </script>.  I thought that staticJavascript purpose was to allow things like:  <script src=""""<html:rewrite page='/staticJs.jsp' />"""" type=""""text/javascript""""></script>  And create a jsp with something like:  <%@ page language=""""java"""" contentType=""""text/javascript"""" %> <%@ taglib uri=""""struts-html"""" prefix=""""html"""" %> <html:javascript dynamicJavascript=""""false"""" staticJavascript=""""true"""" />  And take profit of browsers cache.  No this is broken. As a workaround   <jsp:include page=""""/staticJs.jsp"""" flush=""""true"""" />  can be used but this hasn't any benefint over generating both static and dynamic javascript in the same tag (except form multiple forms per page)   If this is the intended behaviour I'm sorry for reopenning the bug.  """
Created an attachment (id=1425) Proposed patch
Created an attachment (id=293) Trivial patch that fixes this bug
The patch is attached.
Hi Rupert!   Sorry it took so long but i thought i'd test building in a linux environment too so i set up a Centos 6.3 machine running Maven 3.0.4 and Java 1.6_41.   Got the exact same error on both machines. I have attached corresponding logs both from windows8 and centos to this issue. Link here: https://issues.apache.org/jira/secure/attachment/12570981/surefire_reportsR1450112_win8_centos6.3.zip  Kind regards Esse  EDIT: I'll also run mvn clean install for just jenatdb with logging when i get home.
Hi Florent where is the code you extracted?
Reopened because of issues with RC5
Patch attached.
I don't think this is a bug. CachedSqlEntityProcessor will execute the query only once and that is its USP. If you don't want the caching then just use SqlEntityProcessor.
Committed revision 1504570. Committed revision 1504596. Committed revision 1504597.
"""bq. accountNames => {""""shalinmangar""""""""steffkes""""""""otis""""'ErikHatcher'}  Shalin Stefan Otis and Erik: I've added all of you to the solr-committers solr-admins and confluence-administrators groups."""
Committed revision 1480515. Committed revision 1480517.
Fixed the bug description - sorry for the noise
I've gotten to the bottom of this - patch attached.
Hi which version do I need to apply the patch on?
First draft.  This just does the mapping in the constructor to the comparator.  We could do setNextReader but I can't imagine it will really make much difference given the small number of items that we typically would expect to be elevated.
I created a class that extends UpdateRequestProcessor which logs calls to each method. In the console output you'll see that only the processAdd methods are called when using the DIH in debug mode and the user checks verbose and commit.  I've attached the SOLR configuration that can be used to replicate the issue. I'm assuming that once Lucene commits the files the UpdateRequestProcessor.processCommit method should be called too.   *UpdateRequestProcessorTest.class*  package org.apache.solr.handler.dataimport
Bulk close after 3.5 is released
Added to trunk in #1098800 3.x in #1098896  sorry about visad.UnimplementedException!
see also SOLR-2399 -- this has done some great work revamping the admin UI
"""Hi Tommaso  bq. However I think it should be good if it was possible to alternatively get rid of the uimaConfig file defining each parameter inside the Processor with Solr elements (str/lst/int etc.) as well.  I've done this in the attached patch. Please review it. I'm not confident about the part:  {code} <lst name=""""fieldMappings"""">   <lst name=""""mapping"""">     <str name=""""type"""">org.apache.uima.SentenceAnnotation</str>     <str name=""""feature"""">coveredText</str>     <str name=""""field"""">sentence</str>   </lst> </lst> {code}  the structure is appropriate or not."""
The DismaxRequestHandler and StandardRequestHandler are both deprecated and replaced with SearchHandler.
bq. Actually DIH started as a standalone webapp inside AOL. We changed it because we didn't want to duplicate the schema in two places and also because we wanted to have it available by default in Solr installations.  Another web app means you need to procure hardware plan capacity/failover create firewall holes etc  Even if it were a standalone app I would still run it on the same hardware that runs Solr though I might run it on the secondary servers that aren't normally seeing query load.  Why would you need to have the schema in two places?  My DIH config doesn't mention any field names because all Solr field names match the MySQL field names.  Even if they didn't I'm not sure why it would be any different than any other SolrJ client which doesn't need to have a copy of the schema.  bq. Talking to multiple collections was never a goal for DIH â I'm not sure what value it will bring.   I've got a sharded index
"""{quote} ...but even if we don't do that i suppose it's also conceivable that someone might have their own Similarity implementation that is expensive to instantiate (ie: maintains some big in memory data structures?) and might want to be able to declare one instance and then refer to it by name in many different fieldType declarations. {quote}  I don't think this is really a use case we need to support: the purpose of Similarity today is to do term weighting not to be a huge data-structure holder.  While I know Mike's original patch went this way with LUCENE-2392 (e.g. norms) I'm not sure i like it being in Similarity in the future either.  Otherwise concepts like lazy-loading norms and all this other stuff get pushed onto the sim which is an awkward place (imagine if you have many fields).   So I think we shouldn't really design for abuses of the API. If there are other use cases for """"named similarity"""" that have to do with term weighting I'm interested. """
Provide MMap and SimpleFS DirectoryFactory impls.  Didn't include the DirectIOLinuxDirectory yet since that still seems highly experimental.
Test cases for geomultidist() function.  Add this and SOLR.2155.p3.patch
branch_3x: Committed revision 1003739.
An updated documentation of the Processor is now at http://wiki.apache.org/solr/LanguageDetection  @Lance: What params were on your mind as candidates for keyword instead of true/false and for what potential future reasons?
Hi everyone   What's the status of this issue? Have you found and workarounds?
This is something that some people have asked for since my CNET days...  I thought there was already an open issue for this but I can't seem to find it (so I guess not!)
Hi Andreas  Sorry for my late reply.  I haven't looked in to the difference between using the ResponseBuilder#getFilters and using filter's in a normal query. Are there any functional differences between the two ways other than that one of them utilizes Solr's filterCache and the other doesn't?
Dumb user
Can the ExtractionRequestHandler go away?
This looks good to me as a first step - tested with both config and schema errors.  Would be nice if single core with the solr.xml also worked but no biggie - we can fix with the rest of multi-core.  bq.  Note for no good reason what so ever   Well I think it was supposed to work (even though the whole idea is kind of broken anway) cause it attempted to so reason is prob a bug ...
I can also see this would get screwed up in Zookeeper mode if the elevate file didn't exist.
Committed revision 949471.  merged to branch-1.4 for 1.4.1
Did you clean everything out?
"""Of course I totally agree with Yonik.  I don't care that much what the default include is (upper lower ...) as long as it doesn't double-count. Double-counting is bad -- it can lead to a bad user experience. There's something about """"lower"""" that I feel makes it slightly better than """"upper"""" but I can't really explain the rationale. I don't think there's any point in compliance with legacy if nobody depended on the behavior (they couldn't specify the behavior before either).  Just because its easy to set the include doesn't mean the default is arbitrary.  Any way if the default remains to double-count I'm going to insist that my readers for the second edition of my book change this value."""
We got lots of votes on this issue seems like we should take some action here! I will assign it and make sure it will be resolved rather sooner than later.  bq. Please add this feauture  to get expectations right we are working on releasing 3.2 soon and this one should not block it. I will work towards 3.3 here
Committed revision 771270.
I'll add resuableToken support to this patch.
bq. I'm attaching a patch which adds a second query() method to SolrServer which takes the request method as the second argument.  Shouldn't that be specific to an HTTP server?
Sorry this new patch is the correct one. Still learning the ropes :-)
Ah this is in Solr-land sorry thought it was Lucene.
Otis You need to grab the 'zipped' version aka solr-215.patch.zip (since June 23). I was trying to be space & bandwidth friendly... Sorry I did not make it more obvious in some previous comments. Henri
Mike: patch reads nice and clean to me (didn't try applying though)  two nits:   1) it would be good to have a test of the case where a boolean with the default boost is specified in one bq and a seperate blank bq is specified to force the first BQ to be treated as a single query  2) let's assign  params.getParams(DMP.BQ) to a temp variable so we don't have to call it twice in three lines.
Hi Guillaume  I think that what you are saying is right and i find ( despite is on the specification or not ) that to have the error status is quite useful.  But i don't agree that if InOnly MEP terminate with an ERROR status it should propagated back.  I agree totally with Gianfranco this should be done only when we're working in sync mode otherwise it's better not  to propagate the error back and use feature like restart from the failure point that we've just implemeted.  Do you agree with this??  Andrea
maybe we can replace wsn-http-binding in the future
That's indeed odd - I didn't knowingly reformat - must have been a mishap..
path of proposed change
dupe of SLING-1529
Closing as a duplicate of SLING-394
Performing the conversion of the time out interval from seconds to milliseconds using longs in the getTimeout method avoids the overflow. Could still add additional methods to the Session API if desired.
Sorry for my delay. I would have answered early but I completely missed the notification mail about Christiano's comment :(  @Christiano: I've just tested it again and it seams to work for me:  {code} karaf@root> list START LEVEL 100  List Threshold: 50    ID   State         Blueprint      Level  Name [  42] [Active     ] [            ] [   60] Apache Shiro :: Core (1.2.0.SNAPSHOT) [  43] [Active     ] [            ] [   60] Apache ServiceMix :: Bundles :: ehcache (2.3.0.1) [  44] [Active     ] [            ] [   60] Apache Shiro :: Support :: EHCache (1.2.0.SNAPSHOT) {code}  Looking into the shiro ehcache pom i found the following:  {code} <Import-Package>   org.apache.shiro*
Finally it should be noted that the 'noSession' filter only prevents new sessions from being created.  It allows access to any existing session that might have been created in another part of the application by the application developer.
Committed by Ryan Baxter as revision 1352277
I had no idea about fetching binary data sorry. Is it flash?   Looks like my patch addressed all three points failing only for handling raw bytes.
Created an attachment (id=16986) Proposed addition to Javadocs.
Patches for 1) applied to trunk and J6
Re-attaching as I didn't grant ASF license first time I attached it!
{quote} because they are non-javadoc Eclipse-related only and not everybody uses Eclipse. {quote}  ouch... sorry about that. You will not see that again.
"""No I never see the """"Completed rebuilding index"""" message. Here's what I see on startup:  INFO  2011-03-21 09:50:41961 IndexManagerImpl:<init> - search enabled: true INFO  2011-03-21 09:50:41973 IndexManagerImpl:<init> - index dir: /home/raible/roller_data/search-index INFO  2011-03-21 09:50:42011 ReferrerQueueManagerImpl:<init> - Instantiating Referrer Queue Manager INFO  2011-03-21 09:50:42012 ReferrerQueueManagerImpl:<init> - Asynchronous referrer processing = false INFO  2011-03-21 09:50:42020 ThreadManagerImpl:<init> - Instantiating Thread Manager INFO  2011-03-21 09:50:42031 WebloggerFactory:bootstrap - Roller Weblogger business tier successfully bootstrapped INFO  2011-03-21 09:50:42031 WebloggerFactory:bootstrap -    Version: 5.0.0-RC4 INFO  2011-03-21 09:50:42032 WebloggerFactory:bootstrap -    Revision: r1076253 INFO  2011-03-21 09:50:42032 WebloggerImpl:initialize - Initializing Roller Weblogger business tier WARN  2011-03-21 09:50:49462 SharedThemeFromDir:loadThemeFromDisk - Couldn't read theme [Andreas08] preview image file [sm-theme-andreas08.png] INFO  2011-03-21 09:50:49482 ThemeManagerImpl:initialize - Loaded 7 themes from disk. INFO  2011-03-21 09:50:49482 ThreadManagerImpl:initialize - Initializing task: ScheduledEntriesTask INFO  2011-03-21 09:50:49944 ThreadManagerImpl:initialize - Initializing task: ResetHitCountsTask INFO  2011-03-21 09:50:49993 ThreadManagerImpl:initialize - Initializing task: TurnoverReferersTask INFO  2011-03-21 09:50:50049 ThreadManagerImpl:initialize - Initializing task: PingQueueTask DEBUG 2011-03-21 09:50:50077 IndexManagerImpl:initialize - Index inconsistent: new DEBUG 2011-03-21 09:50:50079 IndexManagerImpl:initialize - Creating index INFO  2011-03-21 09:50:50122 IndexManagerImpl:initialize - Index was inconsistent. Rebuilding index in the background... DEBUG 2011-03-21 09:50:50127 IndexManagerImpl:scheduleIndexOperation - Starting scheduled index operation: org.apache.roller.weblogger.business.search.operations.RebuildWebsiteIndexOperation DEBUG 2011-03-21 09:50:50191 WriteToIndexOperation:run - Starting search index operation DEBUG 2011-03-21 09:50:50192 RebuildWebsiteIndexOperation:doRun - Reindexining entire site INFO  2011-03-21 09:50:50275 WebloggerImpl:initialize - Roller Weblogger business tier successfully initialized INFO  2011-03-21 09:50:50276 RollerContext:initializeSecurityFeatures - Remember Me enabled: true INFO  2011-03-21 09:50:50278 RollerContext:initializeSecurityFeatures - Password Encryption Algorithm set to 'SHA' INFO  2011-03-21 09:50:50278 RollerContext:setupVelocity - Initializing Velocity INFO  2011-03-21 09:50:56132 RequestMappingFilter:init - Request mapping filter initialized 1 mappers configured. INFO  2011-03-21 09:50:56157 IPBanFilter:init - INIT IPBanFilter INFO  2011-03-21 09:50:56158 CompressionFilter:init - Compressed Output ENABLED INFO  2011-03-21 09:50:56186 MediaResourceServlet:init - Initializing ResourceServlet INFO  2011-03-21 09:50:56220 PageServlet:init - Initializing PageServlet INFO  2011-03-21 09:50:56222 WeblogPageCache:<init> - {timeout=3600 enabled=true size=400 id=cache.weblogpage} INFO  2011-03-21 09:50:56229 CacheManager:<clinit> - Cache Manager Initialized. INFO  2011-03-21 09:50:56230 CacheManager:<clinit> - Cache Factory = org.apache.roller.weblogger.util.cache.ExpiringLRUCacheFactoryImpl INFO  2011-03-21 09:50:56236 SiteWideCache:<init> - {timeout=1800 enabled=true size=50 id=cache.sitewide} INFO  2011-03-21 09:50:56237 PageServlet:init - Referrer processing enabled = true INFO  2011-03-21 09:50:56241 FeedServlet:init - Initializing FeedServlet INFO  2011-03-21 09:50:56244 WeblogFeedCache:<init> - {timeout=3600 enabled=true size=200 id=cache.weblogfeed} INFO  2011-03-21 09:50:56246 ResourceServlet:init - Initializing ResourceServlet INFO  2011-03-21 09:50:56249 SearchServlet:init - Initializing SearchServlet INFO  2011-03-21 09:50:56255 CommentServlet:init - Initializing CommentServlet INFO  2011-03-21 09:50:56297 CommentValidationManager:<init> - Configured CommentValidator: Blacklist Comment Validator / org.apache.roller.weblogger.ui.rendering.plugins.comments.BlacklistCommentValidator INFO  2011-03-21 09:50:56299 CommentValidationManager:<init> - Configured CommentValidator: Excess Links Comment Validator / org.apache.roller.weblogger.ui.rendering.plugins.comments.ExcessLinksCommentValidator INFO  2011-03-21 09:50:56301 CommentValidationManager:<init> - Configured CommentValidator: Excess Size Comment Validator / org.apache.roller.weblogger.ui.rendering.plugins.comments.ExcessSizeCommentValidator INFO  2011-03-21 09:50:56301 CommentValidationManager:<init> - Configured 3 CommentValidators INFO  2011-03-21 09:50:56302 CommentServlet:init - Comment Throttling DISABLED INFO  2011-03-21 09:50:56309 PlanetFeedServlet:init - Initializing PlanetRssServlet INFO  2011-03-21 09:50:56311 PlanetCache:<init> - Planet cache = {timeout=1800 enabled=true size=10 id=cache.planet} INFO  2011-03-21 09:50:56314 CommentValidationManager:<init> - Configured CommentValidator: Blacklist Comment Validator / org.apache.roller.weblogger.ui.rendering.plugins.comments.BlacklistCommentValidator INFO  2011-03-21 09:50:56314 CommentValidationManager:<init> - Configured CommentValidator: Excess Links Comment Validator / org.apache.roller.weblogger.ui.rendering.plugins.comments.ExcessLinksCommentValidator INFO  2011-03-21 09:50:56315 CommentValidationManager:<init> - Configured CommentValidator: Excess Size Comment Validator / org.apache.roller.weblogger.ui.rendering.plugins.comments.ExcessSizeCommentValidator INFO  2011-03-21 09:50:56315 CommentValidationManager:<init> - Configured 3 CommentValidators INFO  2011-03-21 09:50:56318 RSDServlet:init - Initializing RSDServlet INFO  2011-03-21 09:50:56320 PreviewResourceServlet:init - Initializing PreviewResourceServlet INFO  2011-03-21 09:50:56340 PreviewServlet:init - Initializing PreviewServlet INFO  2011-03-21 09:50:56342 PreviewThemeImageServlet:init - Initializing PreviewThemeImageServlet INFO  2011-03-21 09:51:24197 ContinuousWorkerThread:run - HitCountQueueProcessor Started. INFO  2011-03-21 09:50:56342 PreviewThemeImageServlet:init - Initializing PreviewThemeImageServlet INFO  2011-03-21 09:51:24197 ContinuousWorkerThread:run - HitCountQueueProcessor Started. INFO  2011-03-21 09:51:29473 Blacklist:<clinit> - Initializing MT Blacklist WARN  2011-03-21 09:51:29478 Blacklist:loadBlacklistFromFile - Couldn't find downloaded blacklist loaded blacklist.txt from classpath instead INFO  2011-03-21 09:51:29494 Blacklist:loadBlacklistFromFile - Number of blacklist string rules: 3102 INFO  2011-03-21 09:51:29495 Blacklist:loadBlacklistFromFile - Number of blacklist regex rules: 15 INFO  2011-03-21 09:51:35146 RendererManager:<clinit> - Renderer Manager Initialized. INFO  2011-03-21 09:51:35151 RollerVelocity:<clinit> - Initializing Velocity Rendering Engine DEBUG 2011-03-21 09:51:35940 IndexManagerImpl:executeIndexOperationNow - Executing index operation now: org.apache.roller.weblogger.business.search.operations.SearchOperation DEBUG 2011-03-21 09:51:36324 IndexManagerImpl:executeIndexOperationNow - Executing index operation now: org.apache.roller.weblogger.business.search.operations.SearchOperation DEBUG 2011-03-21 09:53:49889 IndexManagerImpl:executeIndexOperationNow - Executing index operation now: org.apache.roller.weblogger.business.search.operations.SearchOperation DEBUG 2011-03-21 09:53:50561 IndexManagerImpl:executeIndexOperationNow - Executing index operation now: org.apache.roller.weblogger.business.search.operations.SearchOperation"""
Closing as a duplicate of ROL-1642.
A patch file is attached here to address these problems. Many files have been touched. Sorry for it. But I'm sure there are more files than these that have i18n problems. I'd like to dig more into it as soon as these critical ones are resolved.
I must have clicked the wrong link.  I didn't mean to mark this resolved.
newPost now sets publishEntry to true.
sorry should be: path -p6 < pluto-1.1.0-taglib-el.diff
Sorry I missed Bill's comment.  bq. We've seen similar exceptions when loading data that contains text with the column delimiter in it which produces shorter than expected tuples. Could that be the case here?  I don't think that's the case here.  However I've made many silly mistakes&misunderstandings in pig before.  Let me double check.
I'm afraid one can't extract the text from the given pdfs. Both are using fonts with a non human readable encoding and as there isn't included any mapping you'll get rubbish instead of the text. Even the acrobat reader can't extract the text.
Hi  Here is the missing file. I'm sorry for the delay.  Regards Eric
"""Ok I just downloaded trunk and the patch is 99% the same with mine so I'll close the issue now...One comment though and sorry for nitpicking...Shouldn't the second constructor simply be """"this(patterns null)"""
"""Sorry Alvaro  lot of things need to be done at the same time.  But I am still not 100% clear what you want me to do / change.  I put installation/wiki links on top-right now: http://incubator.apache.org/openmeetings/index.html  What about the links to the Tutorials you want to put those in the section """"Installation and Upgrade"""" right?  Sebastian"""
Sorry I put a comment here which was for another issue (tired as said already)
Sorry again I tried with the same revision same language same theme and I use also FF4 and I still don't reproduce (see screenshot-1)  It would be interesting ot have other persons tests
Sorry to hack your issue Bilgin my intention is to keep readable traces (in ML you can't read easily stack)
Sorry for the late reply. Your corrections look good I've just applied them. Thanks!
"""Unfortunately we can't do that because that is not explicit on the spec javadoc and there is a way to do what is expected using """"targets"""" property."""
Sorry I was not able to create one diff-file so I added for every build.xml an own diff.
Sorry I use JDK 1.4.2_06 and not 1.4.1 :)
Phil  I must have missed this email. I am sorry. I will get this done and the other changes to support the UpdatingMultipleLinearRegression class.  I updated my source and noticed the changes were missing. I did my detective work and noticed this.  Mea Culpa!!  -Greg
Sorry looks like we took to long to get to this the patch no longer applies.  Can you please upmerge to latest?
Sorry to come in late. Some clarifications: # MR1 JT kills all running tasks on a TT when it's deemed 'lost'. # It also kills all completed maps on that TT for 'active' jobs. # The tasks are marked KILLED rather than FAILED and thus don't count towards the job which is correct since it wasn't the job's fault.  Hope this helps.
bq. I'm gonna hold off on LUCENE-2771 until we figure this one out... because it would make your getSequentialSubReaders in the synced=true case quite heavy (without modifications).  Sorry I was wrong on this... I totally forgot the norms cache is lazy-loaded always in that patch. I'll commit LUCENE-2771 it shouldnt affect this!
Strange things going on. With Google Chrome uploading patch files corrupts the file. With MSIE it worked. Sorry for the noise. Yesterday it worked normally...
bq. Here's a patch for the general case and it also adds a warning that you should set your similarity with Similarity.setDefault especially if you omit norms.   Is there no way to remove this stupid static default and deprecate Similarity.(g|s)etDefault()? Can we not use the Similarity from IndexWriter for the case of NormsWriter?
"""Working: - Bash on Linux  Not Working: - Bash on Solaris - CSH on Solaris - KSH on AIX - Bash on AIX  So you're right. I close this bug as """"work as designed"""".  Sorry for the noise :)"""
Sorry for the late review. Have a few minor questions/comments.  60. What happens if have 2 instances of Consumers with the same clientid in the same jvm? Does one of them fail because it fails to register metrics? Ditto for Producers.  61. ConsumerTopicStats: What if a topic is named AllTopics? We use to handle this by adding a - in topic specific stats.  62. ZookeeperConsumerConnector: Do we need to validate groupid?  63. ClientId: Does the clientid length need to be different from topic length?  64. AbstractFetcherThread: When building a fetch request do we need to pass in brokerInfo as part of the client id? BrokerInfo contains the source broker info and the fetch requests are always made to the source broker.
Here is a stronger/generalized version of this patch. Due to a lack to time on my part this has not been compiled or tested (sorry). Please test if you can David/Joachim.
sorry for the delay
Hi  I just made some more (long running) tests and found some problems in the patch I submitted. The mechanism is working but it shrinks the caches too quickly and it prints too much log output (log.info instead of log.debug). I will fix those problems and submit a new patch. Sorry.  Thomas
Oh sorry will submit them tomorrow. Hmm svn diff is really strange. Don't know why the files were not included though I did the diff from the src/java/o/a/c/jci dir.
<pctony> Herbert sorry we cannot do this.  The scope of the blemlist AIUI is at the SVN repo level - and we run a single repo for all the ASF projects.  However buildbot works at the per-project level. Short of perhaps re-writing all of our buildbot mechanisms from the SVN master this is not do-able. Sorry.
Sorry forgot that dev lists were merged before JMeter was made TLP
You are missing my point sorry   By 'check in normally' I mean just commit them using 'svn add ...' 'svn commit ..' not 'svn import' you cant do that.  people.apache.org was down for a while but is now back that however is not related as you dont need to access people.apache.org to commit to svn.
Sorry I was wrong about needing to fix Gump - that just needs the build.xml patch to remove the copy of the files in lib.
Sorry my bad. Let me upload what I have.
Carl - Sorry I missed that though I did see it on the logs 2x the other being this one. Fixed in this patch.
Running tests on this. Will commit once it is done. Sorry this one fell off my radar...
Sorry for trouble. I didn't realize that port and context path is configured via pax web & cxf bundle configuration in Servicemix
Hi Lars. Sorry I wasn't watching this before so I missed your work til you mentioned it on the HBase JIRA. I'll try to take a look at this this week. Feel free to grab me on IRC if you have specific questions.
Patch is not well formed cancel patch.  Sorry about this.
+1  Jihoon Thank you for your contribution.  Sergio Thank you for your review.
+1  Looks good to me.  Thanks Hyunsik!
+1  This is a very desired feature for me. Ship it!
Thanks for your review.  I've just committed.
Many thanks ... looks good
Thanks Lukas for pointing these issues out. Will review your fixes and do the necessary changes. The JMS transport fix should go to Axis2 Transports project.
Thanks for valuable idea.   I haven't yet decided  the API for this. Once I decide  I will publish it here . I would consider your valuable suggestion when designing API.    Thanks  Indika
Sumeet we did it purposely because if the proxy services are dynamically loaded then we need to build those services at the runtime and redeploy them which is a little bit risky on a production environment. Also there is no mechanism to check whether a particular proxy definition is updated in the registry or not because this configuration is used only at the startup.  We can add this functionality to load the proxy services only at the startup time from the registry (this doesn't mean they are dynamic) but I don't see any value there. If you could explain a bit about your exact requirement I might be able to provide an alternative to this.  Thanks Ruwan
Thank Henri and Don!  I found there are two common-validator.jar in my jboss application server env. One in %JBOSS_HOME%/server/default/lib/ and the other in the lib directory of my web application. Then I delete the latter and it works well.   Thank you so much!
Carl Lindberg absolutely you are right. I was not far-sighted enough to think  of that. Thank you about your point.^^ My one is not a good idea. I will try to fix it. (T.T). Thanks again.
As Olivier does all the great work here I assigned the issue to him. Thank you!
Thanks Jarcec!
The patch is in: https://git-wip-us.apache.org/repos/asf?p=sqoop.git
Committed to sqoop2. Thanks Jarcec!
Thanks Jarcec - that clarifies.
Committed to trunk. Thanks Jarcec!
Patch is in.  Thanks Jarcec.
Patch committed. Thanks Jarek!
Thanks  Klaus!
Thanks Uwe!
bq. Here the patch for the groovy script.  cool looks good thanks!
Thanks for the commit!
Thanks Martijn!
Thanks Otis.
Committed revision 741710.  Thanks Patrick!
fixed in trunk... thanks Henri!
added in rev 546223.  Thanks Will!
Guillaume Thanks for your commit. It fix this problem and my example works fine now with this fix.
Guillaume  I took a quick look at your suggestions.  I agree that we can reduce the code a bit.  However I think that we need to keep the activate method as it initializes the servingXml internals.   I'm not sure about the deactivate method.  I also noticed that you have added a patch but being new to jira I'm not sure if that means that you've made changes to my attachment or something else.  Are you still looking for me to correct the formatting and to remove the extraneous methods/classes?  Thanks James
Thanks. Moved
Great thanks for fixing the OSGi import. I'll create a new issue for the failing trunk build with more info... https://issues.apache.org/jira/browse/SLING-2770
Looks good. Thanks for your work on this!
Hi Ian  this looks brilliant to me! So fat +1 from me.
Hi [~justinedelson] thanks for applying the patch and fixing the formatting.
Looks good thanks!
Awesome - thanks so much for the submission Scott!
patch applied.  thanks!
Code looks good to me patch's been committed thanks!  Awesome to now be able both the 0.8 and 0.9 messaging API through the same interface this is going to save us a lot of headaches :)
Looks good to me. Works for me  too. :-)
Bruno you mentioned in a gtalk conversation that you had a patch for this but I can't find it here in JIRA.. could you attach it please? thanks!
Looks good to me.  Colm.
Thanks for the patch!  http://svn.apache.org/viewvc?view=revision&revision=1361531
This was a temporary connectivity issue.  You can close this bug.  Thank you.
Applied the patch. Thanks Dennis!
Patch applied in revision 1055819.
Applied the patch : At revision: 1051526
Lets fix this post rampart-1.3
Released in Qpid 0.24 http://qpid.apache.org/releases/qpid-0.24/index.html
Reviewed by Rob.  Approve for 0.20.
Changes look reasonable to me.
Hi Robbie would you mind reviewing this commit?
Thanks for catching this I have fixed it.
This change was only applied to the 0.5.x-dev branch. It needs to be merged to trunk.
Looks good to me.
This is a duplicate of QPID-2488
Hi Aidan can you review this change please thanks.
Updating 'Fix For'  to Unknown on issues not targeted for 0.8
Hi Aidan can you take care of this when you do QPID-1010. Thanks
Hey Aidan can you cast your eye over this again. Thanks.
the code has been checked in. Thanks to Gordon for that.
All existing documentation/discussion from Moin Moin wiki now copied across to Confluence JIRA and re-formatted as appropriate.  Should now all be updating on Confluence wiki and retire Moin Moin in a couple of weeks when people have been able to check that their pages look ok on Confluence.
Hi Rustam Finally I will not commit the IMAP-351 patch so it will be good that you attach your patch here. Thx Eric
Fixed in 1.4 release branch in r605105.   Leaving open until fix (with other pending changes) is ported back to trunk.
missed encoding for non-array request parameters.  It doesn't look like I can remove previously attached files.  Whoever is reviewing the code  should use the latest patch.
Wow fast. Thanks!
Committed to trunk. Thank you Lorand!
committed to trunk. Thanks [~elserj]!
[~bikassaha] Thanks for the heads-up Bikas! This JIRA is not concerned with the Tez integration for Pig and is simply the abstraction in Pig to allow for alternate ExecutionEngines in Pig. But will certainly change this on the Tez integration side of stuff.  Thanks a lot [~cheolsoo] for continuing this! I think everything looks good from my end. I can certainly see why we may want to keep this on a different branch until everything is finalized. Certain things may still need more work. For example OutputStats is not completed abstracted out as it still has references to POStore which is a MR implementation construct. ScriptState/PPNL/JobStats may still need more abstraction (especially PPNL) and reworking to incorporate a new ExecutionEngine abstraction. I think what we have done here is the minimum foundation for an abstraction though and it would be appropriate to put into trunk but these are not my decisions to make.   With regard to public methods that were changed I don't think most of them are a big deal besides as Cheolsoo said the PigServer throwing PigException. I never thought IOException was a good exception to throw but I think reverting PigServer back to IOException as it is userfacing code is not a big deal. The rest of the method signature changes shouldn't be worrisome because most of them are internal to the project.   However the change from JobStats to MRJobStats while necessary (as each ExecutionEngine would have it's own type of JobStats it would present to the end user) could be problematic because it is userfacing code and would probably break people who were previously using JobStats. That I think is the most important thing to keep in mind. The task of making the PPNL and JobStats clearly tied to the ExecutionEngine should be thought through also.
Committed to trunk. Thanks Mathias!
Thanks for volunteering for this Prashanth!
Committed to 0.11.1 branch and trunk. Thanks a ton Kai!
Committed to trunk. Thanks Johnny!
Patch committed to trunk and 0.11 branch.  Zhijie thanks for the patch!
Thanks Jon. +1. Patch looks good.   Minor nitpick I have is that it would be nice to give the test a more relevant name than testAutomaticallyMadeName. Something like testRelationAliasForBinCond. But I am not going to insist.
Committed to trunk. Thanks for the review Alan!
"""changing the name of JIRA from """"multithreaded"""" to """"parallel"""" as we've done it with forks instead of threads."""
Yes I missed this basic case. Thanks for pointing out. I will cancel this patch.
Committed to trunk. Thanks Jeff!
Hi Alan I was merely commenting on the title of the JIRA not the UDF name.
Yes actually I take out the hbase part when I commit.
+1.  Looks good to me.
Review board comment: https://reviews.apache.org/r/314/
Attached patch should apply cleanly to the current trunk. Please review.
Patch committed to trunk. Thanks Niraj.
bq. Though pig itself compiled fine and is ready to go the contrib projects (owlzebrapiggybank/hiverc) didnt compile I think because either it didn't download dependices of those projects or didn't include them in the build path.  Ashutosh yes. I did realize it and as you rightly guessed it is because the ivy-dependency list doesn't have some jars that are actually being used by contrib packages. This is not an issue related to eclipse-classpath. However there has to be a JIRA opened to solve this issue of downloading the missing jars by ivy.   Can you please file a JIRA for this? Thanks
"""Hi Gianmarco Thanks for your concern. Actually we need one additional step to make bin/pig work. We shall copy $PIG_HOME/build/pig-0.8.0-dev.jar to $PIG_HOME/pig-0.8.0-core.jar. This will be handled in ant's """"package"""" target when releasing. But if you check out from svn we will do this additional step to work with bin/pig."""
Patch is committed to the trunk. Thanks Niraj.
Just back from vacation. Have updated the code with required changes. It should be good to go now. Pradeep can you or any other committer review it ?
Patch was commited on May 13 2009.
Hudson does not work well patch has been manually tested.
Patch looks good. One minor comment PlanHelper.LoadStoreFinder may better be PlanHelper.LoadStoreNativeFinder.
patch committed. Thanks pradeep!
Thanks Pi.  I do not think there is any special tricks besides those with Linux (such as you will need ant junit java svn etc).  If you want you can also send me your pig.jar and then I will try it out for you.  Best regards Xu   org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:373>> )
Fix checked in at revision 648768.  Thanks Xu.
In MapReducePlanCompiler.java   {code} if (PigContext.instantiateFuncFromSpec(materializedResult.outFileSpec.getFuncSpec())          instanceof ReversibleLoadStoreFunc) {     POMapreduce pom = new POMapreduce(logicalKey.getScope()                                       nodeIdGenerator.getNextNodeId(logicalKey.getScope())                                       execEngine.getPhysicalOpTable()                                       logicalKey                                       pigContext)
Antonio could you just create a patch file that I can apply.
Totally agree. I don't see any good reason why Pig should require 1.6 at this stage. Don't make it harder for the users (and developers under OSX - that includes me) than it already is.
>Tilman Hausherr   hello finally it  works! I just downloaded the revision 1535956 and bulded it with Ant. and added the new *.jar to my project's build path. it printed very well.  thank you very much.
"""I've revised DateConverter to conform to java 6:      changed isAlphabetic to isDigit (and reversed the ?: values)      use SimpleDateFormat(""""Z"""") as Tilman suggested.  Also fixed some Javadoc in both DateConverter and TestDateUtil"""
change default assembly behaviour in preflight
Hello Andreas  Thank you for the information.  Shall try the same and upload the patch shortly.  Thanks & Regards Ravikiran Mane.
A PDF which demonstrates the problem
After applying Mathias patch the sample documents works fine.  Thanks Mathias.
Direct link to mail item didn't work.
Will be taken care of next releases
released with OWB-1.1.4
"""If you run the train command you get an error in the model serializer I suggest to train on the ppa data set serialize the model and then load it again to ensure there are no runtime errors in this code path. Its also easier to debug in an IDE if it fails in a junit test.  That is the exception I get:  Caused by: java.lang.NullPointerException"
Committed revision 1475954.
"""Hi Eduardo  I have tried with your settings for the timezone. I was not able to reproduce it. The meeting was correctly displayed in the calendar the meeting times in the iCal invitations where send fine I tried also with """"simple email"""". The invitation hash also worked fine and had the correct time validity."""
Johannes --   Thanks for the complete testcase! Sorry about taking so long to fix this one.   Thanks Rick
Hi Michael thank you for your answer. I thought that it would not be so easy to solve this issue. Unfortunately.  I agree with you that the main problem here is that a copy action is missing and I guess AttachStrategy.persist is the right place to add some kind of copy functionality.  I started looking into the source code of the enhanced class from the test. PersistenceCapable declares a public method to copy fields (pcCopyFields(...)). Unfortunately you can't use in AttachStrategy.persist because the entity to persist has no statemanager at this point (an exception InvalidStateException will be the result).  My suggestion is to modify the class enhancement and remove the StateManager check in pcCopyFields (I don't see the need for this check).  Afterwards you can use pcCopyFields in AttachStrategy.persist by adding something like      ...         if (manager.getCopyNew()) {             int[] fields = new int[meta.getFields().length]
Thanks for the patch Janko. I put the changes into 1.3.x and 2.0.0.
"""Added the following to getRollbackOnly():  if (status == Status.STATUS_NO_TRANSACTION) {     throw new IllegalStateException(""""There is no transaction in progess."""")"""
committed to branch-4.0 and trunk. Thanks Rohini.
Closing issue
Didn't modify any testcases because its just some minor typo fixes.
Thanks Robert. Committed to trunk.
Closing issue
I had almost the same interface for the command-line option in mind but sticking to the previous formats for specifying the id:  ./wmgr-client --url http://localhost:9001 --operation --getWorkflowInstMet --id <workflowInstId>  And in return display the values in the format as you suggested above.
Hi Antonio  I can take this one if nobody is working on it.  Would be a nice one to get my feet wet... :)  Thx Sam
Hi Suresh  I think this task is also related to AMBER-63 since both OAuth 2 and Open Id Connect use JSON Web Token (JWT).
Thanks Jacques :)
Hi Tri  Those files looks good. I cursorily checked only.CommonUiLabels.xml.patch but from their sides I guess the others are good as well. But done from the OFBiz roo directory it's the main OFBiz directory where you fine the LICENSE and APACHE2_HEADER files. Anyway not a big deal I will try to apply them as is and will let you know  You don't have to remove the old files as you can see they are now grayed and it's good that you checked the grant license box.  Thanks
https://cwiki.apache.org/confluence/display/OFBADMIN/OFBiz+Contributors+Best+Practices#OFBizContributorsBestPractices-HowtoSendinYourContributions%28orhowtocreateandapplypatches%29
Fixed rev 1190423. Thanks Jose!
Hi Leon thanks i'll take a look. Anyway i thought of refactoring this parsing mechanism we can do better :-).   Cheers Sascha
Thank you Jacques for the quick commite !!  Pierre
Thanks RenÃ©  Your patch is in trunk at r1061167 R10.04 at r1061173 R9.04 at r1061174
Thanks Ankit  Your patch is in trunk at r1060220 I did not backport as it's not breaking anything
Thanks Erwan for you're effort :-)
Hi Sascha  There is an overlap with OFBIZ-3843 could you please check?  Thanks
Hi Erwan  If you don't mind I have re-assigned to you as you assigned yourself the umbrella task
Done at 946323 and 946322 Thanks Blas !
Done at revs 946320 and 946321   thanks Blas
Hi Jacques hi Ankit ... i think i found the reason why we can't call layer recursivly ... i made a patch but i can't sync my trunk today (i'm at our friends house and they have only internet via gps connection ... it's so slow :-D). I will provide the patch on monday morning.  Have a good Weekend Sascha
Thanks Amit & Vivek - Committed at r907620.  -- Ashish
Thanks Mirdul committed in trunk r903428 and 9.04 r903429
Thank you Ashish Vijaywargiya and Pranay Pandey :)   Thanks and Regards -- Akash Jain
Great last enhancements thanks guys!
Ashish This is working fine for me. Please take a look at https://issues.apache.org/jira/browse/OFBIZ-2277
"""Hi Bilgin  you are right the entitymode.xml can be removed but I when I add it I'm thinking to add a new filed to PosTerminalInternTx  entity to store """"Pos Paid Reason"""" instead of store it in the reasonComment field.  I have already created some enumeration into DemoPosData.xml but it is still not been used. There are still a lot of things that has been not implemented or not working so contribution from other people are welcome. Removing the hardcoded value pos-1 into WebPosEvents.java at line 77 is something still to be done. I'm thinking to add a combo box after logged in to choose the posTerminalId but I didn't get the time to do it.  Thanks  Marco"""
Thanks Bruno and Bilgin  You patches are in trunk revision: 691953   Great to see good cooperation at work :)
Jacopo  Thank you for your work! I'll test it today and let you know what I find.
Thanks Alok Agnihotri for your valuable patch. Special thanks to Rishi SolankiPranay PandeyBrajesh Patel and others in helping Alok.  Changes are in rev # 607341.  -- Ashish Vijaywargiya
Sorry Jacques I have not seen that it was a grouped bugs.  In this case I have used to set in the grouped bugs the sum of the components used by detailed issues.  I didn't like unknow components.  Otherwise we can add a new fictitious component - GROUPED ISSUES - and assign this component to this type of issue.   Thanks  Marco
Yes I have been watching OFBIZ-1453 with interest.  I approve of the issue and implementation and didn't have anything to add so I haven't been involved.  The thing I have been thinking about is that there's some assumptions to be made here for pricing and the type of question.  I.e. multiselect questions should display the absolute price.  I'm also trying to work over some other scenarios.  I'll clean up what I have an submit a patch tomorrow.
Marco BTW I have only helped Santosh Malviya for OFBIZ-1468 where ever he needs and reviewed his work. After that Its only his effort to convert java to minilang + testing the patch. So majority of credit goes to Santosh Malviya :)
"""I have added          <property file=""""framework/base/config/cache.properties""""/>         <echo message=""""NOTICE: deleting ${cache.file.store}.db""""/>         <delete file=""""${cache.file.store}.db"""" verbose=""""true""""/>  to the clean task of my local main build.xml.  After some more testing/discussion perhaps I will post it as a patch. """
This patch solved this issue in a test case.
About EPL licenceplease have a look at http://people.apache.org/~rubys/3party.html#category-b. Seems fine with some light constraints
"""Can't remember how to put the state of this issue to """"In progress"""" not a big deal anyway..."""
Ray Jacoo  I finally commited it in revision: 485087.  Thanks
Thanks Scott  I have committed your last work in svn with rev. 7876
Works fine with my minimal test case and the tutorial. Thank you!
Hi Daisy  could you please dispatch this review to one of your team? Mingfei did not respond on that and I would like to be certain we got all provided patches with us for 0.7.  Thanks in advance Svante
Applied. Thanks!
The patch looks good just applied on the 1.1 branch and trunk. Thanks a lot keep them coming! :)
Could you make sure that the following path exists on your machine:  C:\Program Files\Apache\Tomcat 6.0\webapps\ode\WEB-INF/jpadb  It sometimes happen that Tomcat for some reason fails to open the WAR which results in an incomplete deployment.
i don't think that any special filtering should be performed for the access control content such as e.g. rep:policy nodes. but as far as i can see you didn't incorporate this into the patch right?  testing for Tree#exists in the NodeDelegate looks ok to me...
that's nice! The patch looks good Emanuele :)
Further optimized caching in KernelNodeStore a bit in revision: 1450145  The test now just runs for a very long time (with -Xmx128m) and didn't run out of memory so far on my machine. Though it is still working on 51200 child nodes after it reported '25600 nodes in 1440293 ms'.
In revision 1406185 I applied the patch from https://github.com/jukka/jackrabbit-oak/commit/e01558418008e41a94c44ea00494f6a939ed58a2. This fixes the problem.
[~anchela] which improvements do you have in mind?
Hi Talat - no this patch is for the branch_2x branch only.
Committed @revision 1440266 in trunk Thanks Lufeng if and when you get around to the patch for 2.x I can review. The patches for tests are very welcome in Nutch. Thank you.
Looks good to me Markus. +1
+1 thanks
Actually i made a simple test on 2 small linkdb and i didn't see any improvment. You're right. There is no need to add it.
"""Using nutch with solr has been a very demanding request so it will be very useful when this makes into trunk. I have spend some time reviewing the patch which I find quite elegant.   Some improvements to the patch would be  - make NutchDocument implement VersionedWritable instead of writable and delegate version checking to superclass - refactor getDetails() methods in HitDetailer to Searcher (it is not likely that a class would implement Searcher but not HitDetailer) - use Searcher delete HitDetailer and SearchBean  - Rename XXXBean classes so that they do not include """"bean"""". (I think it is confusing to have bean objects that have non-trivial functionality) - refactor LuceneSearchBean.VERSION to RPCSearchBean - remove unrelated changes from the patch.(the changes in NGramProfile HTMLLanguageParserLanguageIdentifier... correct me if i'm wrong)  As far as i can see we do not need any metadata for Solr backend and only need StoreIndex and Vector options for lucene backend so i think we can simplify NutchDocument#metadata. We may implement :   {code} class FieldMeta { o.a.l.document.Field.Store store"""
Re: setReaderThread I didn't know we had such a method.  It looks like it was added here: http://cvs.apache.org/viewcvs.cgi/jakarta-commons/net/src/java/org/apache/commons/net/telnet/TelnetClient.java?rev=1.9&view=markup  Other than adding documentation to FTPClient about this it sounds like like there isn't any other action to take on this report???
I didn't know about a Trinidad regex Validator. I've created it from scratch.  Didn't look for any Validator code on the internet because I wanted to make sure it was JSF 2.0 compliant.
Duplicate of MYFACES-2970
HI Ronald  The fix I put in on the 30th caused pages to stop drawing in the RC1.  I have reverted part of the fix for this bug to get 657 resolved. Could you please take a look at the comments for 657 and test out the head of the 1.1.1 branch to make sure that this version works for your case.
"""sure only """"/"""" works. if path="""""""" that's considered as no path defined."""
I can confirm this is not happening on Ubuntu 12.04 with OpenJDK 1.6.0_24. Did anyone get a chance to look at this apparently Mac-specific issue?
Hi I've started working on this. I think I have an idea of what to do design-wise but if anyone has any suggestions please let me know.
Fixed in subversion repository as of r1328492
{quote} I don't think it makes sense to argue about who the exception messages are for [...] {quote}  I think the opposite because it has an impact on the library design. Not that CM cannot go on with the current way
3.1.0-incubating released!
Attached another refresh
[~arpitgupta] thanks for helping narrow this down!
Could you please share how is it impacting ?
[~acmurthy] did you commit to both branch-1.2 and branch-1?
I just thought of one more thing we should do.  We should make the StringInterner as @Public and @Stable.  The API is simple enough I don't see much of a problem locking it down.
Hi Jason! I was thinking one of those parameters was the time the NM should wait before sending the SIGTERM. Alas I was wrong ContainerLaunch.java:337 sends the SIGTERM pretty much instantly. That is messing me up in MAPREDUCE-4135.  I'm still thinking having a small time delay before sending even the SIGTERM should be good. If we can do it on the NM and not on the RM I think we might be able to conserve RM resources.
Can you please address the findbugs warning. (inconsistent synchronization)  I did a quick look at your patch and I am a bit confused by the implementation of setSignaled.  I realize that you did not really change the code at all but is there a reason that the parameter is ignored?  If you don't know I can look into it myself.
Actually now that I think of it we never had anything exposed via jmx for the RM? Looks like this is the first bean we are adding. Is that right? I thought some of the work that Thomas did on web services was also exposed via jmx. No?
Attaching a patch that adds API for accessing the other info (which is not writtent to trace file).
Nit-corrected patch. Committing.
I just committed this. Thanks Anupam!
Uploaded new patch by addressing review comments from Amar.
+1 lgtm to get rid of the extra logging and to kill the child process.
addressed the Cos and Balaji comments. Latest patch for yahoo security branch.
Thanks for the update Tom this is getting close!  Some more comments:  {quote} > src/java/org/apache/hadoop/mapreduce/TaskType.java > -> Public? Stable? It is public. Not sure about whether it is stable yet. {quote} I'd tend to lean towards making it Stable to ensure it's never broken it probably deserves that status given how central it is to the framework. Thoughts? I don't see either 'audience' for this yet it the patch maybe you missed it?  bq. Are MarkableIterator and MarkableIteratorInterface public user classes? The unit test for them (TestValueIterReset) implies that the user instantiates a MarkableIterator. +1 public  bq. I left all of the lib.db classes public evolving since they are used by various extensions and users. Does this sound reasonable? +1  {quote} > src/java/org/apache/hadoop/mapreduce/Job.java > -> public  Yes.  > src/java/org/apache/hadoop/mapreduce/JobCounter.java > -> public  Yes.  > src/java/org/apache/hadoop/mapreduce/JobPriority.java > -> public  Yes. {quote}  Again I don't see 'audience' as 'public' another human miss on your part? Or am I seeing the wrong patch?  ----  Comments on rest of the classes I missed in the first pass:  src/java/org/apache/hadoop/mapreduce/security/token/package-info.java -> Unstable src/java/org/apache/hadoop/mapreduce/security/token/delegation/package-info.java -> Unstable src/java/org/apache/hadoop/mapreduce/TaskTrackerInfo.java -> Public used in Cluster src/java/org/apache/hadoop/mapreduce/util/package-info.java -> Unstable src/java/org/apache/hadoop/mapreduce/TaskCompletionEvent.java -> Public Job.getTaskCompletionEvent src/java/org/apache/hadoop/mapred/JobInProgress.java -> limited-private for schedulers src/java/org/apache/hadoop/mapred/Task.java -> limited-private for schedulers src/java/org/apache/hadoop/mapred/JobEndNotifier.java -> public maybe evolving since end-users use this directly src/java/org/apache/hadoop/mapred/LineRecordReader.java -> limited-private (mr pig) src/java/org/apache/hadoop/mapred/JobProfile.java -> Odd I thought this was 'public' - but I can't find any uses for it. The FairScheduler uses it so 'limited-private' ? src/java/org/apache/hadoop/mapred/TaskCompletionEvent.java -> public accessible via JobClient
uploading the patch for commit to internal branch it has review comments from sreekanth and cos implemented.
Looks like your change did not fix the findbugs. But i think you need to suppress the findbugs warning : HRS_REQUEST_PARAMETER_TO_HTTP_HEADER. It is suppressed for jobdetails.jsp in MAPREDUCE-1185.
Patch that fixes this problem and adds a test case demonstrating it.
Ported to trunk
Updating patch synch'ed with trunk.
yes go ahead.
Never mind figured it out will be committing patch in the next few days.
"""(If it's just reading ints I was thinking just write 4-byte ints. That's got to be the fastest of all.)  The author would be Ted really his call. If it's more of an example for the book it could be attached to the book. If it stays that's cool too just need to have a think about fixing/documenting the issues raised here.  You've raised a different an interesting point about performance though. You find that the slow-down is actually in addToVector where it converts a String to byte[]? The thing is the corresponding line in the """"fast"""" version skips this step and adds null.  Indeed also passing null in the """"normal"""" version makes it twice as fast for me. It's still twice as slow as the """"fast"""" version though. But I do wonder whether the example deserves a bit more attention. I may not know what I'm doing. Is that a difference that shouldn't exist between the two benchmarks?"""
OK understood. Can you re-create the patch? It's out of sync with head at the moment.
I see what you mean
Just some variations
"""{quote} This wasn't a problem with my patch right?  That was an issue of the mahout script in trunk itself? {quote}  Yes it was a problem with the script in trunk. I believe this was due to the fact that the job files were on the classpath instead of all of the dependency jars. Adding the job files to the classpath does not add the dependency jars they contain to the classpath as well. So no you didn't add this but it should be fixed (and is in the patch)  {quote} What is the -core option for?  I've never used it how does it work? {quote}  when you're running bin/mahout in the context of a build the -core option is used to tell it to use the build classpath instead of the classpath used for a binary release. This just follows the pattern established (by Doug?) in the hadoop and nutch launch scripts.  {quote} Also added a help message for the 'run' argument. {quote}  near line 72 in bin/mahout: (this is different from the --help question I had)  {code}   echo """"  seq2sparse            generate sparse vectors from a sequence file""""   echo """"  vectordump            dump vectors from a sequence file""""   echo """"  run                   run mahout tasks using the MahoutDriver see: http://cwiki.apache.org/MAHOUT/mahoutdriver.html"""" {code}  {quote} So you already added the ability to load via classpath right? If we merge that way of thinking with what I'm currently working on (having a configurable """"MAHOUT_CONF_DIR"""" which is used for all these props files) we could just have the mahout shell script just add MAHOUT_CONF_DIR to the classpath (the way you already have it adding the hardwired core/src/main/resources directory) and then it would work that way. {quote}  Yep that should do it as long as MAHOUT_CONF_DIR appears before src/main/resources we should be good to go. It should be added outside of the section of the script that determines if -core has been specified on the command-line.  """
The inline patch came out garbled. Same patch attached.
It could have been created as a patch and that is far as its gotten.   Assuming that we're talking we're just talking about creating a simple .snk to create strongly named assemblies when talking about signing assembly:  I didn't see a .snk file in the Lucene.Net_2_9_4g branch when setting up the build scripts. I haven't looked in trunk yet.    The Lucene.Net_4e branch should have a already have a snk. I could add the same snk file to trunk and the 2.9.4 branch when I go add the build scripts to the trunk this weekend so that all the branches are building strong assemblies.  Someone would still need to go back to the tag and create a 2.9.2 version using the snk whenever the next release does come out.
Hi Bianco First of all could you add a apache licence to the file VectorHighlightMapper.cs?  Your work is very good and pass all tests but it is not like just a simple bug fix and there is a divergence from FVH java. This makes life hard while making new versions' ports.  All Lucene.Net community! Any idea about what  we should do?  DIGY
Following the recent facets work I think that as part of this issue we should do the following two things (in addition to what I described above):  * Make OrdinalPolicy an enum with two constants (ALL/NO_PARENTS). It's already hard to support just these two (see LUCENE-4610 and LUCENE-4600) and it's not at all clear how to support arbitrary OrdinalPoilcy-ies that users will provide. ** Rather if one really wishes to encode only say levels 3 and below he can extend FacetFields and provide his own CategoryListBuilder. ** For the search side he'll need to provide whatever will work for him (I cannot even describe here what API exactly because of the complexity)  * Nuke PathPolicy. One can extend FacetFields and provide his own DrillDownStream. I'll need to check about that one but it looks like we can get rid of it too.
"""I just glanced through in general: this is similar to the hack patch i used exploring LUCENE-4089 though I just used a Map<StringString>.  The part i didnt like when exploring was more related to how term index/term dictionary are separated: {quote} divisor: generalize this into something simple like a Map<StringString> of codec """"parameters"""" that you set on IWC/IR. split divisor from """"don't load terms index"""". define these as constants where they belong. I got unhappy here in the """"splitting"""" part because I wanted the divisor part in TermsIndexReaderBase but that doesnt extend FieldsProducer (where i wanted the """"don't load"""" part) and wrap the terms dict instead its backwards and terms dict wraps the TermsIndexReaderBase... maybe we should fix that too? I think this confusing the way it is but I didnt look at how difficult this would be. {quote}  But I think maybe I was trying to tackle too much at once... still as an """"untyped"""" parameter I thought it would be useful to fix the semantics all in one break rather than causing confusion down the road. """
I commented out the assertion for this test ... it's not valid.
Patch adds maybeRefreshBlocking which blocks on refreshLock. It also:  * Renames reopenLock to refreshLock * Shares the refresh logic between maybeRefresh and maybeRefreshBlocking. * Switches to use ReentrantLock instead of Semaphore(1): ** It allows to protectively obtain the lock in doMaybeRefresh (see comment in the code) ** It is better in general because it's equivalent to Semaphore(1) yet protects against an accidental bug where someone changes Semaphore(1) to Semaphore(>1).  I'll add a CHANGES entry after the patch is reviewed and we agree on the approach.
"""lemme see if I can help with the test. I feel bad I didn't supply one with the prototype patch.  About the Solr integration: this looks good! We can use a similar approach for autosuggest too so this could configure the analyzer for LUCENE-3842.  I wonder if we should allow separate configuration of """"index"""" and """"query"""" analyzers? I know I came up with some use-cases for that for autosuggest but I'm not sure about spellchecking. I guess it wouldn't be overkill to allow it though."""
"""bq. I assumed modules doesn't need to be self-contained like Lucene or Solr. I can fix that by enforcing tools' compilation in that macro... I'll do that. bq. Steve will you double check again if everything works  Yes everything works for me now thanks!  bq. and commit this in?  Is there some reason why you can't do this?  IMO committers should commit their own work.  bq. I didn't put an entry into CHANGES so you'd have to add it if it qualifies at all to be mentioned there.  IMO this definitely warrants a CHANGES.txt entry.  This should also be backported to branch_3x.  bq. This reminds me of the """"infrastructure tools"""" problem we had in Carrot2. We finally decided to simply have them as a stand-alone project living within the same repository space with a stored versioned binary artefact updated when tools had to be updated (rarely). This does version a binary file but you don't need to worry about recompiling things over and over.  +1 to do this for this and any other Lucene/Solr Ant tasks."""
From the dev list didn't want to lose this background (or make Uwe type it again <G>)  The idea was to maybe replace RAMDirectory by a âcloneâ¿ of MMapDirectory that uses large DirectByteBuffers outside the JVM heap. The current RAMDirectory is very limited (buffersize hardcoded to 8 KB if you have a 50 Gigabyte Index in this RAMDirectory your GC simply drives crazy â we investigated this several times for customers. RAMDirectory was in fact several times slower than a simple disk-based MMapDir). Also the locking on the RAMFile class is horrible as for large indexes you have to change buffer several times when seeking/reading/â¦ which does heavily locking. In contrast MMapDir is completely lock-free!   Until there is no replacement we will not remove it but the current RAMDirectory is not useable for large indexes. Thatâs a limitation and the design of this class does not support anything else. Itâs currently unfixable and instead of putting work into fixing it the time should be spent in working on a new ByteBuffer-based RAMDir with larger blocs/blocs that merge or IOContext helping to calculate the file size before writing it (e.g. when triggering a merge you know the approximate size of the file before so you can allocate a buffer thatâs better than 8 Kilobytes). Also directByteBuffer helps to make GC happy as the RAMdir is outside JVM heap.....  RAMdir uses more time for switching buffers than reading the data. The problem is that MMapDir does not support *writing* and that why we plan to improve this. Have you tried MMapDir for read access in comparison to RAMDirectory for a larger index it outperforms several times (depending on OS and if file data is in FS cache already). The new directory will simply mimic the MMapIndexInput add MMapIndexOutput but not based on a mmapped buffer instead a in-memory (Direct)ByteBuffer (outside or inside JVM heap â both will be supported). This simplifies code a lot.   The discussions about the limitations of crappy RAMDirectory were discussed on conferences sorry. We did *not*decide to remove it (without a patch/replacement). The whole âmessageâ¿ on the issue was that RAMDirectory is a bad idea. The recommended approach at the moment to handle large in-ram directories would be to use a tmpfs on Linux/Solaris and use MMapDir on top (for larger indexes). The MMap would then directly map the RAM of the underlying tmpfs.....
Committed trunk revision: 1206033 Now backporting.
bq. Can we just make it final when we backport this change?   Duh no we can't since we have core IR impls subclassing IR...
Fixed.  3.x didn't have any problems
I don't think there is any sense in this who cares?  We reported this crash to Oracle in plenty of time and the *worse* wrong-results bug has been open since May 13: http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7044738 but Oracle decided not to fix that too.
"""I think what happens is all bugs are initially internal to Oracle and they need to be """"opened up"""" to the public. I presume this is so that critical bug reports don't immediately become available for exploits... but I'm just guessing."""
Committed revision 1165174.
attached a fix
Committed revision 1128830 to trunk. I will now backport to 3x.
Previous patch was wrong. Here a new one.
"""bq. Hmmm infoStream is just for debugging... should we really make it volatile?  I'll remove its volatile...  {quote} bq. IWC cannot be made immutable â you build it up incrementally (new IWC(...).setThis(...).setThat(...)). Its fields cannot be final.  Setters can return modified immutable copy of 'this'. So you get both incremental building and immutability. {quote}  Oh yeah.  But then we'd clone the full IWC on every set... this seems like overkill in the name of """"purity"""".  {quote} What about earlier compromise mentioned by Shay Mark me? Keep setters for 'live' properties on IW. This clearly draws the line and you don't have to consult Javadocs for each and every setting to know if you can change it live or not. {quote}  I really don't like that this approach would split IW configuration into two places.  Like you look at the javadocs for IWC and think that you cannot change the RAM buffer size.  IWC should be the one place you go to see which settings you can change about the IW.  That some of these settings take effect """"live"""" while others do not is really an orthogonal (and I think secondary ie handled fine w/ jdocs) aspect/concern. """
Uwe: I don't interpret it that way!  I don't think our indexinputs should be doing writes!
Committed optimization in trunk revision: 1037077
Ported back to 3.x  Committed revision 991537.
Static? Weren't you against that!?   But if we remove back compat from analyzers why do we need Version? Or is this API bw that we remove?
Committed revision 941607.
Yes I'm continuing along the path Chris started with his first patch mine is coming soon.  Nikola
There is a bug in PayloadNearQuery. If there are multiple top level spans that match the query only the payloads of the first one are retrieved. This patch fixes this bug by iterating over all the top level spans to get the payloads (see 'setFreqCurrentDoc')  > The base explain method can't be abstract. Something like Ah right. This is included in the patch  >The changes don't seem thread safe any more since there are now member variables. It may still be all right but have you looked at this aspect?  I guess that could be said about PayloadTermSpanScorer and PayloadNearSpanScorer too (payloadScore payloadsSeen). As for the PayloadFunction classes they seem lightweight enough to be created with each query. Is there a better pattern?  Peter
fixed to use reader.termDocs(null) for delete check.
+      if (!more) { +        return false
Commited in r823180 thx robert
{quote} Adriano: Can you summarize all your changes in both issues into a changes.txt entry for both branches? {quote}  Hi Uwe yes I can do it I will need to review the entire code again (3x and trunk). I plan to do this during the weekend. But if you want commit Vinicius's code and I commit the changes to changes.txt later
We still haven't coded in the items on the comments above (Jul 1 and 2). I hope to get to it some time soon though but if you'd like to give those a shot go ahead.
Attached patch.  I plan to commit in a day or two.
I'm looking through DateTools now and can't help but want to clean it up some.  One thing I see that is odd is the use of a Calendar in timeToString(longresolution).  The first two lines look like this right now: {code} calInstance.setTimeInMillis(round(time resolution))
Jason  did you get a chanse to try this out? It seems to work fine for me and I plan to pop it in the trunk in a few days. I think I'll have to add a warning of some kind in runtime though as it could slow down the index a bit if the reader is way fragmented.
bq. I did hit the error while I did that but I will verify again.  Ugh.  If that's the case then maybe catch the OOM in the read and fallback to the temp buffer read solution?
Are there any plans to release a 2.4.1? If yes can this fix be included?
Uwe can't you use ExtendedFieldCache.getLongs passing in your own LongParser?
"""bq. you tweaked a couple little things with the standard cache capture state  Actually I think I just moved things around?  EG I made it the StandardTermsDictReader's job to seek the termsIn file I moved docCount """"up"""" and I made a single cache entry.  I think I also removed a few attrs that we didn't need to store... and downgraded skipOffset from long -> int (it's int on trunk)."""
Committed revision 698035.  it's just about having an optional ant property for specifying the full path to svnversion ... i've made the Hudson config changes for Lucene-trunk (add -Dsvnversion.exe=/opt/subversion-current/bin/svnversion) and manually kicked off build#594.  If i missed something re-open and i'll take a look
Just curious how did you solve the split packages issues and invalid symbolic names with some Lucene modules? I can't see it in the parent pom.
Adds docid Field to the index for EnwikiDocMaker
Brings this patch back up to trunk level.
Are there any plans to also publish the new release to the Maven 1 repository on ibiblio.org? We at Jackrabbit still use Maven 1.0.2 as our build tool.
I just remembered one of the reasons why i didn't do this the last time i looked at it: i don't think FunctionQuery has any good unit tests in the Solr code base -- there might be some tests that use the SOlrTestHarness to trigger function queries but they aren't really portable.
Here is a Test case to reproduce it. I launch it on a JDK1.6 and it deadlocks each 6 to 10 run.
"""Created an attachment (id=17581) Patch for build.xml to add index=""""true"""" for jar tasks """
just pending announcement now
I did nothing special. My understanding was that simply trying to test under JDeveloper caused the problem. Your report says removing xmlparserv2 causes JDeveloper not to start.  Is there something else I need to do to cause the problem?  FYI - I created a Maven project with the code above as the only application. I then imported the application which was a bit frustrating as it didn't seem to accomplish much. I then had to manually add the log4j 2 jars into the classpath along with target/classes and had to make a couple of other changes so that it knew how things were laid out in the project. I then ran the application from JDeveloper and it ran fine.
Created an attachment (id=7421) patch to o.a.c.l.SystemUtils.java. Modified declaration order.
We can't upgrade to maven-assembly-plugin 2.3 as it breaks the build on MacOS due to https://jira.codehaus.org/browse/MASSEMBLY-588.
FWIW I didn't experience the problem with Karaf commands when using Fuse ESB (based on Karaf 2.2.5) but I did have problems with Aries Blueprint when I tried to instantiate some of our own beans as an OSGi service.  Workaround for me was to remove the Aries Proxy 0.3 bundle shipped with Fuse ESB and install Aries Proxy 0.3.1. After I did that and restarted the container the problem was resolved for me.
Quick fix to interchange the order of constructor arguments of ClientIdAndTopic.
Thanks for getting started on this. I have a couple of requests -  1. Could you please upload a patch to this JIRA and grant it to Apache ?  2. This patch doesn't apply cleanly to trunk would you mind updating a patch that would ?   It will make it much easier for us to review it.   Also when you said the zk connect option hangs did you get a chance to take a look at the debug/trace logs to see where it hangs ? Do you mind uploading those log files here ?
Hi ShengTao you would think if you follow:  1. Check if the service is there add if it is not. 2. Then add a bindingTemplate.  Then you would not be overwriting a bindingTemplates. You are right though if you are saving a service it will overwrite the existing one and it will remove any bindingTemplates if they are there. The current behavior is how the UDDI v3 spec tells us to so it but we could add a config option to differentiate from the spec.  Please let me know if you are following the above strategy and *still* seeing issues.   --Kurt
Dupe of JUDDI-310.
Fixed eventhough these are commented out.
bundles have been promoted to OSSRH some minutes ago. It will take roughly 2-4 hours before they appear on Central so this one is close to be fixed.
"""I can access the patch deletion from """"Manage attachments"""" right next to the attachment list.  If you can't access it let me know which patches need to be deleted..."""
"""Looks like this problem is bigger than I thought.  I can't even get a simple form without dot-stuffing to work.  Parsing this simple script:  require [""""fileinto"""" """"reject"""" """"tag"""" """"flag""""]"""
It worked fine in Jetspeed 2.2.0 too
"""Added missing 2 *  package.html.  Some files need to be removed (svn del): src/java/org/apache/commons/jexl/Arithmetic.java src/java/org/apache/commons/jexl/parser/JEXLNode.java (*) src/java/org/apache/commons/jexl/util/PropertyExecutor.java src/java/org/apache/commons/jexl/util/BooleanPropertyExecutor.java src/java/org/apache/commons/jexl/util/introspection/UberspectLoggable.java src/java/org/apache/commons/jexl/util/GetExecutor.java  If on windows you really need to remove it before applying the patch otherwise it will just keep using JEXLNode.java for a class named JexlNode...  The patch does not apply cleanly especially UberspectImpl & Introspector. The """"best"""" approach I've found yet is to force apply updates even if patch thinks it may have already been applied. Then for each of them copy/paste the '.rej' content replacing the end of the original file with the content at the proper place"""
[~mbo] yes & yes IMHO.
Committed in revision 1363218.
Ok I'll add these.
fixed. Committed revision 797662.
Stefan  > the said method is not public since it returns *all* workspace names regardless of user rights. please note that > the a session must only see/know about workspaces it is actually allowed to access.   I'm working on a garbage collector for the data store. The GC doesn't care about user rights or putting it another way it must be able to access all workspaces. Ideally I'd like to use a SystemSession but you can't get one outside RepositoryImpl or o.a.j.c package (i.e. getSystemSession(String) is package private WorkspaceInfo is protected). I understand that these APIs shouldn't be public but they're useful from within the core. In my case for the GC. I'm very interested in using another way to walk every workspace but this other way should guarantee that the repository won't shutdown during a walk operation. I'll check the solution Thomas said he'll be proposing but in the mean time that is what I'm using. Still I don't see why this APIs aren't public. Regards
Hi  We are using Jackrabbit to store/retrieve the content in my application.  Please find the attached config files of my application.    Best Regards RK OCS L3 Support +91 44 39853531 +91 98408 51525.
Please re-open if necessary.
well the above ascii art did not really work in jira :-( will attach a file.
Martin wrote: > Well first of all sorry if I offended someon putting the issue because it seems that I got hard responses.  You don't have to be sorry. I'm not against building a DB PM that uses a connection pool. I just had (and still have) doubts how this contributes to increased concurrency because concurrency is mostly controlled outside of the PM.  It wasn't my intention to criticise anyone on a personal level.
Fixed in SVN.  Will let Vincenzo close it if he is satisfied.
Please submit an diff against the v2.3.1 branch.
"""I did NOT want to add a """"framework"""" but rather *exactly* the opposite. """
[INFO] Repository Target 'org.apache.tez' is created. [INFO] Staging Profile 'org.apache.tez' is created. [INFO] Privileges (CRU) 'org.apache.tez - Public Repositories' are created. [INFO] Privileges (CRUD) 'org.apache.tez - Snapshots' are created. [INFO] Role 'org.apache.tez Deployment Role' is created. [INFO] Role 'tez' is added to user 'acmurthy'. [INFO] Role 'tez' is added to user 'billgraham'. [INFO] Role 'tez' is added to user 'bikas'. [INFO] Role 'tez' is added to user 'bobby'. [INFO] Role 'tez' is added to user 'cdouglas'. [INFO] Role 'tez' is added to user 'daryn'. [INFO] Role 'tez' is added to user 'ddas'. [INFO] Role 'tez' is added to user 'gates'. [INFO] Role 'tez' is added to user 'gopalv'. [INFO] Role 'tez' is added to user 'gunther'. [INFO] Role 'tez' is added to user 'hitesh'. [INFO] Role 'tez' is added to user 'hashutosh'. [INFO] Role 'tez' is added to user 'jghoman'. [INFO] Role 'tez' is added to user 'jitendra'. [INFO] Role 'tez' is added to user 'jlowe'. [INFO] Role 'tez' is added to user 'julien'. [INFO] Role 'tez' is added to user 'kevinwilfong'. [INFO] Role 'tez' is added to user 'mattmann'. [INFO] Role 'tez' is added to user 'mliddell'. [INFO] Role 'tez' is added to user 'namit'. [INFO] Role 'tez' is added to user 'nroberts'. [INFO] Role 'tez' is added to user 'omalley'. [INFO] Role 'tez' is added to user 'sseth'. [INFO] Role 'tez' is added to user 'tgraves'. [INFO] Role 'tez' is added to user 'tomwhite'. [INFO] Role 'tez' is added to user 'vikram'. [INFO] Role 'tez' is added to user 'vinodkv'. -- Nexus repository was prepared successfully. --  Configuration has been prepared now you can: * Deploy snapshot artifacts into repository https://repository.apache.org/content/repositories/snapshots * Deploy release artifacts into the staging repository https://repository.apache.org/service/local/staging/deploy/maven2 * Promote staged artifacts into repository 'Releases' * Download snapshot and release artifacts from group https://repository.apache.org/content/groups/public * Download snapshot release and staged artifacts from staging group https://repository.apache.org/content/groups/staging
<danielsh> yes we're aware of the recurrence
Can put the info in this file on people: -rwx------  1 dblevins  dblevins  0 Jul 12 21:00 /home/dblevins/INFRA-5044
<pctony> This has been done.  It should work just fine I'll be doing some more work on JIRA later and will force a re-index then so this might look better after that. Have a look around.
Duplicate of INFRA-4886
Two more IDs for initial access: greid and brock.
Added to all-developers group which has this permission.
dist dirs created also.
Created the DTACLOUD wiki space.
This was taken care of already.
commits@mina created.
I've upped the httpd timeout from 5 mins to 30. I didn't see any OutOfMemoryErrors in the logs so hopefully this will fix it. Please give it another try.
Hi Arpana
I believe this is a duplicate of HTTPCORE-328  Oleg
Olaf  Please also consider submitting a test case for this issue. This will increase the chances of the patch getting committed sooner rather then later  Oleg
Oleg  I agree it does seem that the presence of the Proxy-Authorization header on the already authenticated connection causes re-authentication.  So for NTLM to work the proxy-auth header must be removed once authenticated.  I don't think either patch 2 or 3 handles this case unless (in the case of patch 2) the host also uses NTLM.  What we really need is a way to remove NTLM headers after the authentication has succeeded.  Mike
Created an attachment (id=6644) SSL Guide (take 1)
[~brocknoland] Did this not patch?
The patch is generated against the latest trunk.
We did agree to produce a hive release in less then 3 months that did not use the NPath trademark. This jira is to specifically address that.
This patch effectively reverts the HIVE-4688 change. The NPE is fixed in VectorizedRowBatch by HIVE-4758.
Brock commented two test failed.   org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testNegativeCliDriver_mapreduce_stack_trace_hadoop20 org.apache.hcatalog.fileformats.TestOrcDynamicPartitioned.testHCatDynamicPartitionedTable  Can you assert that? I cannot reproduce.
Correction to the design of this feature (I can't edit comments because of permissions so adding another comment). In case the seconds field needs more than 31 bit the first VInt is {{-1-reversedDecimal}} regardless of whether {{reversedDecimal}} is zero or not.
Dupe of HIVE-3869
Fixed for comments added new patch.
This bug is a duplicate of HIVE-3253.
Marking as open since looks like it needs some more work.
yes
"""kevinwilfong updated the revision """"HIVE-2797 [jira] Make the IP address of a Thrift client available to HMSHandler."""". Reviewers: JIRA njain ashutoshc    Ran updated my checkout.  REVISION DETAIL   https://reviews.facebook.net/D1701  AFFECTED FILES   shims/src/common/java/org/apache/hadoop/hive/thrift/TUGIContainingTransport.java   metastore/src/test/org/apache/hadoop/hive/metastore/TestRemoteUGIHiveMetaStoreIpAddress.java   metastore/src/test/org/apache/hadoop/hive/metastore/IpAddressListener.java   metastore/src/test/org/apache/hadoop/hive/metastore/TestRemoteHiveMetaStoreIpAddress.java   metastore/src/java/org/apache/hadoop/hive/metastore/TUGIBasedProcessor.java   metastore/src/java/org/apache/hadoop/hive/metastore/TSetIpAddressProcessor.java   metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java """
Hi  Can we bump this please? I just came up against this issue wrote the patch was about to submit it only to find it already here...  Cheers.
Made some more progress with kryo plan serialization. Check-pointing WIP patch.
Maven I am on the fence about it. We actually do not need all the libs I included. Having them in a tarball sounds good but making a maven repo for only this purpose seems to be a lot of work.  {quote} Should we attempt to factor out the HBase commonality immediately or commit the overlapping code and then do refactoring as a followup? I'm fine either way
When I ran make-maven target build failed with following error: {noformat} BUILD FAILED /Users/amarsri/hive/build.xml:706: Warning: Could not find file /Users/amarsri/hive/build/ql/pom.xml to copy. {noformat}  I don't see any pom files created in build directory.
Jerome I did a skeleton of the code to use HashMap. Do you want to start from there and add what is missing?
the mirror.facebook.com/hive site was already taken offlne.
yes- this is HDFS-4866 releated. I've added reference.
Also {noformat} +        NetUtils.getDefaultSocketFactory(conf) +        org.apache.hadoop.ipc.Client.getTimeout(conf) defaultPolicy).getProxy()
critical typo above... should have written:  These *idempotent* calls serve to find the correct active NN and then the following non-idempotent ones succeed
On second thought I removed the patch. This would just be working around the problem. I could live with a verdict that this dev environment is broken in some way that I'm just not seeing. Or there is another option that avoids what looks like an enumeration over local addresses for reverse lookups (including virbr networks). Since this is all localhost testing mocking the resolver is not unreasonable.
One more comment this should be in the incompatible section in CHANGES.txt.
Due to the java compiler bugs Jing mentioned I have reverted the commit.
This actually depends on HDFS-3863 so not committing until that one is in.
Colin the approach and your patch looks good to me.  What's the latest on testing?  Eg you verified a v1 install with hsync files correctly upgraded (eg leases recovered) to v2 or trunk build with this patch?   Nicholas do you have cycles to take a look? I spent a while paging in the relevant code and the patch/testing look good to me but would be good to have another set of eyes. The patch itself is straight-forward so mostly around subtleties.
There is no close() in FileContext.  Users are not supposed to close FileContext.
Hi colin I agree that scatter/gather typically refers to multiple input tuples of (position length). Yes we can extend the api to include that.  The reason my original proposal did not include that was because I was mostly targetting this api to reduce the number of buffer copies.
"""- Mind chiming in on the rationale of the setQuota vs setSpaceQuota naming (ie the former for namespace)? - Agree w Nicholas that we should move setQuota from DFS to this admin class let's do that in a follow on change  There's an extra """"/**"""" in the setQ"""
Yes for create/append
Attached patch
looks like the test-patch build ran before HADOOP-7753 was integrated in the snapshot build. I'll wait half an hour or so and retrigger this test-patch. Meanwhile should be good to review.
Reopened for trunk fix.
Duplicate of HDFS-2321
Remove from 0.20.205 in hopes that HDFS-2342 did indeed fix TestHdfsProxy.testHdfsProxyInterface.
The TestFileConcurrentReader failure seems to be happening on a lot of builds. It's caused by the process running out of file handles. Maybe ulimit needs to be changed on the hudson server.
Hi Eli didn't realize you were going to look at TestDatanodeBlockScanner too.  I did extensive mods to it as part of HDFS-1295.  Let's take a look and compare.
Here's a potential fix for this. close() and hflush() now may throw InterruptedIOException if the thread has interrupt status. It's a bit tricky because they won't *always* throw it - if they weren't going to block they will return immediately (with interrupt status preserved)
Todd I briefly looked at the patch. It looks like you are trying to get rid of the Journal Spool in BN. Correct me if I am wrong. I don't think you can. BN makes a checkpoint from its memory state which differs it from SNN and CN. While it does it the namespace should be locked (for modifications) so the edits go into journal spool which is reapplied to memory after the checkpoint is finished. Please see the design doc in HADOOP-4539.
submit patch
Attaching a patch that adds javadoc in the FileSystem class.
hi dhruba borthakur   I am using the 20.1 version.. I put NameNode in safemode and then i executed the save namespaceBut editlogs are not corrupted.. can u please give exact scenario to reproduce this.
No this is not the right fix. HDFS can *not* depend on MapReduce. Please remove the dependence completely.
Latest version still needs hudson right?
Created HADOOP-6281.
TestBlockTokenWithDFS fails for the same reason.
"""could you also remove """">>>>>>>>>>>>"""" from line 1504 """
"""# In DataStorage.linkAllBlocks() you commented our the {{toDir.mkdir()}} part. You probably wanted to remove it. # I would avoid all spelling formatting commenting empty line changes because they will make merging hard. # Don't think introducing {{FINALIZED_DIR_NAME}} in {{MiniDFSCluster}} is a good idea. It is better than what we have now - a plain string {{current}}. {{DataStorage}} is a public class we should rather add a public static method to it {{getFinalizedDirectoryName()}}. Then """"finalized"""" will defined only in one place."""
The new patch uses fs.makeQualified(path) as suggested.
this patch fixes the dir permissions
HBASE-10079 now looks like a race in the tablename caching. Not related to the locking here.
@JMS & @stack  Tks for your review and commtents my reply as follows... bq.Have you tried it? Does it works for you? Yes I've tested it in our env. and works well {quote} Is this required? {code} executor = null
[~sershe] sorry for late reply. I upload the patch to https://reviews.facebook.net/D13341. Could you help to review it ? thx.
Attached v2.2 rebased to trunk latest.
This looks like it needs rebased onto trunk.  The posted patch includes generated code ({{AdminProtos.java}}) please remove.  Please add javadoc for the new classes and new public methods on existing classes.  {{FavoredNodeAssignmentHelper#placeSecondaryAndTertiaryWithRestrictions}} introduces a {{while(true)}} block containing non-trivial branching logic. Can this be refactored with simpler and/or clearer break conditions?  {{FSUtils}} introduces new configuration {{hbase.client.localityCheck.threadPoolSize}}. Can this be added to {{HConstants}} and/or documented in {{hbase-defaults.xml}}?
bq.It handles both meta is not assigned and it is double assigned. Yes
The same appears to be try for case_studies.xml community.xml and zookeeper.xml. Should those not be copied for some other reason?
Patch attached.
Yes 70x indicates something amiss.
Yes that's right.
Hi Lars sorry about the delay in responding. Good point about the TreeMap always returning result in a deterministic ordering. The patch itself I think is still beneficial because it removes a little bit of redundancy in the existing equality code. IMO the fix makes it easier to understand as well. In addition the tests are beneficial since we were missing coverage there.  I think it'd be better if the fix were in sooner but it's probably not an issue if we moved it to 0.94.7.
v4 Patch does not apply snapshot branch ff25969ea0b2ff38cc25512a02569c351e757d66  Looks like this is built upon some of the online snaphots work.  What branch/hash does this go on Matteo?
From step 3 do you think the double-splitting is a new phenomenon?  It doesn't sound like something that should happen very often.  Maybe that would explain why i didn't get this error in .94.0.  Also please note I went straight from .94.0 to .94.2 so I don't know if it was present in .94.1.
The patch here seems to be attached to a wrong jira. Sorry about the confusion. I have re-attached the path here.
hmm looks like I didn't save all java files after mass-replace of whitespace. I will attach another patch nothing of significance will change
Thanks Elliot! Did you forget to add FailedSanityCheckException to the 0.94 patch? If the same as in 0.96 I'll just add it at commit.  Will the new exception cause compatibility issues in old 0.92.x and 0.94.{0|1} clients? I don't think so... Just making sure.
Attaching a patch with the typo fixed. I'm going to try that along HBASE-6550 and HBASE-6165 now.
Assigning to Sameeer since apparently he can't assign to himself.
@Marcelo I was thinking these changes:  {code} -    public static AuthResult allow(String reason User user Permission.Action action byte[] table) { -      return new AuthResult(true reason user action table null null)
Looks like this broke imports into Intellij
@Andy Thanks for pointing it out.  We did  not take the latest patch by mistake and thought this change was missing.  Sorry about that. But still i think this JIRA we can use to solve  {code} org.apache.hadoop.hbase.mapreduce.TestImportExport.testSimpleCa
What I'm committing... wraps a very long line else what Matteo suppied.
@Lars I don't think it would be all that different. I'll take a crack next week (after dealing with the next round of HBASE-6055 stuff).
----------------------------------------------------------- This is an automatically generated e-mail. To reply visit: https://reviews.apache.org/r/4054/#review5511 -----------------------------------------------------------    pom.xml <https://reviews.apache.org/r/4054/#comment11930>      So all proto generated files go to proto/generated?  All into the same package?  Thanks Jimmy.          Also mind checking out Deveraj's patch?  I'd suggest at least reviewing it to figure if you fellas are using same conventions going all proto.          Good stuff   - Michael   On 2012-02-27 18:54:31 Jimmy Xiang wrote: bq.   bq.  ----------------------------------------------------------- bq.  This is an automatically generated e-mail. To reply visit: bq.  https://reviews.apache.org/r/4054/ bq.  ----------------------------------------------------------- bq.   bq.  (Updated 2012-02-27 18:54:31) bq.   bq.   bq.  Review request for hbase. bq.   bq.   bq.  Summary bq.  ------- bq.   bq.  This is the first draft of the ProtoBuff HRegionProtocol.  The corresponding java vs pb method mapping is attached to the jira: https://issues.apache.org/jira/browse/HBASE-5443 bq.   bq.  Please review.  I'd like to move ahead after we get to some agreement. bq.   bq.   bq.  This addresses bug HBASE-5443. bq.      https://issues.apache.org/jira/browse/HBASE-5443 bq.   bq.   bq.  Diffs bq.  ----- bq.   bq.    pom.xml 066c027  bq.    src/main/proto/HRegionProtocol.proto PRE-CREATION  bq.    src/main/proto/hbase.proto PRE-CREATION  bq.   bq.  Diff: https://reviews.apache.org/r/4054/diff bq.   bq.   bq.  Testing bq.  ------- bq.   bq.   bq.  Thanks bq.   bq.  Jimmy bq.   bq.
Attaching the latest version of the patch from review. Incorporates a trivial javadoc fix as suggested.
----------------------------------------------------------- This is an automatically generated e-mail. To reply visit: https://reviews.apache.org/r/3401/#review4382 -----------------------------------------------------------   Looks good. Does it overwhelm the status display in the UI?  - Lars   On 2012-01-06 03:06:26 Andrew Purtell wrote: bq.   bq.  ----------------------------------------------------------- bq.  This is an automatically generated e-mail. To reply visit: bq.  https://reviews.apache.org/r/3401/ bq.  ----------------------------------------------------------- bq.   bq.  (Updated 2012-01-06 03:06:26) bq.   bq.   bq.  Review request for hbase and Michael Stack. bq.   bq.   bq.  Summary bq.  ------- bq.   bq.  Consider exposing master event handler state as monitored tasks. bq.   bq.   bq.  This addresses bug HBASE-5132. bq.      https://issues.apache.org/jira/browse/HBASE-5132 bq.   bq.   bq.  Diffs bq.  ----- bq.   bq.    src/main/java/org/apache/hadoop/hbase/master/handler/ClosedRegionHandler.java 2dfc3e7  bq.    src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java 5af0690  bq.    src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java 78bb5bf  bq.    src/main/java/org/apache/hadoop/hbase/master/handler/OpenedRegionHandler.java f171a5a  bq.    src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java 2dd497b  bq.    src/main/java/org/apache/hadoop/hbase/master/handler/SplitRegionHandler.java 2d544dd  bq.    src/main/java/org/apache/hadoop/hbase/master/handler/TableEventHandler.java 76c98b3  bq.   bq.  Diff: https://reviews.apache.org/r/3401/diff bq.   bq.   bq.  Testing bq.  ------- bq.   bq.  None yet. bq.   bq.   bq.  Thanks bq.   bq.  Andrew bq.   bq.
See HADOOP-7853 Andy needs a little more time in deciding whether HADOOP-7853 is good enough replacement for 7070.
Created recovery mechanism jira at HBASE-4652
bq. So we can add isAlreadyExpiring() and if true  Sorry it should be  So we can add isAlreadyExpiring() and if flase we can go with expireIfOnline
Will commit it today evening.
Committed by Ted today.
I cannot reproduce this bug on my test machine. Can you produce it reliably? Maybe we should revert HBASE-3899 until the time we can fix it?
Yeah can't cache KV.  Can we have something for one server first?
First draft. Refactor code to prepare for work mentioned above. TestLoadIncrementalHFiles passes.
add 1 more reference overhead in the HRegion. Update the patch.
I committed this change.  Thanks for reporting the issue Kazuki:  {code} Index: src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java =================================================================== --- src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java        (revision 1074909) +++ src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java        (working copy) @@ -3007 +3006 @@        // initialize zookeeper and master address manager        this.zooKeeper = getZooKeeperWatcher()
It'd work but it's a halfway hack to the real fix we should do in 0.92.  My points is that I think it is a good idea in general to raise this timeout (while adding configs so you don't necessarily have to wait since smallish clusters could use the max param if they want to speed this up)... we have run into the situation where not all RS came in the 3 second window (which really is too small given heartbeat frequency) so I've already ramped up the config timeout for fb production.  But I'm fine implementing the halfway fix if my argument still doesn't win you over :)
Fixed in HBASE-2156 after it was reopened.
Bringing this in the latest 0.89
So there is another thing here by deferring the evictions to the background thread you increase the chance that the block will be tenured then it must be collected from CMS space thus radically increasing the GC complexity of the cache.  Originally when the LRU block cache came in it made my collections 10x as slow until I disabled the block cache for compactions.  Unfortunately all these micro-optimizations might be falling under the GC wayside.
er take two uploaded wrong thing.
This patch removes tohtml.xsl and adds configuration_to_docbook_section.xsl.  The latter generates a docbook section from our hbase-default.xml using maven-xml-plugin that runs before the docbook plugin.  Our book then xincludes the generated file (the patch includes this modification too).  Patch includes a edit of our book which adds a preface an index an appendix that talks of configuarion another on compression etc.
Add this in once 0.20.2 clears.
Haven't you done this N?
Committed a few days ago.
Commenting it out in src/test/log4j.properties isn't enough. Setting it to ERROR works though.
Some tests I did with a modified PE. I ran the PE with a number from 10 to 3000 and incremented by 10. The garbage collector was called after each run. Here are the results:  {quote} 08/09/17 16:48:54 INFO hbase.ManyPerformanceEvaluation: 10 42725 08/09/17 16:49:31 INFO hbase.ManyPerformanceEvaluation: 20 36744 08/09/17 16:50:06 INFO hbase.ManyPerformanceEvaluation: 30 34928 08/09/17 16:50:40 INFO hbase.ManyPerformanceEvaluation: 40 34150 08/09/17 16:51:14 INFO hbase.ManyPerformanceEvaluation: 50 33664 08/09/17 16:51:47 INFO hbase.ManyPerformanceEvaluation: 60 33570 08/09/17 16:52:21 INFO hbase.ManyPerformanceEvaluation: 70 33142 08/09/17 16:52:54 INFO hbase.ManyPerformanceEvaluation: 80 32901 08/09/17 16:53:26 INFO hbase.ManyPerformanceEvaluation: 90 32864 08/09/17 16:53:59 INFO hbase.ManyPerformanceEvaluation: 100 32776 08/09/17 16:54:32 INFO hbase.ManyPerformanceEvaluation: 110 32725 08/09/17 16:55:05 INFO hbase.ManyPerformanceEvaluation: 120 32747 08/09/17 16:55:38 INFO hbase.ManyPerformanceEvaluation: 130 32698 08/09/17 16:56:10 INFO hbase.ManyPerformanceEvaluation: 140 32693 08/09/17 16:56:43 INFO hbase.ManyPerformanceEvaluation: 150 32643 08/09/17 16:57:16 INFO hbase.ManyPerformanceEvaluation: 160 32686 08/09/17 16:57:48 INFO hbase.ManyPerformanceEvaluation: 170 32665 08/09/17 16:58:21 INFO hbase.ManyPerformanceEvaluation: 180 32448 08/09/17 16:58:53 INFO hbase.ManyPerformanceEvaluation: 190 32004 08/09/17 16:59:25 INFO hbase.ManyPerformanceEvaluation: 200 31985 08/09/17 16:59:57 INFO hbase.ManyPerformanceEvaluation: 210 31964 08/09/17 17:00:29 INFO hbase.ManyPerformanceEvaluation: 220 31983 08/09/17 17:01:01 INFO hbase.ManyPerformanceEvaluation: 230 31778 08/09/17 17:01:32 INFO hbase.ManyPerformanceEvaluation: 240 31766 08/09/17 17:02:04 INFO hbase.ManyPerformanceEvaluation: 250 31886 08/09/17 17:02:36 INFO hbase.ManyPerformanceEvaluation: 260 31773 08/09/17 17:03:08 INFO hbase.ManyPerformanceEvaluation: 270 31709 08/09/17 17:03:40 INFO hbase.ManyPerformanceEvaluation: 280 31669 08/09/17 17:04:11 INFO hbase.ManyPerformanceEvaluation: 290 31647 08/09/17 17:04:43 INFO hbase.ManyPerformanceEvaluation: 300 31634 08/09/17 17:05:15 INFO hbase.ManyPerformanceEvaluation: 310 31674 08/09/17 17:05:47 INFO hbase.ManyPerformanceEvaluation: 320 31935 08/09/17 17:06:18 INFO hbase.ManyPerformanceEvaluation: 330 31802 08/09/17 17:06:50 INFO hbase.ManyPerformanceEvaluation: 340 31571 08/09/17 17:07:21 INFO hbase.ManyPerformanceEvaluation: 350 31484 08/09/17 17:07:53 INFO hbase.ManyPerformanceEvaluation: 360 31713 08/09/17 17:08:25 INFO hbase.ManyPerformanceEvaluation: 370 31512 08/09/17 17:08:56 INFO hbase.ManyPerformanceEvaluation: 380 31456 08/09/17 17:09:28 INFO hbase.ManyPerformanceEvaluation: 390 32055 08/09/17 17:10:01 INFO hbase.ManyPerformanceEvaluation: 400 32260 08/09/17 17:10:33 INFO hbase.ManyPerformanceEvaluation: 410 32167 08/09/17 17:11:05 INFO hbase.ManyPerformanceEvaluation: 420 31962 08/09/17 17:11:37 INFO hbase.ManyPerformanceEvaluation: 430 32003 08/09/17 17:12:09 INFO hbase.ManyPerformanceEvaluation: 440 32054 08/09/17 17:12:41 INFO hbase.ManyPerformanceEvaluation: 450 32105 08/09/17 17:13:13 INFO hbase.ManyPerformanceEvaluation: 460 32126 08/09/17 17:13:45 INFO hbase.ManyPerformanceEvaluation: 470 31961 08/09/17 17:14:17 INFO hbase.ManyPerformanceEvaluation: 480 31968 ... {quote}  It seems that 2000 was an overkill. The gain is impressive between 1 and 10 then around 300 we pretty much reduced the RPC call at it's smallest value.
Just as we did in HBASE-770 need to check our hbaserpc to make sure it has any changes from hadoop included when we move to 0.18 hadoop jars.
We dropped check for references
I'll add in the jar then.  Tim that dslkit is GPL so I can't check it in (Looks good though... )
hudson was hung just now for 7 hours in TestHRegion.  TestHRegion is unorthodox -- intentionally -- in that it doesn't use inherit from HBaseClusterTestCase so it didn't pick up the changes made w/ mhbc.patch.  Hopefully its this anomaly that was responsible for the hang.  Attaching a patch that has TestHRegion do like the rest of the hbase (and hadoop dfs) tests shutting down the filesystem first and then the dfs cluster.
Yes bytes + comparator would work fine for jaql.  Preferably it would be the same comparators that are already used by the sort code which work on serialized bytes (but also on WritableComparables so that might be a problem).  If these comparators are inconsistent then the sort in map/reduce will not work properly.  I think it is ok to document the transitivity requirement but I don't see how you can enforce it in any way.  Different key types are fine as long as the comparator knows how to handle them.  If you can't reuse the same comparator interface I can always implement my comparisons in another place but I already have two of them (one in the WritableComparable and one in the Comparator).  I don't see how column families and the key comparator are related except that they are both table-level metadata. Yes I agree that a single comparator would be used for the entire table.
What's the status of this issue? Is it still alive? The latest comments seem to indicate it was a DNS misconfiguration problem. Stack could that have been the cause of your experience with the issue?
'count' is protected by 'lock' also extract() and removeAt()  and before invoking extract() and removeAt() there has check to make sure 'count' not zero. So I can't see any chance that 'count' could be invalidate in extrac() and removeAt().
Patch 003 for removing the getPlatformPath code was applied at repo revision r818803.
Address the issue of extracting runtime library info from asm module.
Committed revision 648214 for IPF fix.
Stepan  Sorry I didn't see you comment until I'd committed the change - Sian tested it for me.  I've no idea what the msvcr71.dll issue is - but I don't think that is anything to do with my changes.  The fetch-depends/checksum issue for icu-3.4.zip should be fixed with my patch.
The fix is attached.
committed revision r580334  regression test added.
I don't understand why we need these methods since they are inherited from InetAddress so we are compliant with the spec.
Thanks Sian.  Patch applied in SECURITY modulke and third party notices file at repo revision r543463.  Please check it was applied as you expected.
Wrong patch sorry.
VM behaves like BEA but not as RI. It seems that RI checks method name 2 times and 2 times runs through constant pool. I suppose that first time it runs through constant pool just after parsing and second time before verifying. The first check is obligatory and the second only if -Xverify is on. It looks like code is duplicated. Now our vm makes check after parsing and only if -Xverify is on.   I suppose to leave it as it is or make method name check independently of -Xverify flag.
Missed 2 new files - sorry !
Attached fix patch.
Aleksey is the problem you mentioned is caused by this patch? If yes I can fix it and submit new patch ASAP.
I created a regression test for HARMONY-1654 and attached it to HARMONY-3522.
Mark  Alexey's right the original fix was invalid. Please undo all the patches. Sorry.
"""Sorry I clicked the """"submit"""" too quickly the revision number is r501311."""
I'm trying to conditionally hook this into the build so we can get it into SVN and go from there
Oops it is my fault. Please defer this patch I'll raise jira for other classes/interface as its dependance soon sorry for inconvenience.
Verified by Andrew.
Anton and Spark updated patch applied at revision r447348 thanks a lot for this enhancement please verify that the problem is fully fixed as you expected.
Verified by Oliver.
Andrey  I don't quite understand this patch.  For instance you didn't mention bcprov above but you remove some references to it in build/make/components/extra.xml and build/make/deploy.xml and confusingly the patch doesn't remove build/make/components/extra/bcprov.xml ?  This seems a little inconsistent.  Perhaps I'm missing something.
applied in revision 411104 Paulex please verify it resolves the issue
Sorry that I think I missed some discussion in the mailing list. The implementation and test and BreakIterator is attached.
Tim I have no objection. Let's wait.
Attaching JUnit testcase so I don't loose it.
Oh .. clearOutgoingQueues() and put() seems access to localQueueForNextIteration concurrently.
Now with try/catch and debug level log output.  {noformat} 2011-10-23 16:10:49699 INFO  bsp.YARNBSPJob (YARNBSPJob.java:waitForCompletion(275)) - Job succeeded! {noformat}
And Could you please explain detail?
Local test passed. Submit to hudson.
Submit to hudson.
This was my accident.  I had the wrong JIRA opened.  This should have been filed against Apache Drill.  Opened DRILL-146 instead.  Sorry
This is default for staging in hadoop 1  <property>   <name>mapreduce.jobtracker.staging.root.dir</name>   <value>${hadoop.tmp.dir}/mapred/staging</value>   <description>The root of the staging area for users' job files   In practice this should be the directory where users' home    directories are located (usually /user)   </description> </property>
Reopened for submission to branch-1.2 per [~jlowe].
Did you mean to reference HADOOP-8562?  HADOOP-8952 was never committed although apparently a typo during the checkin of HADOOP-8562 accidentally referenced it.
I believe this is fixed by HADOOP-9984
Bikas -is there any reason to not make the {{close()}} operation in {{unpackEntries()}} part of the try/finally logic? Other than that it looks good to be -the {{Shell.WINDOWS}} checks ensure that the Unix untars will be the existing shell commands and not break anything
I attached a patch to add one test for plaintext.
Attach a screenshot of Local Security Policy console (secpol.msc) where users can add symbolic links privilege to their accounts.
We'll get it fixed soon sorry for the inconvenience. I just wanted to point you (and others who may land here) to alternative resources meanwhile :)
Patch that adds this in along with some extra whitespace removal from that chunk of the javadoc.
Good catch sorry about that. I had run the tests but I guess I didn't re-run on the very latest rev against latest trunk. New patch fixes the NPEs.
> If it's windows and it's a local filesystem URI do the os-specific subbing.  You can't always tell whether it's a local filesystem URI.  Lots of relative paths are constructed that do not contain a scheme.
It is not needed in trunk. Trunk will be taken care of in HADOOP-7853.
attached a patch for branch 20 security.
this patch adds rule for hm and rs to the core-site template
I logged a new ticket HADOOP-9280 as this is critical problem for people trying to upgrade from 0.2 to 1.x
@Owen - I'm working on getting Harmony working on Hadoop at present. We're pretty close and have workarounds for most issues but the trick is to fix the issues in such a way that we can create a patch with a resolution that works for all the JREs.   Yes
Perhaps we should have a filesystem-specific method that indicates whether a path is fully-qualified?
canceling patch to get submit for hudson again.
Duplicate of HADOOP-3906. Plz comment there.
5226_20090212.patch: added license headers.
Setting it to Major as the case seems to be random. Please comment if you disagree.
Submitting to Hudson again. It didn't run the first time.
Marking this as won't fix.
"""Hi. Yes perhaps I got a bit trigger happy. Although I did get a +1 from another committer and I assumed that the """"submit patch to hudson"""" wouldn't be able to pick up on the renaming of the test using patch.sh and would fail. Perhaps there is a better way to do this which I am not aware of?  Glad to get feedback though need to properly get the hang of the process"""
+11 Group (including QA) agrees this is not something for 17.
Duplicate of HADOOP-3000.
"""There are two sections of the diff that don't seem related: {noformat} @@ -1787 +1846 @@      <mkdir dir=""""${build.webapps}/job/WEB-INF""""/>      <mkdir dir=""""${build.webapps}/dfs/WEB-INF""""/>      <mkdir dir=""""${build.webapps}/datanode/WEB-INF""""/> -    <mkdir dir=""""${build.webapps}/history/WEB-INF""""/>      <mkdir dir=""""${build.examples}""""/>      <mkdir dir=""""${build.anttasks}""""/>      <mkdir dir=""""${build.dir}/c++""""/> @@ -27413 +2796 @@      </jsp-compile>        <jsp-compile -     uriroot=""""${src.webapps}/history"""" -     outputdir=""""${build.src}"""" -     package=""""org.apache.hadoop.mapred"""" -     webxml=""""${build.webapps}/history/WEB-INF/web.xml""""> -    </jsp-compile> - -    <jsp-compile       uriroot=""""${src.webapps}/datanode""""       outputdir=""""${build.src}""""       package=""""org.apache.hadoop.dfs"""" {noformat}  This was part of HADOOP-2901 which was recently committed"""
2633_20080122.patch: rewrite TestFsck so that MiniDFSCluster is shutdown properly between each test.
0.14.3 is built with Java 1.5.
That patch file didn't apply cleanly.  Here's a version that does.
First cut at a patch for bin/hadoop
> Do you think attached patch is generic?  Nothing is generic until it's used in more than one place.  The above isn't complete (it needs e.g. 'read()' and 'seek()' implementations) but it looks like a good start.  But the real question is to you: could you use the above?  Can you present the data and checksums as input streams that support read(byte[] int int) and seek(long)?  It also assumes a particular checksum implementation CRC32.  If we wish to allow for others that aspect could be generalized by adding a codec-like interface for checksummers.  But I think we should probably skip that this iteration.
This new patch incorporates a few of Hairong's comments.  1. There is no need to check LOG/isInfoEnabled() because logs are typically at INFO level anyway. 2. Change the comment in addStoredBlock() on why we log when safemode is off.
Sorry I didn't look at the package thoroughly enough. I now see that it can be used in the non-RPC cases also.
Hadoop-475 is for making reduce value iterators cloneable.  Hadoop-531 is for sorting on multiple keys
+1 with the read-ahead removal.
I just committed this.  I changed it so that Mapper and Reducer extend a Closeable interface rather than both providing a close() method directly.
This is my fault didn't try other profiles yarn was broken as well. Works now.
Claudio: https://reviews.apache.org/r/5972/  I'm trying to create a new one so that I can keep it updated. Now I get a new kind of error: it complains it can't find SimpleVertex.java on the server but that's because the patch creates it!  Avery did you recreate the patch before submitting?
this patch is for /branches/2.0.2/plugins/org.apache.geronimo.st.v11.ui/src/org/apache/geronimo/st/v11/ui/sections/CommonGeneralSection.javathanks.
Sorry for the patch error. I have created a patch both for 562 and 564and attached it in 564.
fix the launch console message
Yes exactly -- sorry to be so unclear. Just really wondering/hoping you'd be interested in working on these ?? I'd like to get them cleaned up because there are now so many that other more important and/or meaningful warnings are likely to be ignored.
Reopening this to keep track of progress in WTP's Bug 200715.
any progress on this issue?
Move to wish list since we won't include this in the upcoming G 2.1.5 release.
Ivan could not reproduce this on the latest build so closing.
Changes ported to branches/2.1 in rev 656740.
Ported to branches/2.1 in rev 630315 along with GERONIMO-3840
m2 layout
"""Keep in mind this is to support """"development"""" scenarios  only.  If a project in an IDE references jars in lets say a maven repo.  We need the ability to add individual entries to the shard lib without having to copy each individual jar over to the shared library.    So the answer to your first question is no this is not the same there is no way to add invidivial entries.  I don't quite understand your second bullet.   Third bullet true just thought it would be nice to have an empty template.  Yes agree a test case should be added.    """
Replaced J2eeContextImpl.newContext(...) to J2eeContextImpl.newModuleContext(...)
Patches from Phani and Vamsi that need to be applied to ActiveMQ. I verified the patches work with ActiveMQ 3.2.2 and the latest Geronimo branches/1.0 code form late Feb.
"""I commented out the """"jaasTest"""" GBean since it seems to work fine without it.  If it's necessary then please uncomment it."""
"""Adds leading """"/"""" to root context of default plan.  """
Hello   Sure you are right here I didn't make myself clear enough Im sorry.  As you (and RFC959) say when we are sending the file we should transform to NVT-ASCII but not when we are receiving the file. In this second case i think that the expected behaviour is that the text message be written to a file with the default line separator for the platform.          The sender converts the data from an internal character             representation to the standard 8-bit NVT-ASCII             representation (see the Telnet specification).  The receiver             will convert the data from the standard form to his own             internal form.   Actually  according to the spec before a RETR ASCII operation is transformed we should be converting the charset to ASCII  for our machine could be using EBCDIC  to store files and clients are supposed to expect that files are encoded in NVT-ASCII.  So I don't know ... Any way this has little priority for me (until someone using a non-ascii-compatible encoding complains) so we could leave this for the future. Any way I don't like ASCII mode for it breaks  the 'REST' command. My implementation only supports binary mode.
Committed revision 585929.
"""The issue arises due to the call to <ant> in the """"docs"""" target of plugins/build.xml  It uses the system $ANT_HOME/lib/* if it can find it.  So the solution might be to add a note to howto/howto-buildPlugin.html#ant to describe the abovementioned workarounds.  Is there a better way to call forrest targets from within project ant build files. See FOR-145 """"Make Forrest able to be imported by Ant""""   """
I'm moving this to 0.6 as I really can't stand to release with this. If this becomes too long we can move it again to 0.7.
Attachment FOP_BUG_page-number-citation.fo has been added with description: FO to reproduce bug.
(In reply to comment #29) > (In reply to comment #28) > > Created an attachment (id=19576) [edit] [edit] > > main patch file > >  >  > I'v had problems again applying the patch: something went wrong with FontSetup > looks like the patch wants to remove this file entirely which I don't think > should be the case.  Hmm..  not sure quite why that happened.  > Also why did you change the DEFAULT_STRICT_FO_VALIDATION constant into > DEFAULT_STRICT_VALIDATION? I think it would be clearer to leave it as is.  Have done.
change status from ASSIGNED to NEW for consistency
change status from ASSIGNED to NEW for consistency
add keyword [PATCH] to description.
This is a java problem.
duplicate of FLUME-1475
patch _1 to fit changes from FLUME-1005
This isn't a bug. These tests are outdated and it was unclear that they were useful. They have Junit @Ignore annotations that cause them to be skipped (on purpose).
I believe this issue is resolved by the patch in FLUME-648.
There is no mxmlc.exe in the 4.10 release only mxml (for OSX and Linux) and mxml.bat (for windows). How did you install 4.10?
This issue was ruled a Flash Player issue all the way back in 2009 (by Adobe). It is not an Apache Flex bug so we can't fix it.
I have attached a patch that provides the necessary translations.
InstanceDescription can be extended with HandlerDescription without pain. Only problem is a contribution to ComponentTypeDescription by custom handlers. Simple slot for custom handlers to keep their information in displayable format would be sufficient.
I committed a patch to trunk to make access to bundle state volatile. Please close this issue if you are satisfied.
Same attachement with ASF grant this time (forgot to check the first time sorry)
the compiler works more or less good enough not perfect but it should minimize the manual labor we have to perform.
I didn't see anything in subversion?
I also rename the references in droids-spring.  Commited revisions: r985451 r985452
Sorry to close this David but the Messenger component headed over to codehaus where I think it's pretty much dead.
This indicates the Standard inetorgperson schema under the standard ou=schema. This also shows the cn=institutesinetorgperson schema which is the custom schema
I suggest a potspone to 3.0 as we will redesign the chain.
Yes I'm building MINA myself from trunk. Just to be sure when I checkout the code from <http://svn.apache.org/repos/asf/mina/trunk> I'll get always the latest code!?  I'll see if I can some up with a little application (it may take a few days due to a lot of work I've to finish).  -Roger
I changed my mind and resolved DIRMINA-82 and therefore we don't need to fix this issue anymore.
Hi Ahsan I noticed that there are some good links to suggestions for writing the best proposal from this page: http://google-opensource.blogspot.com/2013/05/student-proposal-deadline-for-google.html
Committed revision 1331265.
Committed revision 1331262.
Committed the 2a patch with revision 1187204.
I think my first preference  would be to make this only a java 7 and up feature. The change is very clear and we can use our normal testing methods.  Meanwhile users can set their umask appropriately.  My second preference would be only  to only change permissions by default  with java 7 and higher. I think we really need to have tests for the default configuration and non-portable tests might have to be customized to different environments which seems like a lot of effort for something that won't be used moving forward.  I am sorry I didn't chime in earlier.  Time has been quite tight recently.
I think clearing private fields should work in all JVMs since that's what the call to f.setAccessible(true) is for. Those fields were final too before DERBY-23 and it looks like modifying final fields via reflection didn't work prior to Java 5 even when calling setAccessible() first. The tests also fail on 1.4.2 (both Sun and IBM).
Attaching a new patch that compiles without errors against JDK1.4.2.  It also accounts for the customizable port part (DERBY-4217) which the previous patch didn't.
Oh one more thing on the tests. You have  if (JVMInfo.JDK_ID >= 6) {  ... test   should it b if (JVMInfo.JDK_ID >= 5) { ?
Patch failover-slave-2b.diff committed as revision 633043.
"""Dag commented (6/30/2011) on the commit of revision 1141368 - apparently replies on the commit messages don't make it back into JIRA so just for the record here is that comment: """"I didn't follow this one but I notice setUp and teardown do not call super.setUp super.tearDown respectively as per the idiom. Is there a reason for this here?  Notably BaseJDBCTestCase#tearDown does some cleaning up. It may not be required here but it's generally good to stick to the idiom.""""  Houx Zhang replied: """"Yes Dag. I agree with you and will adopt it.""""  Looks like Bryan already added super.teardown with revision 1141769. """
Committed revision 1070501.
Sorry for the noise I forgot to grant the license to previous attachment for inclusion in ASF works. Thanks
Assigning back to Dag.
The attached patch adds a test case to RestrictedVTITest that shows that the predicate is not pushed in the following query:      select s_r count(*) from table(integerList()) t group by s_r having s_r > 1  Only the rows with s_r > 1 have to be returned by the table function integerList so pushing the predicate could be useful.  Committed the new test case to trunk with revision 943088.
Closing as a duplicate of DERBY-3061 which is fixed in Derby 10.3.2.1.
Committed revision 1142583.  I also plan to backport the fix to the 10.8 branch.
Unfortunately I have no Sql Server to test against at the moment. But I wonder does this work when using a table name like 'SomeTable' (note the upper case). If you could verify for me that such mixed case names work with your patch when not in delimited mode then I'll apply it ASAP.  Tom
Thanks for accepting this patch. I'm sorry I wasn't sensitive to the brace convention already in the file. I probably could have javadoc'd the inner class too :)  I didn't think about the lack of thread safety of the SimpleDateFormat class: good catch. So this counts as a bugfix by markt (format was being used in an unsynchronized way prior to this patch) as well as an enhancement by me :)  If DBCP typically uses localized error messages I'd be happy to provide another patch that localizes the exception's message.
I misread the original description.
Oops didn't mean to leave that in.
Sorry for delay. This patch is including LRU cache. But changing static class into member is not done. What should I do?
Hello Oliver  Sorry for the late response. I haven't had any time to look into this again maybe it was a misuse on my side after all. Since I haven't been working on the project I encountered the behavior in for months this issue might as well be closed.
sorry After investigation the real source of the problem is when a tag is using dots (<a.variable>)  Apparently XMLConfiguration does not support this.  Should it ?
Oops sorry folks. Wrong project.
"""I am sorry if you think this is nit picking but the logged error still refers to plugins.xml but it should not:  {code} LOG.e(TAG """"ERROR: config.xml is missing.  Add res/xml/plugins.xml to your project."""")"""
Sorry I forgot to mention: This is for the future branch!
Sorry I missed this since it wasn't tagged iOS. Looking at it now.
Sorry it took so long to drag this up.  I think the issue I had was with the Customer => BillingInfo and Customer => ShippingInfo relationships.  BillingInfo and ShippingInfo each inherit from Address.  IIRC a runtime relationship was being created for Customer => Address which was messing things up.  I'll have to dig through the SVN history to see what I did with mandatory relationships that I mentioned being problematic in the initial bug submission.
Sorry for the mess there I was just trying to port CFS and Hive metastore too... but those tests don't work right now so put it on hold getting it to work with CQL3 Column Families is a priority for me right now so will come back to those later.  Just for the Hive handler please look at the (cas-support-simple-hive) branch - https://github.com/milliondreams/hive/tree/cas-support-simple-hive  All the test cases (whatever few they had) pass there and it is working perfectly with Thrift/Compact storage Column Families.
okay I'm convinced.  Sorry about the wait.  As an apology I have coded up the URL-based version in v3.  (Haven't actually tested it on non-local files though but it compiles. :)
Yes sorry I forgot to mention this :-) But a header where this is stored would work great and probably there should be a config item if you want it or not.
Hi Josh  I'm sorry I didn't check all the path that could effect the http method url. After went through the code and I found a way to verify my change and find a minor issue of http component.  I will commit a better patch shortly.  Willem
Dean I am sorry it didn't work out-of-the-box for you.  Yes by all means if you have time later and still have issues then open a new ticket.
I meant the findbugs addon. Sorry for the confusion.
Sorry Ivan attach a new patch with --no-prefix.
Regrettably I do not have a working instance of svn so I can't create a proper patch I have attached the modified source file:  org.apache.tuscany.sca.assembly.builder.impl.CompositeWireBuilderImpl.java  I have commented the place where I modified a single line with:  // FIX FOR JIRA BUG: TUSCANY-1930
Sorry I'd accidentally deleted the --disable-eventfd from my compilation script.
Hi Simo sorry for delay. I had a quick check to org.kohsuke.MetaInfServices . It seems to fit all our needs: use the quickest and more standard way to declare plugins in a transparent fashion. I don't have any objection with this patch for me it is ok.  Thanks Mic
guys... this is so stupid...
I lost the whole morning cause HBase's RegionServer was dying with no logs no nothing... how Am I supposed to debug the issue if u do not even generate a core dump?
{quote}You are messing down deep below hbase in dfs.
And I think if we're going to do a sweep up of shit we should just kill root.
@idiot Yeah I was on that idiot-path for a good while but commons-lang is a hairball at least where this hashcode making and equals is concerned.
Pull it back in if you think different.
i suck.
Will still be stuck in the loop though if can't actually close regions.
Original edited patch was messed up... stupid sed!
Who made this stupid rule?
Sorry but this is just stupid.
But I will quit committing to Hama then because I'm not going to support ONLY a binary format.
This is not what I build a framework for.
Well damn.
Blooper after blooper - I hate SVN.
"This looks reasonable to me... sucks we have to special case handling of "" ""..."
You may want to carefully examine your assumption about this.
Ok wow welcome to Linux hell...
If I get a chance I may even roll it into our dev environment but I am currently in the middle of mem leak hell.
Both equally bad.
Negative I am against fetch groups in principle.
This way lies metadata hell.
"Shit there is an dependency. Please go to Preferences -> LDAP -> Entry Editor and enable the option ""Show operational attributes""."
Damn...
Damn...
Damn !
The evil NPE strik back !!
Bloody hell !!!
Damn time has flown...
Damn it !
This was a very bad bug introduced by me being an idiot.
I have stupidly deleted the original test dir but judging from the suite's output files no output was created after 3 1/2 hours.
damn this was assigned to you since months.
Imo all this just doesn't make sense from a pure performance aspect.
+1 thanks. Don't forget to add line to CHANGES.txt when committing
Release Candidate 1 of this patch.  This patch contains: + add IP Address to CrawlDatum Version 5 (as byte[4])  + a IpAddress Resolver (map runnable) tool to lookup the IP's multithreaded + add a property to define if the IpAddress Resolver should be started as a part of the crawlDb update tool to update the parseoutput folder (contains CrawlDatum Status Linked) of a segment before updating the crawlDb. + using cached IP during Generation  Please review this patch and give me any improvement suggestion I think this is a very important issue since it helps to do _real_ whole web crawls and not end up in a honey pot after some fetch iterations. Also if you like please vote for this issue. :-) Thanks.
Patch applied to trunk/ . Thank you!
Hi Luca  I've applied the changes you mentioned and it worked fine on my end (Windows).  Can you try this on your end as well? I've attached the patch.  Thanks!
Yes it can be resolved. Thank You!
The patches look good :)
thanks to Jakob Korherr for this patch
Mathias please close this issue as it is not reproducing any more...  Thanks.
Fixed. Thanks for report (even if I'm not sure empty password is a good idea :-) ).
"""Bertrand - nice work. This is a great idea and the patch looks good overall. Couple of points:  - Don't forget the Apache header on the new Errors class - Maybe do the grammar corrections on the counter error messages separately? Just so we keep the commits to a single issue each.  - I'm not sure we need to retain the delegating """"formatValueList"""" method in TestDriver. Something to discuss...   As for using Guava to do the string join - I know MRUnit used to have it as a dependency but this was removed in MRUNIT-50. Probably not worth bringing it back for such a simple method. """
Thanks that looks good now
Hi.  The code above is a copy of the CharsetUtil class just run within a Junit Test case.  There is no exception other than junit.framework.AssertionFailedError.  Are you able to reproduce the error?  Thanks  Richard
This seems good to me. +1 to apply it
Hi Chris  Thanks for your detailed response.  Thanks
I've committed this to trunk branch-2 and branch-2.2.  Yingda thank you for the patch.  Chuan thank you for help with the code review.
$NAME
Thanks Karthik!  I committed this to trunk branch-2 and branch-2.1-beta.
Thanks for taking a look Bikas and Devaraj. Committing this.
Thanks.  I'll look into this.
+1 looks good  Thanks Mayank
+1 looks good. Checking this in.
Attaching updated patches. Arun let me know it this looks good.  Thanks
Thanks Harsh good to know.
Thanks Sandy. Committed to trunk and branch-2.
Looks good.  Minor nit can you change info.wait(0)
Committed to branch-2 and trunk. Thanks Ahmed!
Incorporating Konstantin's comment  Thanks Mayank
Thanks for the patch Daryn  +1  I put it into trunk branch-2 and branch-0.23
I just committed this to trunk and branch-2.0. Thanks Xeiguiming!
+1 The patch looks good to me. I will commit this later.
I just committed this. Thanks Vinod!
I just committed this to trunk and branch-0.23. Thanks Arun!
+1 looks good to me.
+1. Just committed this to trunk and branch-0.23. Thanks Jason!
+1 looks good to me. Trying it out.
Thanks for the explanation +1 for the patch.
I just committed this to both trunk and MR-279. Thanks Jeffrey!
addressed review comments.  Thanks Vinod.
+1  looks good
Committed to trunk and branch-22. Thanks Ari!
+1 patch looks good
I just committed this. Thanks Patrick!
+1 The patch looks good to me.
I just committed this. Thank you Scott and Priyo.
Patch looks good to me also.  bq. -1 core tests. TestSpecialCharactersInOutputPath failed with NoClassDefFoundError. I ran the same test on my machine with the patch. It ran successfully.
I just committed this. Thanks Jitendra!
The patch looks good now and the tests are all running fine now. +1.  Let's wait for Hudson..
+1. Looks good to me.
I've just committed this. Thanks Aaron!
+1  I committed this. Thanks Kay Kay!
I just committed this to trunk thanks Owen
+1  I committed this. Thanks Hong!
+1 patch looks good.
"""Patch looks mostly good to me.  One comment: I think the name """"moveAndDeleteLocalFiles"""" isn't quite descriptive enough. Perhaps """"asyncDeletePathOnEachVolume"""" or something? The fact that the path is relative to *all* volumes and will be deleted on each of them is the key part I didn't understand at the first pass through the JIRA.  I agree with Vinod that it would be nice to share code with CleanupQueue but would be fine seeing it in another JIRA."""
+1  I committed this. Thanks Jothi!
I just committed this. Thanks Boris!
thanks tom....let me see if I can come up with a test case for the LineReader reset. I did forget adding a test case for that.  I probably file that as part of this patch.
I just committed this. Thanks Omer!
Thanks for taking this patch looks good. Is this good to be committed?
Hi Nick  Can you please re-create this patch as I can't get the code to compile?  For starters I can't get the class AttributeImplItem to compile.  Please use the latest trunk when doing so  Thanks.  -- George
Hi DIGY  I like to know a bit more about this patch:  1) what test case is it fixing? 2) how did you determine the race condition?  The changes to the ThreadClass is considerable and I'm trying to understand why this wasn't a problem in earlier versions.  Thanks.  -- George
I rather we don't work on older versions.  If this is still an issue in 2.3.1 please submit a new patch for 2.3.1.  Thanks.
"""Patch looks great!  Thanks Artem.  No more mixing in of fuzzy-ness into AnalyzingSuggester.  It looks like we are doing the utf8 conversion + det twice per lookup once in convertAutomaton and once in getFullPrefixPaths.  But I think this is inevitable: the first conversion is on the """"straight"""" automaton for exactFirst match and the second one is on the lev automaton for non-exactFirst.  Really we should only do the first convertAutomaton if exactFirst is true ... but this is an optimization so we don't need to fix it now. """
Committed to trunk and 4x. Thanks Simon!
Thanks for raising these documentation issues! I just committed a fix.
Thanks Scott. git formatted patches are fine in my opinion: lets get this fixed
Committed.  Thanks Chris!
Thanks Dawid!
"""Thanks Dawid.  bq. Yes this is pretty much the top-n nodes reordering that I did albeit without any optimization of how many n to take (the hardcoded magic constants should probably be justified somehow? Or replaced by a default in FST somewhere?).  bq. Deciding how many nodes to reorder is I think hard â I failed to provide any sensible fast heuristic for that and I simply do a simulated annealing to find a local optimum.  Yeah the algo is simplistic now... and it does force caller to pick the params (though minInCountDeref=3 and maxDerefNodes=Inf are probably pretty good)... we can and should make it more sophisticated over time.  We have at least one spare bit to still use in the arc flags :)  bq. One thing I was wondering is why you decided to integrate the packer with the fst â wouldn't it be cleaner to separate packing from construction? Granted this requires a double pass over the fst nodes and more intermediate memory but it wouldn't add any more complexity to the builder which is already ehm a bit complex . You can compare this design in Morfologik:  Well... it's tricky.  First I decided to change node targets to ords so that pack could use an array to (relatively!) efficiently hold node data eg inCount newAddress etc.  That required making the first pass FST """"modal"""" (deref'd or not).  If we didn't do this then the packer would have to use even less RAM efficient data structures (eg Map<IntX>) I think?  Second the format written by the packer is tightly coupled with the FST reading ie there are sizable differences when reading packed vs unpacked FST.  But I agree it'd be cleaner if we could move packing out (eg Util.pack) and more strongly decouple packing from FST format...  One thing I'd really like to somehow do is create different classes for FST writing vs reading and different classes for each format.  We now have write-ord write-non-ord read-packed read-unpacked (and even the two readers have 3 different modes depending on INPUT_TYPE).  """
bq. Opened LUCENE-3595.  thanks! I committed this to trunk and backported to 3.x   nice iterations..
I think its a good idea to go ahead and put a patch together so we can discuss it directly.
+1 patch looks good.
"""{quote} I don't really know much about NamedSPILoader but I think what you're suggesting. How would we support Factories loading unrelated classes like they can through ResourceLoader now? Assume they're on the classpath and use Class.forName? {quote}  It needs more discussion (and input from Uwe would help!) but it works like Charset.forName(""""ASCII"""") etc. We use this already for codecs and postingsformats (Codec.forName Codec.listAllCodecs ...).  Have a look at lucene/core/src/resources/META-INF/services for the idea. Basically you """"register"""" your classes in your jar file this way: additional jar files (e.g. look at lucene/test-framework/src/resources/META-INF) can load more classes.  So this could support some idea like TokenizerFactory.forName(""""Whitespace"""") or something simple like that. So someone would not need to use org.apache.solr.analysis.xxx namespace to be able to load their analyzer stuff easily they use whatever package they want and register in their META_INF. And added jar files (other analysis jars) are automatically available this way.  I think Uwe mentioned this idea before though I think he had Analyzers in mind (e.g. provide language code and get back analyzer or something). Anyway thats for another issue :)  Just something worth consideration if we want to make these modules really pluggable. On the other hand we shouldn't use anything overkill if its not the right fit... """
Good catch!  Thanks for the thorough explanation and suggestions.  I think it all makes sense.  Will work on a patch.
Looks like a good solution!  Thanks for taking care of this Uwe!  {quote} Should we backport this to 2.9 and 3.0 (which is easy)? {quote}  +1
bq. If you back-port this to 2.9 you can't use any of the java.util.concurrent.* Very good point! - didn't thought about back porting at all.   bq. I'm not sure you need a separate SearcherHolder class - can't you re-use IndexReader's decRef/incRef? I guess I did not see the simplicity the reader offers - blind due to  java.util.concurrent.* :)  bq. I think there are some thread safety issues..  this is weird - on my dev machine and in the patch it is not synchronized.. on the machine I run the tests it is. Anyway you are right.   I changed the code to be compatible with 2.9 using indexReaders.dec/incRef.. will attache in a minute
Looks good.  bq. I didn't add Version to StopFilter nor StopAnalyzer
Simon thanks please commit this :)
Thanks Steve!
Committed revision 728746.  Thanks Shai!
Thanks patch applied.
Thanks I've committed your patch leaving out the change to equals() and not  removing mergeBooleanQueries() as that is a public method which someone might  be using. Also could you please check if the test case is correct now? I  couldn't apply that part of your patch cleanly and something might have been  broken.
Thanks! :-)
Could you see my 2nd patch? :)  Thank you.
Red color isn't really important it was just a suggestion to be more homogeneous ... but that can be revisited in a different jira. I agree an option is a good idea for automatically acknowledging hosts.  Not sure what the default should be: the current behavior is obviously to not confirm so we could keep that one or go for a safer one I don't have any strong opinion on that.
Thanks for fixing it!
thanks for the patch Michael Van Geertruy :)
Thanks for the reviews guys!
Thanks for the patch. Committed to 0.8.
Thanks very much for your response.   I want to thank your team for the great job they've done building a superior open-source solution. Not only do I use but I also promote it everywhere I go. I just did a hands-on lab using JSPWiki to create 'qwiki' WEB apps for 40 people at DEVCON 2007. It got excellent reviews.  I promote this as an Enterprise Open-source solution. If a company wanted to purchase a support contract or services for a production implementation of the product who should they contact?  Thanks again  Jim Mason   https://issues.apache.org/jira/browse/JSPWIKI-24?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ---------------------------------------------------------------------------------------------------------------   __________________________________________________ Do You Yahoo!? Tired of spam?  Yahoo! Mail has the best spam protection around  http://mail.yahoo.com
Great (it turned out simpler than what I would have thought). Thanks Andy.
Re: patch looks fine. I'd include <optional> too on geronimo/ant deps as per project.xml
The fknames patch looks good!
"""1. My feeling is that """"if (file.lastModified() < now + ACCESS_TIME_RESOLUTION)"""" is not necessary since the lastModified value will already be truncated to the system resolution but I do not think it is harmful.  2. I chose to throw exception only if file.canWrite because I was trying to minimise the impact of the change. It's not clear to me whether setLastModified might work when !file.canWrite. However it certainly must work (because of GC) if file.canWrite. I would consider moving the canWrite test outside of the setLastModified call similar to getRecordIfStored it just wasn't necessary to fix the immediate problem.  3. Well not swallowed exactly but caught and converted to failure return code (-1) e.g. see BLOBInDataStore.getSize. In any case the javadoc for Property.getLength/getLengths documents -1 as a possible """"failure"""" return code so it had to be tested. I think if Property.getLength/getLengths throws an exception that is already correctly handled.  Thanks for the quick response.  """
"""> I've reading the specification and can't find any reference to transient session in Node.lock(). Do you mean that Node.lock() don't modify transient session because it performs a Node.save() internally (there is no need to call save)?  whether an implementation internally calls save() is an implementation detail. """"there's no need to call save"""" means that there are no transient modifications which need to be saved by the client.  > My english is a little bad an sometimes perhaps I don't use the correct words and can't express my ideas in the rigth way.  no problem most committers and a lot of people on the list are non-native english speaking  """
Thanks for your patch and your investigations! I reopen the issue will apply the patch soon.
Tnx for the quick fix. Works perfect!
Patch applied thank you.
Hi Chris could you add my username as initial developers for the Spark JIRA? Thanks.
Much obliged! Thank you! :)
Hi Pieter  it looks fine now
Thanks Joe!
Thanks Daniel. We will get back on this as soon as we have the correct information. Thank you.
Thanks did that. Hopefully it syncs now. Cheers --Kurt
Hey thanks for that I gave it some testing and thought about it over the weekend and it looks perfect thanks!  - Aidan
Thanks!
Oh well. Many thanks anyways  Oleg
Thanks!
Great idea.
Oleg  A minor comment.. I don't see why the close() method would throw an IOException..  thanks asankha
You are right. I missed that point. I am +1  Oleg
That fixes my testcase.  Thank you!
Looks fine to me.   Oleg
Committed patch 2a to trunk with revision 784701.
Reopening the issue to merge the changes into 10.3 codeline
Committed revision 1125305.
Committed revision 568039.
"""Reattaching with """"grant license..."""""""
It should be noted that the client driver already caches the LOB lengths (for both Clobs and Blobs). I planning to go ahead with the patch for StoreStreamClob in the embedded driver.
I haven't spent enough time thinking about this but I think it is more than just fixing the error message string. It seems that the comparable method in DTD should return int rather than boolean and the return int value will decide if we should throw what we are throwing today (ie something like ERROR 42818: Comparisons between 'VARCHAR' and 'CHAR' are not supported. ) or throw a new message because collations don't match. This is just my initial thought haven't spend enough time to think about a solution
I found the reason why we observe different behaviors with the network server: Since you run with the default security manager you actually get derby.system.home set as a system property as a side effect (see line ca line 586 in NetworkServerControl.java) as part of the set-up of the security manager. I ran without the security manager so I did not get it set.   I do think though that the old behavior was buggy: I can't see why one should need to set derby.system.home explicitly to get the auto-boot behavior. Unless someone can explain why this should be so I'll file a bug for that.
Please find the attached screenshot for reference
Committed 3a with revision 506899.
Attaching the doc updates for the Tuning Guide. Please review and comment if these updates are ready to be committed.
Committed to trunk with revision 429550.
I propose to commit drop_column_v8_restored_tests.diff. If there are any reviewers currently examining this patch please let me know. Unless any additional review comments are raised in the next 48 hours I will commit the patch.
Committed to trunk with revision 467578. Committed to 10.2 with revision 467580.
description of the changes made in this patch
Committed derby-716-10-datatypesCollation-aa.diff at subversion revision 585710.
Kathey I think you should just go ahead and port the fix to 10.1.  Merge command:    svn merge -r 289538:289539 https://svn.apache.org/repos/asf/db/derby/code/trunk  I have started working on a general check-that-resources-are-freed test for DERBY-23 DERBY-210 and DERBY-557 but while doing that I ran into another resource leak (DERBY-594) that I wanted to resolve first.
Any thought of porting this patch to 10.1? I just ran derbyall on the 10.1 branch and this came up.
AfterDeploymentValidation#addDeploymentProblem failed. since we have DELTASPIKE-262 we should just enable it again for v0.4.
1) Yea we'll need Java 6 for the future stuff. Wasn't sure what the project's posture on that is. 2) Not a problem... documentation is key.  However I don't think this patch should be accepted as-is :-) There is a LOT of duplicate code in my patch. I did this because I didn't want to mess with the QueryRunner code if everyone thought my idea was stupid. However there are a number of methods in the AsyncQueryRunner are improved from the QueryRunner (see DBUTILS-79).  I propose that we create a base or abstract class which both QueryRunner and AsyncQueryRunner extend. Thoughts?
Created an attachment (id=13383) Patch to apsupport.m4 to support AMD64
Created a new issue and copied the patch there:  https://issues.apache.org/jira/browse/DAEMON-93
As soon as I made the change in Karaf I will submit the CXF patch.
Added patch generated from CXF 2.2.6
when we have @RequestWrapper/@ResponseWrapper annotion the generated WSDL contains duplicate schemas.  See test: JaxwsServiceBuilderTest.testDocLit()
Noah said heâs going to fix this in his build-fauxton branch but Iâd like to explain this for posterity.  make indeed throws this error I didnât suggest it is a grunt.js error but itâs the way we use Autotools. In particular in how we use Autotools for packaging. Each file that is supposed to end up in our final apache-couchdb-x.y.z.tar.gz tarball needs to be marked as such in a Makefile.am. For Fauxton that means we maintain a list of Fauxton files in src/Makefila.am specified as `FAUXTON_FILES`:  FAUXTON_FILES = \     fauxton/app/addons/activetasks/assets/less/activetasks.less \     fauxton/app/addons/activetasks/base.js \     fauxton/app/addons/activetasks/resources.js \     â¦and so on  In that list there is a line: `fauxton/app/initialize.js` e.g. where we say that file is supposed to go into the release tarball. That file however does not exist at the time `make dist` is run so make complains. I assume the solution is that make needs to call grunt at some point to turn fauxton/app/initialize.underscore into fauxton/app/initialize.js and I think that is what Noah is doing in his branch. I just wanted to explain why this isnât a make error.
Dang just realized this might be hard.  Each individual node can't really predict when/where conflicts will happen. So how would it avoid compacting away the shared parent?
That's what reminded me of ubjson actually. I looked at UBF and it looks rather complicated for a data exchange. I wouldn't say no but I don't have any interest in supporting it.  ubjson shouldn't be anything more than writing a parser (which should be ~trivial) and then adding the logic to switch (de)serializers based on content and accept types. I have no idea what UBF really even is under the hood but it sounded a lot more complicated.
Duplicate of COUCHDB-1415
@Paul. Can we close this?
Corrects problems with continuous replication timeouts introduced by r916518 and r916868.
By default the auth prompt is suppressed making this a rare issue closing.
Changing resolution to WFM
There have been changes to the method updateCurrentIterator() between these versions that we need so we can't just reverse the change. I'd love to fix any bug that can be identified cleanly. For now though I'm placing this in NEEDINFO awaiting a test case.
"""Why not just make it the default behavior?  Would an error be a violation of the base64 spec?  I have a hard time imagining a scenario where I'd say """"I'm glad the parsing library ignored that erroneous data and didn't tell me""""."""
Yes that's it.  Since it fixes the problem I'll commit the fix.
Created an attachment (id=12544) mimeInput.eml for this test case
Would you post saxon error message?
Created an attachment (id=16279) Patch to add percentage-dimensions and jpep quality handling (against Trunk)
Well since you didn't respond to my request I finally did without and checked out what you did.   First of all looking at CIncludeTransformer I have to say that I don't understand why it is doing its own caching. Caching should be the concern of pipelines and sources. If you want preemptive loading of cached content this would much better be implemented as a Source wrapper. In fact we now have such a class in the scratchpad area.  One unpleasant implication of the current design is that as an unsuspecting user of this class when you turn on caching in this transformer the cached contents of any included cocoon:// source that identifies a caching pipeline would be cached twice! Once by the pipeline itself and once by the CIncludeTransformer.  Now concerning your patch. I don't quite understand why you retain the validity of the cached contents. The algorithm the caching pipeline uses to determine the validity of the cached contents is as follows: first it looks at the validity of the cached pipeline contents itself then if the validity by itself indicates it needs a current validity to check against then the pipeline contructs a pipeline validity by calling the participating components' getValidity() methods. If your component now returns the same validity object that you previously returned then  that does not make much sense to me.  Since I really needed this functionality myself and I didn't have enough time to discuss and refactor the CIncludeTransformer I implemented my own very basic IncludeTransformer that does exactly what I need no more no less. I'll post it here if anyone want to use it.  I also looked at the XIncludeTransformer to see if I could implement caching in that one instead. But it seems that the XIncludeTransformer reads included content from URL objects instead of from Source objects.
Created an attachment (id=9504) relevant portion of stack dump
Created an attachment (id=8392) patch
Created an attachment (id=6729) modified SQLTransformer (in full)
Not commited in cvs.
Hmm not quite. Added this above entry to my web.xml but I'm unable to load  the jdbc driver oracle.jdbc.driver.OracleDriver. What's weird is that this  class is in Cocoon's normal classpath. How do I specify the classpath? Once I  get this answered (either here or in cocoon-users which I've also asked) I'll  close the bug.
It was looking good for jackrabbit for modeshape there is one more tweak required
You should do conn.getObject(entry null). In the AtomPub case it's actually going to be optimized if the entry is already an APPObjectEntry and no futher remote repository access will be done.
Are you using the latest Agent code? I had verified on with latest kvm agent jar copied to kvm host and tried. It worked without issues. Edision had implemented this with the above commit mentioned. Last Saturday I had verified it and its working .  Migrating volume across ZONE/Cluster wide is working perfectly.
Kishan If this is already merged can you change status to resolved??
Closing this record since 4.1.0 is now released.
closing since 4.1.0 is now released
Released in ClickIDE 2.2.0.0.
Hi Malcolm  updated for the latest check-style.  Christian
Committed revision 1517592.
Created an attachment (id=17244) Fix misspelled function names
reopening issue for a partial revert as requested by Tsuy see http://mail-archives.apache.org/mod_mbox/incubator-clerezza-dev/201009.mbox/%3CAANLkTikODvHWEzyUzsseJ7nZWjUDFhri6qKxjBmiS9WF@mail.gmail.com%3E
The tricky thing with all the stacktraces we have as of now is that they don't seem to be the cause of the issue. Something is eating all the memory and as a consequence stuff starts to go wrong in arbitrary places. We need to find whatever is eating the memory in the first place. It might be very well be releated to the security provider. Can you try to run a memory profiler and see whether you can spot what is holding all the memory?
Created an attachment (id=12961) [chain] CatalogFactoryBase
Plugman seems to not honour the npm proxy in fact. That's the problem.
resovled commits in comments
THis is now resolved and pushed to npm as 3.0.1. Uses npm's proxy configs in lazy-load module.
Ahhh sorry didn't realise that set -e took care of it for you.  I will have to go back and work out why it's failing but not triggering my CI system.  As such you can close this.
I can't reproduce this error.  I also can't properly navigate in your example since the UI doesn't properly work on a Nexus 4.  Tried on the Nexus 10 but your splash screen took too long.
true.
dupe CB-675. See my latest comment there regarding Cordova 2.1.0
I've just gotten a chance to look at this.  A few things jump out immediately.  1) I have to chew over the changes to Cayenne core a bit more.  Off hand they don't seem bad but something feels weird about it.  2) While I can appreciate the cut working like a remove it's a bit weird to be prompted to delete the item.  3) I can't paste an ObjEntity I just cut back to the same DataMap.  With the lack of an undo feature the only reasonable thing for me to do is a revert the datamap.  4) I think my font changed for labels.  I'll have to verify that.  Looking at #3 again actually I can't seem to paste at all.  I even tried creating a new DataMap and was unable to paste the ObjEntity to that.
in addition to that DataContext.newObject() shouldn't throw on classes that are not DataObjects.
I was actually trying to debug the issue and didn't see anything that looked like it would nuke the DataMap/DataNode under the DataDomain and was wondering if it was a visual issue but didn't try saving the model to see if it had actually deleted it.  Something about all that information disappearing was unsettling.  This is definitely a post 1.2 thing I think.  Thanks!
I don't think this is a bug -- you essentially moved files out from under a running instance something cassandra can't expect.  In the end sstablesplit should only be done offline.
bq. One possibility is that getToken of OPP can return hex value if it fails to encode bytes to UTF-8 instead of throwing error.  I can't think of how it would break anything to accept keys we previously rejected.
"""Not sure if the patch was pushed for review or not. In any case fixing the build is pretty trivial the changes made to ModificationStatement should just be reverted (and we should just pass the CL in argument to executeWithCondition() to SP.cas()).  Other than that: * I believe we now refuse CL.SERIAL for the cas() method (because CL.blockfor() throw an UnsupportedOperationException for SERIAL). Maybe we can make SERIAL mean """"wait for nothing on commit not even a local one"""" (which probably amount to making CL.blockfor() return 0 for SERIAL and adding a 'if' in WriteResponseHandler ctor to signal right away if blockfor is 0). * I don't think CL.ANY will really work as for normal writes since we don't generate hints for commit. Maybe we could (generate hints) ultimately btw but in the meantime it might make more sense to refuse it (and thus make the default for CAS be either SERIAL if we do the preceding point or ONE). * We don't call CL.validateForWrite() when calling CAS (and we shouldn't as it complains with SERIAL) but maybe we should add a validateForConditionalWrite that would at least check we have the right replication strategy when using the multi-DC specific CLs."""
patch attached.
The mutateLevel problem looks like what I predicted -- Windows won't let you rename over an existing file.
Duplicate of CASSANDRA-4587
And I notice that the marcuse/4338 line still doesn't have latency metrics if you'd like me to re-run for those stats I can. Just need to rebase off of CASSANDRA-6153 (or rewrite my tool to use a known good cassandra-stress
Updating patch against new 4164.patch-2.txt from CASSANDRA-4164.
I can't think of a good reason to snapshot the entire keyspace and doing so can dramatically increase the space needed to enable snapshot_before_compaction.
I think I'd rather have a CL option so it can still raise an error if you think (say) just one node is down but a second goes offline briefly as you run the loader.  But --skip-dead-nodes would also be reasonable.
Hi Brandon Let me know if the attached patch is sufficient... It looks like the following.  Address         DC          Rack        Status State   Load            Owns    Token                                                                                                                       143633478586163499463326301508681906517      10.123.42.165   us-east     1a          Down   Init    ?               ?       ?                                            10.42.134.229   us-east     1a          Up     Normal  1.74 GB         35.40%  33724529808132598296109669138912087817       10.93.19.6      us-east     1a          Down   Normal  1.78 GB         38.69%  99546828780538918038465713665698202555       10.93.74.164    us-east     1a          Up     Normal  120.37 MB       2.53%   103845090001698524309715695190561870103      10.123.59.26    us-east     1a          Up     Normal  1.15 GB         23.39%  143633478586163499463326301508681906517
(This is the Debian build of the 1.0.6 RC that Sylvain just put up)
"""We can get average column size from row size / column count so we could make it """"If column size > N bytes or sstable is older than gc_grace_period."""""""
The attached patch combined with an `svn rm drivers/' should be enough to call this a Done Deal.
"""bq. CASSANDRA-2444 got in the way  I'm not sure what the right solution is here.  I buy the premise of 2444 that you don't necessarily want to get hammered by compaction when you're first starting up (warming up caches).  So I don't think """"check for compactions ever N seconds"""" is a great policy.  But I'm not sure """"check every N seconds starting M minutes after startup"""" is great either because it's not something a user will just guess when he's wondering """"why aren't compactions happening yet?""""  Any other ideas?"""
"""Split out some fixes to the SSTII bytes tracker getting out of sync w/ the underlying stream and did some cleanup to make the streamed/file versions less divergent.  Also adds parallel compaction testing to LazilyCompactedRowTest.  CliTest and DefsTest generate compaction loads (in DefsTest's case on the Migrations CF -- haven't dug into CliTest as much) that break w/ parallel enabled although the test doesn't actually fail (argh).  Haven't figured out what's causing that and haven't come up with a way to reproduce in a """"real"""" test yet.  The DefsTest does mix lazy/nonlazy iteration in the merge which may be relevant.  bq. I'm also no proposing to complicate things.    You're right poor choice of words on my part.  Latest gives the merge executor a SynchronousQueue.  I think that's a better way to cut worst-case than the Deserializer for the reason given previously.  bq. 'if...instanceof' business is a bit error prone/ugly  Agreed. Added getColumnCount + reset to ICountableColumnIterator sub-interface.  bq. say how multithreaded_compaction is different from concurrent_compactors and that multithread_compaction is likely only useful for SSDs in cassandra.yaml  done  bq. The bytesRead """"race"""" should also be fixed in CompactionIterable  done  bq. I would have put the code in CompactedRow.close() at the end of the LCR.write() instead of adding a new method as it avoids forgetting calling close   I did consider that but it feels weird to me to have write implicitly call close.  I guess we could just change the method name? :)  bq. We can make PreCompactedRow.removeDeletedAndOldShards a public method and use it in PCI.MergeTask  done"""
bq. guess we need to supply a tool to rename sstables files if anyone is on longer names?  We probably don't need to do anything. I don't think anyone is really using names long enough for them to it the file system limit the goal of limiting the names is just so to prevent this from happening but there will be no other assumption that the names are short from the code.  I also don't think anything will prevent rolling upgrades do you had something in mind?  Note: I have a long flight ahead of me so I plan to update my last patch with both those changes as I still like the moving of all the directories handling in a dedicated class even if we don't support both layout.
"""bq. I'm not sure I understand are you saying that B would violate this or just that the status quo does?  I'm saying B would violate this yes. B was """"bootstrap from the right token but if that one isn't up bootstrap from any other token preferring the closer ones"""" right? I'm saying we can't just automatically choose another token if the user didn't specifically say it's ok."""
Did you get a chance to try a patched build?
Truth is that the sheer size of CFStore (over 2K line) annoys me. I think this prove CFStore has too much things in it and we messed up at encapsulation. So I tried to make DataTracker be about dealing with the memtables and sstables as much as possible. And being able to have View not leak from DataTracker sound like a proof that we do have some encapsulation there. I'm not claiming it's perfect but I really don't think that CFStore is our most beautiful class in there and I do believe that DataTracker improves this a bit.  But there is a good part of subjectivity in all this anyway. And if people really don't like the move of the bits of querying in there fine I'd move them back in CFStore and leak the View from DataTracker. I would however be more reluctant to remove DataTracker altogether and put the AtomicReference in CFStore. Because this would move all the CAS related logic in CFStore and seriously there is already too much stuff in CFStore. My humble opinion though.
Patch to allow removing the token and throw UE instead of an internal error when trying to insert and the number of endpoints is less than the RF.
attached patch puts all MessageDigest creation into util class.
It's hard to say. I lost 5 nodes in about an hour but I don't know how many I lost last time
StringBuilder is used because we can't stream JSON row before we have all columns serialized (columns serialized in portions according to PAGE_SIZE) see serializeRow(SSTableReader SSTableIdentityIterator DecoratedKey PrintStream) method. If you have a better idea then using StringBuilder in this case please share...
Looks good to me for the first 3.  I think ANY should be equal to ONE for hints=off.  I.e. when it's off we *never* create hints.
Hmm I can't apply 0002-V2 on top of either my 0001 + trunk from feb 5 or current trunk + rebased 0001.  And Eclipse generates the diff in a different order from git commandline so I can't tell what's changed by eyeballing the diff either.  I've attached rebased-to-current-trunk versions of my 0001-0003 can you attach your fixes as a 0004 on top of those?
I do have a very large # of rows (>150-200MM on many nodes)
No noticeable performance impact with stress.py defaults or with one column per row in periodic mode.
I am in the middle of reviewing both of these patches and noticed that they are related and won't mix well.
Need to add the reconciler as part of the keyspace attributes...  just so I don't forget.
"""> but I renamed a set of sstable files in data/ to the """"wrong"""" version and it did not catch this at startup. I'm not sure what you mean. Which is the """"wrong"""" version: the legacy version?  > (does it still ignore tmp files at startup?) I'll double check but I assume so. I didn't change the check that removes tmp files because it did it based on looking for TEMPFILE_MARKER anywhere in the file name which should continue to work."""
There's good discussion on that project and LRU cache in general in HBASE-1460 if you missed it.
Just changed the workflow scheme should work now.
Duplicate of CAMEL-6918
Hi Willem yes you're right.. I did not realized that I had a explicit dependency on the wrong version... You can close this ticket.
"""It's only committed to the trunk at present (you can check this by selecting the """"All"""" tab in the """"Activity"""" section).  @Willem: Anything other to do here? I'm asking because the issue is still in the status open. Any reason not to back port this feature also to our 2.10.x and 2.9.x feature branch?"""
JB do you expect problems when we upgrade Camel 2.10.0 to Spring 3.1.0 (and deploy it into Karaf 2.2.x)? Because Camel leverage on the Karaf Spring feature it will use Spring 3.0.x when running inside Karaf 3.0x and Spring 3.1.x when running in Karaf 3.0.x.   IMO when we can upgrade to Spring 3.1.0 without any modification it should not a problem...
Nevermind.  I believe someone has already addressed it in CAMEL-4294.  I have linked the issue here.
Closing all resolved tickets from 2010 or older
Closing all resolved tickets from 2010 or older
Closing all resolved tickets from 2010 or older
Closing all resolved tickets from 2010 or older
Closing all resolved tickets from 2010 or older
Closing all resolved tickets from 2010 or older
The ASF is going to get the document signed this week will keep you posted.
Hi Christian I just tried to apply your patch but I found there is no pom.xml in the camel-soap module. Can you submit it to the JIRA ? BTW did you run the test with JDK 1.5.0 ?
Hi Roman  Which version of Servicemix are you using ? There is an camel feature (which includes camel-core camel-spring and camel-osgi) in the org.apache.felix.karaf.features.cfg you need to replace it with camel-core and camel-spring-osgi. Please make sure there is no camel-spring camel-spring-osgi camel-osgi installed at the same time.
Closing all resolved tickets from 2010 or older
Will be naming this camel-rss rather than camel-rome in case we change to some other rss lib in the future.
a patch is attached to camel-606 also address this issue.
Closing all resolved tickets from 2010 or older
Hmm - it seems that the patch cannot be applied on a few files - e.g. the CxfEndpoint.java patch. As it seems your patch is against an old version of the code?  e.g. your patch uses the line  {code}   * @version $Revision$ {code}  when trunk is  {code}  * @version $Revision: 563665 $ {code}  Any chance you could do an 'svn update' to ensure you are using the latest/greatest trunk then create the patch again please? As right now I cant apply it
Here is the curent state. It is the samples/servlet/src/script/share build.xml.  It work for tomcat5x (it is hard coded). The test are passe but I need to remove TestSampleFilter.java TestFilterHttpHeaders.java and TestBasicAuthentication.java from the test execution. I don't know why they failed but continu to look at this.  Nicolas
Somehow it seems that this problem is more on the rubygems side. I didn't have time to investigate it any further but looks like rubygems is using @gems internally sometimes as a hash and sometimes as an array.  You could try doing the following change in rubygems/source_index.rb (line 410):  410c410 <       @gems.replace(new_index.gems) --- >       @gems = Hash[*new_index.flatten]
The proposed solution I outlined above on github: http://github.com/assaf/buildr/tree/BUILDR-4  But I can't find a way to use this in combination with ArtifactNamespace so it will have to be one or the other.
[~hustlmsp] I've created JIRA BOOKKEEPER-603 for the issue.
We have fixed this in the twitter fork. You can review the fix at   https://github.com/twitter/bookkeeper/commit/a48472911b176d75905399e7c32bb782f3564b83  If you are fine with the change I will submit this as a patch.
Committed revision 1343996.
Attached the patch which addresses the issue in EntryLogger.
attach a patch fix NPE in debug message.
yes parseLedgerConfig did throw an IOException.  since the bug is found in hub server topic can't be acquired due to hub server can't closing such kind ledger. if we don't add code in LedgerOpenOp we have to find all such kind ledgers and handle them manually (maybe delete them). throwing IOException here is OK for me. but I am not sure do we need to keep backward compatibility to handle data/metadata produced by old version. what is your opinion?
S3 would be interesting
If removing leads to the reduced coverage then I am not a happy camper.
I did an ubuntu 10.x apt-get install and flume/* downloaded the following unwanted packages: flumotion_0.6.1-1_amd64.deb foomatic-filters_4.0.4-0ubuntu1.1_amd64.deb
Created an attachment (id=12011) LazyDynaClass Test Case
It looks like we have to rebuild StackMapTable from scratch during method generation.   I've tried BCEL code and it looks like it already have implementation we need but I don't see ways to reuse it for now...  By the way I recommend to increase this issue priority to high or blocker.
This is fixed in CVS (post 1.5b5).
I believe this is now fixed.  The test case itself was the problem - missing suffix of ULL or LL for unix and UI64 or I64 for windows.  I am closing the issue.
I have attached the modifications to the ClientStubWriter.java in wsdl. This modification will solve the problem. But I didn't check further memory leaks with the modifications.
Apparently this is not completely solved.
Patch commited
Hi Dushshantha  Please find the attached file AXIS2C-1193-manual.patch.
I've done some modifications and checked with valgrind and reduce around 6000 bytes of a loss with echo sample. regs lahiru
What is interesting here is that the OOM error already occurs while processing the SOAP part of the message and not while accessing the attachment part (as one would expect). What is the stack trace that you get with Axis2 1.6.1 and Axiom 1.2.12?
Committed revision 1068985
Patch is attached
Hi I am still having the same problem with the 1.4.1 build. Can you please check this a.s.a.p.  Best Regards
Committed revision 644436.
Yes someone should have a look at this. I just ran it again and it is still failing.
This doesn't exactly seem fixed to me. First of all I can't make this work on an actual RELEASED version of the plugin. Second I can't even get the most recent snapshot to work at all. The POM includes all sorts of noise that just shouldn't be required per Chris's point. Can this issue get some attention? I'm fairly certain this plugin just flat out doesn't work in its current state.
Fixed. Now we dont have restPath only service path.
Patch attached.
Duplicate of AXIS2-897
Would anybody now if this is fixed in a later release?
hmm... i couldn't run your huge 7.5 MB application. Sorry for asking again but will it be possible to post a ***smaller*** test case? Actually that will also help you to isolate the issue.  - venkat
I added a patch to fix the deadlock issue.  I used the same type of approach as Brian Ewins did with his patch. So the addition of some of his concepts could be applied to fix them both at the same time.  Reference of his patch suggestion: http://nagoya.apache.org/jira/browse/AXIS-238?page=history
Attached a patch that adds a flag for opt-in and adds a test
"""Modification of patch-2 to introduce a double class to resolve the final ambiguity for [""""float""""""""double""""] unions."""
TestWordCountTether times out.  I suspect the wrong version of Java is used for subprocess.
Didn't make 1.1
Didn't make 1.1
Reopening this issue and Richard will add more information into the JIRA.
Hi Fuhwei  I was a bit worried by the big override method in your patch so I have taken a good long look at this but as its an area of the code I'm not very familiar with I can't necessarily jump in with an expert comment.  My current analysis of this is that EMF forces an assumption in the two lines of XSDEcoreBuilder's createFeature method      XSDTypeDefinition elementTypeDefinition = getEffectiveTypeDefinition(xsdComponent xsdElementDeclaration)
This is not a major issue
I don't see that he fixed anything for TS-2216 are you thinking of his commit for TS-1988 ? I was going to try that.
I mean keep purging that cache asset.
Committed @revision 1340076 in trunk
"""The appassembler author has rejected a long outstanding bug in this area as """"Won't Fix"""" http://jira.codehaus.org/browse/MAPPASM-54   They seem to think that because the script is not internally broken that they shouldn't need to worry about it.  """
We found the root cause of the problem. Will add a patch
I added your tests and didn't see any failures against ActiveMQ.  However I did make a change anyway to StompFrame similar to you patch expect that I just used the DataInputStream's readFully method to ensure all the data gets pulled off the wire.
Resolved in trunk.  There were two places where the ack message was getting the Destination from the Message assigned to it instead of the Destination from the MessageDispatch command.
Nathan  I have not been able to get the time to run the other tests.  However I did try changing the timeoutwait.  I set it to 10000 as suggested.  The test failed as it did before.  I then decided to change it to an unrealistic number 60000.  The program died as it did before.  Whats interesting with this test is that it did not even run for a minute.  As such I dont think changing this value had any impact on the problem.
The DebugDLL and ReleaseDLL build configurations for vs2005-activemq didn't have the /DEBUG linker flag turned on.
From what i can see on the system the libpthread is in /usr/lib and in /usr/lib64 and both of those paths are in my library config.
Ran the test with a 5.8-SNAPSHOT build an everything seems fine.
Fresh build works for me now too.  Looks like the missing snapshot has been published somewhere now.
receive()  and receive(timeout ) now return null on transport failure - so consistent behaviour with a close() on the connection.
Enabled garbage collection on transient and durable queue message managers.  Also added a check on capacity limit before removing idle containers.
Commited to trunk.
Can you run: {noformat} yum repolist all {noformat}  and post what you see?
User specified directory is not created by agent
Attached
I checked it on IE and firefox and it was working fine. Though i am not sure why it didnt work particularly in chrome i put those elements in a separate div and this helped. Now its working fine in Chrome also.
How do you know that a key will exceed a threshold until after you've passed it?
Fixed. Committed revision 745895.
I did try some variations as well and I have to say I only got confused.
Confused about this.
Inserting timestamps automagically would be bad because it would limit a whole swath of use cases.
If they do then Suse's cpio works for old ascii but the -c switch is broken.
This is pretty embarassing!
When you install a node module globally it goes into /usr/local where you don't have permissions to move shit into unless you sudo.
That being said I'm not totally opposed to making the limit configurable.
My bad I screwed up the assertion -> RuntimeException transition.
Well that sucks.
No it's not off the table but it's moot unless we decide we want cqlsh in-tree at all.
Ok here is a patch that works. I was stupid enough not to see it...
It was a pretty stupid idea to use a static array of instances for QuorumOpMonitor.
All I'm trying to say is that it's pretty easy to end up in propagation failure hell here or change something else that blows things up for use cases that are not foreseen.
Just to clarify... you made the DTPE handle exceptions the same way as CassandraDaemon by updating it to use NBHS instead of CSLS...   Did you just tell me to go fuck myself?
+1 (damn intellij it's not like patch format hasn't been standard for years)
Wow it's like Thrift is going out of their way to make life suck for us this month.
Here is quite bad.
The samples you gave are different.
"I just noticed your public tweet...  bq. ""@wilhelmbierbaum True. Fuck the Avro C API."""
if so can you see if you can git bisect this bad boy?
It seems some of the wrappers in ink_hrtime.h() might no longer be used either e.g. ink_gettimeofday() is never used so lets get rid of that shit.
Heh that's one hell of a bug 275 bytes exactly eh?
What a stupid name I chose for that object... )-:
I think it's time to just close this issue.
I'm not entering bogus reports for the hell of it )  Matt
I am currently trying to get it into AJDT dev builds but my git push is timing out (damn thing!
Stupid Maven
Ok. Stupid user Error here.
Converted from a single FlatFileItemWriter to a database and flatfileitemwriter and didn't register the flatfile one as a stream.
If using a single FlatFileItemWriter it is enough to register it as an ItemWriter however when using the composite the adding it as a stream is also needed.
Why the hell do they deliver Duration if they cannot instantiate it :-/
The defect  from hell 666 be scared..
This really sucks!
Why the hell is this not a bug?
I'm confused...
What the hell are you waiting for ?
you can't reopen stuff fixed in the past without upsetting JIRA.
I've fixed that locally and am doing a full build but I'm going to build a test case to duplicate the error and then verify my proposed fix corrects it.
I suspect it has nothing to do with the file system connector or Infinispan connectors and is simply a (stupid) mistake in the federated join processor.
I'm an idiot.
The Cenqua licensing section doesn't have a license like this but the Clover licensing section does have a link to one.
It really sucks that there is now way currently to include transitive dependencies.
I'm an idiot... there's nothing wrong with the readme.txt.
Crap forgot to verify that this is inconsistant with MRI it's not.
Please close as this is just me being stupid.
Holy hell I think I found one problem with turning a bignum into anything else.
This sucks *so much*...
Damn I would close this out from the fix to JBSEAM-3742 except the quartz and seampay examples use a custom XML namespace and the XML schema validator is not happy with that.
*Damn* I forgot.
Also your username was stupidly named and confusing so I deleted it and created a new one more appropriate.
Damn it seemed it didn't work.
It sucks to lose the code readability but it seems like a reasonable price to pay.
I don't have to ensure that the classloader knows groovy classes *you* must do that.
And debugging is hell because the test environment needs to have the exact same loader setup.
who the hell starts using our jira for typing lessons ?!?
fuck u
Hell this is gonna take me a lot of work to raise.
That irritated the hell out of me too but I didn't realise it was configurable in the .launch file.
This sucks badly.
Why on earth does createLink() need a request?
Finally closing this bug from hell.
I sure as hell don't want to start attaching models for the sake of detaching them directly afterwards.
That sucks why isn't the escape mechanism working?
I am an idiot - this was a dupe of GUVNOR-84
That is really confusing.
Sun RI sucks why the hell is saveState() and restoreState() not called with with javax.faces.STATE_SAVING_METHOD = server???
Swear to god I'm not a total idiot.
I don't know how the hell my diff program decided to add seemingly random CR chars but I've removed them now.
durrrh that sucks.
Im stuck with IE6 unfortunately.
What the hell I'll give this a shot.
{quote} searcher.getAtomicReader().getSortedDocValues(uniqueKey) {quote}  This is a performance killer.
As far as the query shit i have no idea if solrdispatchfilter or whatever could/should do Thread.currentThread().setName(x) or whatever (and maybe restore after)
BTW: This is one reason why I hate this autoboxing shit since Java 5. I would love to have a way to prevent it in code (forbidden checks should detect this somehow).
Hmm that sucks.
tricky shit man! I think I found the issue!
Oh man ... it's a fucking precendence problem.
Ugh well that sucks.
I didn't know we needed to retain backwards config capability across 3.x to 4.x  :-(  That sucks and it'll make old code stick around longer.
bq. this is solr's fault by having a getter that does some heavy duty xml shit.
That sounds like some serious buck passing.
This whole thing is very distressing to me.
I think we all know it just sucks.
though I disagree with a signficant amount of statements you madeI don't think we would ever come to agreement anyway.
that's some freaky shit ...
i could have sworn i clicked the other radio button
I think you made mistakes in ivy.xml.
This is where it gets confusing.
If I can give you one advice on this one I'd say this: fuck Derby use H2 instead as the default embedded database.
Of course it is.
I am slightly confused by the above.
Anyway created a patch ...
Ah HA!
or a log message?
Until there is a general mechanism Hbase should do something.
but there is no such limitation in the consistent algorithm behind ZK.
It will be taken care as part of HADOOP-7642.
The contribution archive.
What is convenient for the user to download two archives to run the project (or install maven)?
+1 for the docs in the SRC though.
Okay Edward you haven't understood how it works.
Maybe you know it better when I show you some code.
If you want to have a binary sequencefile format then do this.
What if you want to change this binary format?
Do you want to recreate every file on the whole planet?
You must take care about versioning then and that is because we need a proxy between the inputformat and our vertex API.
Contributed by Vinay.
Previously we are mainly focus on trunk failures.
Yea I think it would be worth distinguishing between the reasons.
Gimme a few mins to clean this up properly.
Is the -put command returning non-0 to indicate failure?
Is it by mistake?
We should be assigning atleast 50% of it's capacity.
pread I mentioned is FSInputStrea.read(long buf offset len).
multiple pread invocations hitting contiguous byte ranges is quite tricky.
I would have hopped that the URI(String) constructor would have decoded things properly (or encoded them as it happens to be here).
Continue with stupid questions :)  Who is sponsor and mentor of project?
So who should create mail lists bug databases etc.
Fixed bug of <hr /> the siteinfo is always after the rest of the page now.
batch transition to closed for remaining resolved bugs
Hi Vincent  I wasn't aware of that.
"We are all different"" Crowd ""I'm not"" Dissenter Monty Python's Life of Brian"
"Turns out that Ubuntu dropped 32 bit support for 'whatever' in 12.04 and now you need to run ""sudo apt-get install ia32-libs"" in order to get older executables to run."
I wonder why not so many people use Linux?
On 2009-06-08 19:22:03.496 kjian commented: Localization data error.
On 2009-06-08 19:22:40.967 kjian commented: Localization issue.
Will close this as a dup.
[rvollmar 3/27/07] There are two bugs that are causing backwards compatibility issues and I think the emails we've been exchanging have confused them.
First an attempt is made to convert       * each argument.
It is better to express such things in Java or in a query language that is designed for that.
Why would I want to use XML when i can use the QBC API or HQL (HQL can be embedded in the XML remember) to express this stuff much more naturally?
"In addition in the longterm we will all most likely be using annotations and the ""fetch group"" notion does *not* work with annotations."
It's generally pretty straightforward but extensibility is not quite clear.
I have changed one test to see what will happend if I set the pwdIdle to b > 0 and the test passes.
What exactly is your configuration ?
The more I think about the issue the more I find it critical.
If we remove an entry while someone is doing a search the search will fail.
Also we have some problem during replication just when we try to replicate some deletion and it might prefectly explain why we get those issues.
(Ok as expected ...)  Waiting for a better log when you will have installed a new server build from 1.0-trunks then.
What you can do is trying this request with a different browser when the server is suppose to hang and also try to do another request when the serveur is supposed to be hang to see if it's not a pb with the PDU length ( what I suspect is that when trying to decode this request the decoder is waiting for more bytes so wait ad vitam aeternam ...) I gonna try to implement the decoding of this PDU as a test case just to check.
The search result entry requests does not return the correct DN because it use the internal representation (byte[]) instead of using the user provided value which contains escaped values.
Here we have two options : 1) work with the internal value but escape all the special chars 2) work with the stored UP value.
I think the bnest solution is (2) because the UP value is supposed to be valid - it has successfully been parsed when the first request arrived -.
This will impact two parts : - searchResultEntry - all the response that contains a LdapResult and which returns a MatchedDN.
I already have a fix for the first request but the question is : which version will integrate this modification ?
any suggestion ?
I just applied the patch ran tests.
Then you might switch filter chains to ?
However I don't see any failures.
We need a proper JUnit test for this!
Thanks for reviewing the patch Mamta!
I've been trying to test a patched build with ibm 1.5. on iseries but have run into trouble: - OOM with suites.All.
A second run (with 2048M) only got worse (sooner) OOM behavior and after that I had trouble getting rid of my processes - apparent hang with derbyall:lang/wisconsin.java.
On rerun I let it go for 20 mins but apart from the database getting booted (derby.log) nothing at all showed up in any of the output files (i.e.
.tmp was empty).
During the 10.5.1.1 test cycle this test took about 5 mins (and passed).
So the original problem appears addressed (as I said the 3 failing tests now pass) but I will investigate these behaviors and run on another type of machine just to be safe.
I think I prefer this approach.
I remember that rule for line breaking in Japanese was very different from that of English.
In English sentence there exists space between words and that space tells where the line can be breaked.
However Japanese sentence does not have such a useful space between words.
As I knows only punctuation tells where the line can *not* be breaked.
This may be well known problem in i18n in typography ....
Can this be closed?
derbynetmats had no failures.
You can close this out.
plz fix it properly till tomorrow Ort I'll close this for 0.4 again.
What was the outcome of the discussion?
Afair we settled on not implementing this as logging is hell anyway and we do not like to introduce just another logging framework.
Also we would need to handle Serialisation which is _only_ implementable in a performant way if you know the underlying logging framework.
I'll open one now so I don't forget.
Think you could make a failing test case and/or patch to show the behaviour?
But I could join them.
For VMware we noticed that there are several places requiring cleanup of staging storage object which requires a generic approach to thoroughly fix them and cannot make it in 4.2.0.
OK this solution sucks and since people are relying on these scripts instead of using tools like ant we need to give them the ability to upgrade these tools without having to create a new project.
Any ideas?
Why is it unsafe to share filter in SelectStatement for NQF?
Looks like you need to report this to the high-scale-lib project.
I thought that there are some common cases like tail and head caching or exclusion of columns that could be implemented.
But never mind - maybe the proposed cache is all greatness.
So to prevent a rude awakening for some users it might be cool to provide some means (config or whatever) that works similar as the current version.
Or at least schedule this for a release which allows for a downgrade.
Gotta show off the shinies first of course.
So we convert Scanner back into an Iterator over a lazily fetched Slice subclass?
Disagree.
For *all* this protocols/endpoints Camel already offer consumers.
Closing all 1.6.0 issues
I'm very very keen on preserving backwards compatibility.
The Calculator sample now works with SimpleAxisServer.
Is there any documentation how to use axis2-wsdl2code-maven-plugin with jibx binding and how on hell you define  xmlns:tns from you binding file
Do you think you can make us a minimal test case which demonstrates the problem?
Please close AXIS-2271 if that patch is taken.
This practice in general seems really confusing to me this bug is refed in the 1.5.0 release notes tagged in svn.
I think all properties and permissions should be maintained.
Well I'm not sure what was happening but I just upgraded to Spring 1.2 final and this issue does not happen anymore.
It really *did* happen in an earlier version though....
I disagree.
The manual should provide sufficient insight into the important use cases.
People tend to review javadoc when things don't go as expected or are unclear.
In my case I wrote a custom filter fucked around with the configuration getting it to work which means you really have to get into it.
etc.
I reviewed javadoc but right or wrong I didn't come across this.
Google was also strangely quiet.
stupid autoresize option doesn't resize it correctly
) - it should be in AJDT in a couple of days.
I am sufficiently archaic as to have only a pre-ANSI-C K&R.
You can put *expressions* separated by commas in a for even back then:    for (i=1 j=100 i<j i++ j--)  But I guess that the tweak to allow *declarations* there only has one.
I was surprised to see this issue still getting so much commentary.
"I'm going to jump back to Robert's initial pushback here:  ""1."
I understand job execution creation needs to be atomic but I don't think it implies the whole logic around creating job execution must be handled by repository.
Using technical tricks like automatically appending parameters to create unique JobInstance makes alarm bell ring - we should not be trying to do clever arbitrary things in classes that form the foundation of the framework.
If job instance has a lifecycle that ends with first successful execution we should refuse to start the same job instance ever after.
We described before that there are times that operators for whatever reason what's to restart a job with the same instance.
It could be something as stupid as accidentally deleting the output file.
So it's like getting a new copy.
My bad.
Same goes for fontSize and fontWeight and other font styles.
But on the other hand I also think if the processor has to do this initialization activity (i.e.
Yeah it helps though I was more questioning whether there's not already a boolean that keeps track of this but apparently not.
Seems a bit restrictive to me that using rich:comboBox with s:convertEntity only displays entity IDs in the combo.
"""rich:comboBox is not a select component by nature"" - so what is it then if it's not used for selecting stuff?"
Guys this defect is 5 years old now.
If it's a regression please open a new ticket.
Yeah I didn't mean to imply that setting a null value was the problem which is why I changed my comment to denote the problem was actually in the federated connector.
It appears to be a simple cast to the wrong type (from a copy/paste error during coding).
I'll copy that into a LICENSE.txt file.
The instructions are fine.
Not sure how I got that all wrong.
Note to self: Always check for platform-dependent behavior!
I've tested with Ruby 1.8 Ruby 1.9 JRuby and even Rubinius but all on Linux.
This lack of proper documentation on the standard Ruby API drives me mad.
Should this be raised to Ruby core folks to get the clarification?
Someone either needs to write a custom XSD and link it to the document or we need to disable validation for those examples pending another solution.
This request is unclear.
Why are you asking us for samples?
I'm reopening the issue and Graeme will attach a new key which should be an OpenSSH SSH2-DSA key hopefully.
Nobody is able to tell what will happen here if something does not work.
Additionally... GroovyShell doesn't care about threading.
And even if it would be who makes sure that the context loader at the time the shell was created is the same as the context loader at the time the shell is used?
Can we even be sure that the thread is the same?
Logic like that has nothing to do with GroovyShell in my eyes.
If a threaded management is needed then a new class should be used that ensures the conditions are fulfilled.... that is at last that the context loader is unchanged.
I know in your webapp this condition might be fulfilled but GroovyShell is not special to web applications.
GroovyClassLoader does not know Groovy classes the parent *must* know them.
Because of that the default parent is the same loader as the loader that loaded the class GroovyShell - this loader knows the groovy classes.
Setting the context class loader would require a test if the context loader knows Groovy classes and if this class is the same as the expected loader... what do we do if that is not the case?
Throw an exception?
I don't want to put too much magic in the classloader structure because it becomes messy as hell in an instant.
Maybe first see the results of the discussion at: http://www.nabble.com/Undocumenten-constraints---consistency-of-templates-td21863067.html
I'll make that change and close the issue.
Woo hoo!
Why attach the page?
In other words using the setGenerateImportedSchemas() setter or not obviously makes a difference and given the fact that your main XML schema imports/includes the other one I'd suggest that you change your code generation strategy.
That's basically all I am saying.
I know this is a closed issue but here goes anyway.
First there are some backwards compatibility concerns.
Clearly you don't understand the goals of Velocity or this is one hell of a humor piece.
Is this according to the spec?
Do I miss something there?
Of course in the end I'm not the one you need to convince as I'm not a maintainer :)
The ci server does not close issue only comments on them I closed this issue.
This is a really cool idea but I don't think this is going to make it into TRUNK before the 0.4 branch gets cut.
Duplicated here TAP5-1017
Fixed in nightly build 20020623.
The reason the tests weren't failing is because i was an idiot and hadn't configured the queryResultCache properly in the test configs.
now that i've made the tests fail i can finally start trying to figure out why the tests fail.
SOLR-5232: SolrCloud should distribute updates via streaming rather than buffering.
Maybe instead of overlays try configuring maven-war-plugin {{<packagingExcludes>}}: http://maven.apache.org/plugins/maven-war-plugin/examples/skinny-wars.html
I wonder if we could do some sort of auto-detect  I'll poke -- getStream().mark()/reset() can probably work for XML/JSON but it may break things for javabin
Erick   First of all anyone upgrading should examine CHANGES.txt which would certainly have a note on this explicitly in the upgrading section of that document.
Secondly most point-release upgrades people do are ones in which the config stays the same.
Thirdly the scenario you present in which someone copies a subset of their 3.5 schema would only error if that person ALSO chose to omit the defaultSearchField declaration -- but why would they do that?
"Besides I don't think there should be any expectation of ""it'll just work"" if you copy some arbitrary subset of an old schema into a new example config."
If that's true it'd be nice if we had a standard way to annotate such code so we can methodically remove the old stuff.
"""@deprecated"" isn't enough because it's just at the method/class level not an internal if branch."
{quote} you lost me there ... they can use the exact same configs ??? that's kind of the point: testing the exact example configs as we ship them (with <lib/> declarations that point at dirs which may or may not contain jars depending on what contribs are built and request handler / response writer declarations configured that use lazyloading to dynamic load things as needed.
Ah - i jumped the gun on you.
This has been the history of Lucene and the thing about the project that I admired most - heavy hands where checked at the door.
I'm no angel and I'm not perfect about this myself.
But I still try and police it because I think its important.
My personal beef in this issue is not with that though - I simply am very offended by heavy handed reverts and such.
I feel the same way about JIRA.
Words before action in my book - many words - hard as that can be for all of us sometimes.
explanation on SM-623
Should this issue be closed?
Didn't we just do this a month ago?
Thus the case is closed as non-reproducible.
The included things (usually chars or tokens) in the span are represented here as an interval and the end point is excluded.
Have a look here: https://en.wikipedia.org/wiki/Interval_(mathematics)#Excluding_the_endpoints
If the dictionary were created with the flag to true then all entries would use a case sensitive comparison.
This JIRA will be closed.
I think this is the third one you created in the same day?
Looks good to me.  Mike
Committed to trunk. Thanks Sergey!
Committed to trunk. Thanks Prasanth!
Thank you for the patch Xuefu! I have committed it to trunk!
Committed to trunk. Thanks Teddy!
Committed to trunk. Thanks Sushanth!
Committed to trunk. Thanks Sushanth!
Committed to trunk. Thanks Navis! [~thejas] I will recommend inclusion of this bug fix in 0.12 as well.
Committed to trunk. Thanks Harish!
Committed to trunk. Thanks Harish!
I see you rebased the patch thanks a lot.
Committed to branch. Thanks Remus!
Committed to trunk. Thanks Gunther!
Committed. Thanks Gang Tim Liu
Committed. Thanks Navis
Committed to trunk. Thanks Thejas!
Committed. Thanks Jarek
Hi  Thanks! Too be honest I haven't looked at noop much. It was just used in the original patch. I'look at your updated patch and see what more work needs to be done.
Committed to trunk and 0.9 branch. Thanks Arup!
[~shreepadma] thank you for working on it.   Do you have plan when you submit a patch? I know it takes efforts to code and test in 4 different types of databases.   Would you please release mysql version first? Then file a jira to follow up Postgres/Oracle/Derby. Just a suggestion for fast release but up to you to decide.   thanks a lot again Tim
+1 Thanks Sambavi
Committed to trunk. Thanks Prasad!
Committed thanks Jingwei.
Committed thanks Sambavi.
Committed thanks Maheshwaran.
Committed. Thanks Tim
Committed thanks Namit.
Committed to trunk. Thanks Tim!
Committed to trunk. Thanks Francis!
Committed. Thanks Robert!
committed thanks Xiao Li!
Passed tests and committed to trunk.  Thanks Charles!
Committed. Thanks Kevin!
Committed and was my first commit. Thanks to all.
siying can you rebase this patch? thanks!
----------------------------------------------------------- This is an automatically generated e-mail. To reply visit: https://reviews.apache.org/r/898/ -----------------------------------------------------------  (Updated 2011-06-15 20:24:15.631412)   Review request for hive.   Changes -------  address Ning's comments. Did the minimum change and the performance is acceptable. We can try to remove empty path check if in future we see the latency is not good.   Summary -------  speedup addInputPaths   This addresses bug HIVE-2218.     https://issues.apache.org/jira/browse/HIVE-2218   Diffs (updated) -----    trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java 1135335    trunk/ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java 1135335   Diff: https://reviews.apache.org/r/898/diff   Testing -------  yes.   Thanks  Yongqiang
Committed thanks Jiayan!
Committed. Thanks Ning
+1 Patch looks good
Committed! Thanks Namit!
Committed! Thanks Liyin!
Committed. Thanks Yongqiang
Committed. Thanks Siying
committed to trunk. thanks Ning.
Committed. Thanks Ashish!
Committed. Thanks Namit!
Committed. Thanks Ning
Committed. Thanks Yuntao!
Committed. Thanks Zheng!
Committed. Thanks Namit!
Committed. Thanks Namit.
Thanks Zheng!
+1  looks good - will commit if the tests pass
Hi   Iâm trying to use hive performance benchmark. I read README file and followed step by step and finally I reached to the  data generation part. Teragen has no problem but I face a problem with htmlgen.  I configured something to my own cluster (40 VMs) and launched generateData.py  but it never finished! I launched it about 10 hours ago..  So I modified UserVisits and Rankings in config.txt to be smaller but it was also failed. Even it consumed very little resources when I saw 'top' information in the ubuntu.  Have you guys ever met this kinds of problems?  And if you have how did you solve this problem?  Thank you so much
Committed. Thanks a lot very much Ashish!!
Committed. Thanks Yongqiang
committed! to branch hadoop-eclipse-merge. Thanks Srimanth.
+1 patch looks good.  The checkstyle.xml is specified in hadoop-project-dist/pom.xml .
Thanks Uma for looking at the Jira. I think it should work fine with BPOfferService#registrationSucceeded(..) synchronized. Will post a patch soon after verifying it.
the patch looks good to me aside from the lack of unit tests.
+1 patch looks good.
+1 patch looks good.  This cleans up a lot of code.
Thanks a lot for the review Todd.  The test failure is unrelated and is already tracked by another JIRA. I'm going to commit this momentarily.
I have committed the patch to trunk branch-2 and branch-2.0.4-alpha. Thanks you Jagane!
I've just committed this to trunk and branch-2. Thanks a lot for the reviews/testing Chris and Todd.
I've just committed this to the HDFS-347 branch.  Thanks a lot for the contribution Colin.
The patch looks good to me. Some minors:  1. javadoc for two getINode() in SnapshotDiffList.java  2. We have DirectoryDiff ChildrenDiff Diff SnapshotDiff this is a little bit hard to follow. How about some renaming like SnapshotDiffForDirectory SnapshotDiffForFile?  3. SnapshotDiff#combinePosterior(final D posterior final BlocksMapUpdateInfo collectedBlocks)    can be renamed to deletePosterior or combineAndDeletePosterior so as to distinguish with Diff#combinePosterior?
"""The latest patch looks pretty good to me. Two small nits +1 once these are addressed: # Generally in the Hadoop codebase we put the """"@Test"""" annotation right above the method declaration below the method comment: {code} +  @Test +  /* +   * test #toString() for typical VolumeId equality classes +   */ {code} # In the class declaration of INVALID_VOLUME_ID I recommend putting a blank line between the methods being declared there.  Thanks a lot for improving the test coverage in this area of the code Ivan and thanks a lot to Andrew for your reviews of these patches."""
That sounds good. New patch uploaded.
+1 patch looks good.
Thanks to Todd and Colin for the suggestions this is much cleaner.  I'm still looking for pointers on where to go for my Checksum byte-wrangling (now in FSEditLogOp#getRawChecksum).
Thanks a lot for the review Todd. I've just committed this to the HDFS-3077 branch.
Thanks Kihwal!
The patch looks pretty good to me. One question for you though: does loosening this check for the QJM case not unnecessarily weaken the check in the non-QJM case?
The patch looks good to me. +1 pending Jenkins.
I have committed this.  Thanks Junping!
+1 Looks good I will commit this patch.
+1 patch looks good.  I have committed this.
"""Hi Ivan  Thanks a lot for the patch. Few comments.  1)  Patch does not apply cleanly. many Hunk failure while applying patch.  {noformat} Hunk #13 FAILED at 264. Hunk #14 FAILED at 281. Hunk #15 FAILED at 293. Hunk #16 FAILED at 309. Hunk #17 FAILED at 332. Hunk #18 FAILED at 345. Hunk #19 FAILED at 362. Hunk #20 FAILED at 372. Hunk #21 FAILED at 395. {noformat}  Could you please regenerate on the correct trunk code base?  2) TestBookKeeperHACheckpoints.java:   {code} conf.set(DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY +             BKJMUtil.createJournalURI(""""/checkpointing"""" + journalCount++) +             .toString())"""
Looks very good. +1
+1 looks good. As for the naming: [JIRA].patch stands for trunk patch IMO. Any branch designations can be add as qualifiers. Thanks
+1 Thanks Harsh the patch looks good.  > Also might it be reasonable to mark just isInSafeMode @InterfaceStability.Stable?  Since this is a new API we are not sure if want to change it.  Let's leave it as-is for the moment.
Thanks for reviewing Suresh. Committed to trunk.
I just committed this to trunk. Thank you Uma.  The patch does not apply to 0.23 branch any more. I recommend it for back porting.
Thanks a lot for fixing those issues Alejandro. I re-ran all the tests I ran last night and everything worked great.  Additionally I tried out test-patch.sh which seemed to work flawlessly.  +1 patch looks good to me.
Committed to branch thanks Eli
Thanks a ton for your review Todd. Here's an updated patch which addresses all of your comments except the following.  bq. In EditLogFileInputStream why do we need to pass isInProgress as a boolean? If it's in-progress that means we don't know the lastTxId so it would be INVALID_TXID right? So we can implement isInProgress() by just comparing lastTxId.  Because in the case of pre-transactional ELFIS we pass in INVALID_TXID even though the log is not yet in progress.  I'm still working on producing an updated patch which also adds testing for the pending DN message queues.
Great I think we are all on the same page then! We'll move forward down this route. Thanks for taking a look.
I have committed this.  Thanks CW!
I have committed the patch for trunk.  Thanks Tanping!
+1  Looks good to me!
"""Hi Hairong the speedups you measured with this patch are very impressive.  It would be great to have it included in the system.  The patch still applied successfully to current trunk when I tried it.  So can you please go ahead and """"submit patch"""" so we see how it does with test-patch?  And I'll review it and try to get someone else to also.  Thanks!"""
+1 patch looks good. Please submit the patch for verification so it can be committed into the trunk and merged all the way back. Thanks.
I've just committed this. Thanks Edwin!
Looks good.  +1 pending Hudson.
+1 patch looks good.
Committed to trunk and branch-0.4. Thanks Rohini.
+1 looks good.  I'll commit once HCATALOG-239 is in.
Committed to trunk. Thanks Enis!
Integrated into trunk and 0.96. Thanks.
+1 on cursory review.  I like the cleanup and the tightening checking target servername too before going ahead w/ op.
Thanks
Jean-Marc can you review it? Thanks.
I've committed this to 0.940.95 and trunk. Thanks for the patch gautam.
Hi Ted Matteo  Thanks for the review.  The commands.rb has some existing logic that tries to 'rescue' some exceptions. It will get more and more difficult to keep it generic artificially as we evolve and add more ... The approach to expand (instead of shrink) this logic is not a bad or ugly one ...  But your suggested solution is fine.
Looks like the 0.95 issues are timeout related...should be good to go.
+1 for 0.95. Thanks.
trunk and 0.95.  Thanks for the review JD.
committed to 0.94 0.95 and trunk  Thanks Jerry for the patch and Stack for the review!
Committed this. Thanks.
I'm +1 on this patch for trunk and 0.95 after looking at latest version -- thanks for addressing the review -- and given you fellas are running it.  Let me retry hadoopqa to see if the above fails are just sporadic.
hi. Ping? :)
Integrated to 0.94  Thanks for the backport Jeff.  Thanks for the review Lars.
What the hell?!
(Do nothing): Are you serious about that?
"@Oleg please accept that my ""logical"" is different from your ""logical""."
So I'm fine with your logic but given that I don't understand it please don't get upset when I ask questions on how I have to use it.
This was my bad.
The rest are *totally* unrelated.
Oh fuck :-) Sorry. Not my day more beer needed!
"Sorry I guess I'm against ""never computing this shit""... because you guys think returning NaN is ok."
if you want to make these optimizations fix the APIs so its intuitive otherwise no way.
who the fuck is asking for scores but not the max score and why does their insanely specialized use case justify all these booleans on a central lucene class.
I don't care about code duplication here.
We should not invoke the global synced fieldnumbers shit for every element only when the setting actually changes
I disagree... and I guess I'm willing to go to bat for this.
HOLY SHIT!
I think the comments here are a bad sign... I sent us down an unfortunate slippery slope  we are a search engine library!  don't put shit in the index directory! or we will delete your shit. dead simple.
I wholeheartedly agree this API is confusing as hell.
Not only it's the first one in my life I'm seeing multiple ways of closing the same thing it also conceals programming errors.
Sucks to rush a commit when an issue is under discussion.
I'm against such heavy committing myself without some consensus to do so.
I'm confused.
> I hated that aspect of working for commercial companies.
My bad.
My bad.
Igor is an idiot and we *do* need gmake.
Shit I missed a cast.
But even if it wasn't it would be totally worthless where it is at.
This is clearly bad webserver behaviour.
Forget what I made(D8067).
PreCommit-HDFS-Build is stuck.
So it is stuck somewhere.
I don't want one-offs like this to become lost and forgotten.
But now that I think about it it may crap out when coming back to read even on a recovered file.
@Matt... I'm an idiot I used the wrong patch. Sigh.
I'm however very interested in this feature as it is essential to me.
bq. this is solr's fault by having a getter that does some heavy duty xml shit.
Holy complicated-as-shit-algorithm Batman!  The complexity of our implementation vs the complexity of what we're actually doing is starting to worry me here.
It also happens at the very startup after he has loaded only 3 sstables (hence DataTracker likely have 3 sstables not a shit load).
Shit my bad. The posted patch is from a completely wrong branch of my git repo - I had not realized I ran the test-patch against a branch not branch-1.
(In this particular case Hoss clearly intended to use the no-no word as an intensifier and not to stir shit.)
Maybe only one of us should play lawyer and the other one play engineer.
I used to be the latter but then I went to law school and got a degree in this shit.
Then to make matters worse since the mid-1990's I've been offering pro bono advice to non-profit foundations whose members insult me regularly and assume I'm an amateur.
Someone but I'm not sure who owes me a public apology here.  /Larry
"Shit there is an dependency. Please go to Preferences -> LDAP -> Entry Editor and enable the option ""Show operational attributes""."
that's bad Content-Type from gSOAP.
Pull it back in if you think different.
Hmm ... this is quite bad.
You are messing down deep below hbase in dfs.
Damn it sry lost the orientation.
Oh my stupid:)
oh ...geez I'm an idiot.
@jira - I wish you didn't suck so much.
Indeed that would be VERY bad design.
My bad.
(How bad is this?
I'm confused.
This is pretty trivial.
Of course it could...  :-)  DOH!
Then you're pretty well screwed.
Damn Chuck is scary.
This is bad.
Well that sucks.
Bad memory.
"The state stuck at ""Allocated state"" It is a dup of 3788."
This is a pretty awful bug.
Replaced patch with fix to catch more bad variables
Does this suck?
Of course StrFieldSource is a private class  No it's not.
CSS is really confusing sometimes.
Django is a fullstack framework Click is not.
My bad.
Credit of course goes to Dick.
It seems really confusing.
+1 for this: my bad.
Hell or high water.
-1 to jar hell.
That would suck.
If so a test that succeeds with errors is confusing as hell...
If the different segments are somewhat homogeneous then these stats should pretty much be very close anyway.
Hi Amardeep  Are we sure that this does not introduce any side effects regressions?
Hi Jacques  The http*.jar are Release 4.0.
So now I have convert again all the head3 to h3 and now seams to me better.
hi charlie  if you request this feature for myfaces-core (the jsf impl itself) you have to file the issue at https://issues.apache.org/jira/browse/MYFACES  regards gerhard
A message is already logged.
Continue processing is the right way i think.
@ 1.
It's a feature which seems to be enabled by default.
Its implemented in a normal release.
@2.
That would be a improvement in trinidad but that doesn't solve the core problem in MyFaces.
@3.
I will make the change for 2.2.
That's why I need to ask questions now when I want to understand the current logic (as it is not logical for me).
If I put my hands in the code to fix stuff myself I will probably introduce something that is illogical for you (e.g.
"the logical solution to me was to add the methods to the interface) so I guess I did the right thing opening this bug and asking for your opinion and I can't accept your ""for the sake of sweet Jesus"" in reply."
In many ways I think that the support for 3rd part performance and health monitoring is already there.
5% seems pretty low.
We can use a BlockingQueue to make the patch simpler
I think we should distinguish between the two
{quote} viewColumn v.s.
{quote}  Neither of these produce a copy.
it's not a merge setting and I think it's rather confusing today since we take the merge setting and use it for indexing.
More tomorrow.
Same for the settings.xml with cleartext password (this caused my initial 401 problem no way in hell im leaving such a thing around on my computer).
Maybe ant could read the password like it does for GPG and handle this for us?
Thats why I think its good to keep them (of course as short and minimal as possible).
I think if you use either LogDoc/ByteSizeMergePolicy forceMerge also does what it says.
{quote}  I fail to see why that is a bad thing if we're looking at the rare scenario of having to postpone the sorting decision to search time.
If we're so bent on backcompat I suggest deprecating one of these methods now and removing it in next major release.
The patch also removes FreqProxMergeState because we don't have to interleave posting lists from different threads anymore of course.
The patch also removes FreqProxMergeState because we don't have to interleave posting lists from different threads anymore of course.
Easy to say we can come back to this easy not to.
ConstantScoreQuery and FilterQuery need to AND the filters   together.
It uses a parallel benchmark that roughly models what our real-life benchmark is like.
Too bad we don't have the power to do that... -)  Here's what I think should be happening:  Internally absolutely everything should be handled UTF-8 for consistency's sake.
Nobody has any permissions unless explicitly granted.
While I am not a spokesperson for my employer by my assessment their interests are aligned with mine -- they sponsor my work and as a consumer of the products that I contribute to it would be bad for them if their investment were to be harmed.
and close this issue ?
How should integer division work for decimals and floating point?
e.g.
should the following be true?
4.5 quotient 1.5 => 3 5.9 quotient 1.5 => 3
"OFFSET X LIMIT N  (slice X _   (top (N ...)     (...)   ))  that is put a slice over the ""top"" and apply the top to N+X."
I will make the change.
I think I would be happy with a process where a weinre build is kicked off for every commit to master.
Hi  Could u please change the moderator of this list to Davanum Srinivas : dims@apache.org  Chamikara
"Okay ""unfortunately"" nothing  abnormal there."
Otherwise the constant will be converted   to DOUBLE.
{quote} Dear Azuryy Yu  This message acknowledges receipt of your ICLA which has been filed in the Apache Software Foundation records.
Contributed by Ashish Singhi.
It will be taken care as part of HADOOP-7642.
I think the rename should follow the same pattern.
If so make the formatting same as it is for rest of codebase on commit?
Neither is the rest of this.
I'll make an updated patch.
I think this is what caught me the first time I tried this
I'm going to close this out.
Marvin I've clogged these lists too often today so I'll just say this to you.
Under copyright law there are several potential kinds of damages one can obtain for infringement.
The only forms of damage that make practical litigation sense in the FOSS world are *statutory damages* and *attorney's fees*.
To obtain those kinds of damages a copyright notice is required.
(There are other requirements for these types of damages also all described in 17 USC 401 et seq.) OTOH if you want to waste your time proving *actual infringement damages* for free software don't bother with a notice but also don't bother trying to find an attorney to argue your case.
Forgot to close
I forgot to resolve this bug.
I think there is enough there to close the issue.
sigh.
CASE_SENSITIVE boolean=> TRUE    9.
Ummm did you forget to annex the patch Toby?
You cannot load classes from a bundle after the framework shuts down.
Of course none that should really matter .
Of course this means that I can't compile/build the code using AxisRC3.
Clearly one Visitor interface just can't satisfy everyone.
This is going to be a big performance hit.
Committing shortly.
Any objections to committing the security.rb part from the trunk patch to the 0.95 branch now?
Like skip.jar.loading= true once ---> Proposed behaviour.
Is not really needed I guess.
Then what's the story for code reuse for different flushers.
"""else { this.services.getAssignmentManager().assign(e.getKey() true) }"" is called for another assignment."
OK.
It is ok for goog.requires to be missing.
It is not ok for the interface reference not be fully qualified.
OK done added some more javadocs as well.
) then I'm ok with that as well.
Ok.
OK to commit to 1.4.4
"Also I find the name ""stillGoodJournals"" confusing because when a journal was found to be bad in the ""// do the sync"" block it isn't removed from the ""stillGoodJournals"" list."
With balancer initialy at true: {code} hbase@node3:~/hbase-0.94.3$ bin/graceful_stop.sh --restart --reload --debug node6 Disabling balancer!
I just hope it does wait forever on sockets or anything stupid like that...  but we'll see.
Unrelated should the PBImpls be moved from yarn-api to yarn-common (They're private anyway).
Unrelated should ApplcaitionTokenIdentifer be renamed to something like AMTokenIdentifier ?
This also seems pretty reproducible.
if (!jj_semLA || jj_3R_60()) return true     if (jj_3R_61()) return true     return false   }  The simple fix could be to wrap the lookingAhead assignment  after the call within finally block.
The true   usages are found in calls to executeSubstatement now.
But even if it wasn't it would be totally worthless where it is at.
@filipe I forgot the case where includes are in include/js (or mozjs).
"audit warning seems to be due to ""hadoop-hdfs-project/hadoop-hdfs-httpfs/dev-support/findbugsExcludeFile.xml""   javadoc warnings also seem to be unrelated."
But if the expression isgraph(wctob(wc)) evaluates to true (which cannot occur for wc == L' ' of course) then either iswgraph(wc) or iswprint(wc) && iswspace(wc) is true but not both.
If that's not true then you can pretty much ignore everything in this particular comment...
It just depends upon how smart the plug in framework and the actual plug in is.
Plus of course you don't have to hardcode the name of the jarfile.
Currently we just assume it might be a laggard and thus isSlowTracker() will return true.
Rajith: This is actually not true.
@ Thomas  Your framework is available?
The latest  patch for SimpleURIResolver and resolver framework.
Matt what's the cut-off date for providing a patch?
br Matt
*.. info.ftl might have confused you :)  Divesh
> I'm confused.
Pretty fancy.
Though it is still a possibility it is pretty corner-case.
I think there were a mistake.
Of course this is debatable.
Of course I have en_US for my locale.
For testing purposes of course...
True enough....
"This is also true for ""ACCT"" ""MLSD"" ""OPTS"" and ""REIN"""
That is not true.
Default is true.
"Therefore Visa.matches() will now return ""true"" for Mastercards too."
Yes that's true.
True.
Definitely true!
Pull it back in if you think different.
But that's of course completely unrelated to your patch.
duplication of such a small amount of trivial code is far less evil than yet another dependency.
"This might be a stupid comment but shouldn't ""keyDerivationeterceptor"" be ""keyDerivationInterceptor""?"
HttpClient stock code is JDK 1.2 compatible (hell 1.2 is old).
The first step is of course selling us through our site.
Brandon's pretty confident but I'd rather not cram it into a minor release just in case there's something we missed.
I encounter the same NullPointerException in the 4.1.0 source code release but currently I don't have a git 4.1 environment to verify it.
Matt are you still working on this?
But in any case please keep things simple and stupid :)
James I think it is time to do some of the tasks that you have outlined and I'm willing to do the work.
I'm fairly convinced that signing is the only way to go without turning the knobs so far up that the AM is pretty useless (no images!
Start an app with all network connections off and try to do a console.log(navigator.onLine) and it will return true.
Please do not forget about adding the ASL header to all new classes.
Matt
Matt
I will take this into account of course.
Ah of course.
"Are there other parts of your app that interact with or manipulate the ""course"" bean or its ""labKits"" collection?"
The name for all of them was thus pretty obvious except for engine.evaluate(...) and #evaluate(...).
Must have been confused.
Of course that would be a separate bug...
Of course I meant 2.1.8 beta not 2.1.8 dev.
This seems backward to me (but of course I'm no expert in this area).
Specially the framework.
This may fit for Pregel but not for our Framework since our communication is much more expensive.
Patch is pretty trivial but gives +15% on MT/SerialBench on 2-core Core2.
OK works for me.
Please re-open if I'm confused.
I must admit I'm a little confused by this one.
You mixing fd and HostComparator confused me.
Otherwise the HTable and the HConnection will get very confused.
Of course there is no NotNullException.
of course it could be built on the fly...
Of course this only applies if someone where to finish 831 and it where accepted...
Of course if you're writing your own serializer it's your responsiblity to deal  with this appropriately.
After a while that .api package will get pretty crowded too.
I will post a new patch pretty soon.
Framework translations in rev 701679.
How about a slightly different take?
Damn copy/paste programming.
Pretty impressive.
If so that's a pretty bad admin experience.
It seems pretty reliable.
I think the ability to hotswap a valve or even an entire pipeline real-time is pretty damn kewl!
Well your static analysis seems pretty persuasive to me.
What mocking framework should be used for the webui?
The framework is incomplete without it.
In revision 1064741 the framework is functional and activated for the enhancer.
At least it was intuitive for me and I am pretty new to all this.
Is there a way to stop the hbase with the regionserver dead?
If I am not wrong the follower that is stuck never accepts the leadership of the Leader in the ensemble because it is an established ensemble and it sees no quorum in it.
The 0.95 branch is dead and defunct.
I'm assuming it's bytes since the variables are named 'buf' as opposed to the 'str' names used in the protocol APIs but want to double-check.
But I didn't get any memory errors.
It was confusing for me too.
Hmmm... confusing.
Doing things based on an organizationPartyId associated with the user is BAD BAD BAD and basically kills the multi-organization support in OFBiz.
I deleted the original attached files because they contain company informations that I'm not allowed to disclose.
So I'm totally +1 for the HBASE-8723 patch.
The verification of state over in timeout monitor before acting is another aspect?
The number of failed tests is pretty high compared to last time's performance (67 bad vs 3 last time).
Too late I'm away to bed and have lost concentration.
Converting to a double[ ] is where the precision is lost.
"And PayloadTermQuery lost an ""if (includeSpanScore)""?"
I'll have to bisect the revision since when it is broken and this could take some time.
The CDE is using the base UIMA framework to get the metadata.
However I'd be interested if later this functionality can also be used by ServiceMix i.e.
It's named .gitignore which may be confusing.
The original replica of RUR may in finalized state.
I just noticed that!)
volatile prevents the variables from being cached in the registers and creates a memory barrier so that's not that surprising to see an impact on the performances.
The Felix framework does not support exporting/importing the default package.
Yep my bad too - I didn't look and was just guessing at what the problem was.
"- the computedHashCode() didn't use the a single ""qualifier"" now it does."
"Even during the full-blown ""validation"" of the descriptors (done just before ""saving"") the CDE is invoking the base UIMA framework in  special mode to prevent instantiation."
Move startTime from DecommingStatus to DatanodeDescriptor so startTime could be used to represent either decommissioning or dead start time 2.
once this error occurs the route cannot be used any more.
However controller passing live brokers minus shutting down brokers as live leaders in the LeaderAndIsrRequest sounds like a premature optimization and a bug.
The memory stream transformation core-dump was the same case (rb_tree - erase).
However the statement still performs a full scan of SUMMA_RECORDS which sounds sub-optimal.
> Actually I just noticed something when trying to commit.
Wicket 6 now features an experimental module that integrates the Atmosphere framework (wicket-atmosphere).
Then the algorithm gets lost as it did not expect this discontinuity.
A single table could be made up of many thousands even hundreds of thousands of regions?
bq.In order to maintain different states at which we are in while creating table should we go for a special node under the table znode or can we add new states to the existing table states like (DISABLING ENABLING DISABLED etc) IMO go with separate node.
Some further bug tracing shows that pdfbox' RandomAccessBuffer is responsible for the Out of Memory error.
#2 wontbe so bad... filters are pretty deep and will be just as efficient as hacking scan query Matcher I think.
Maybe you want to do shuffle in this way: Firstly copying remote map outputs and store the shuffled segments into memory as usual.
The outer read can continue for a much longer period of time.
The fix is only disabled on releases prior to 10.8.2 on the assumption that the fix gets into the upcoming 10.8.2 release.
Note that I wouldn't be totally opposed on adding additional support for the dot notation on inserts (provided all fields are indeed given) for symmetry/convenience.
I don't know that there's an alternative that would be less invasive than reverse logical time.
I don't quite see why the attached implementation would do more heap operations but I agree that it could be slower due to lengthier/more complex loops a few more if statements etc.
So what this patch does is totally revamps the makefiles for erlang.
"If I get to this first I will make the ""frameworks"" fix as well."
Show time since declared as dead on the deadnode web UI.
just after the event) is the opposite of the sign just before the event.
The times will be grep'ed and used for inspection in Excel.
Unfortunately the xml parser in use is event based in the wrong direction(push instead of pull) so the bulk of the change is flipping that around.
2) The ZKVersion used in the <topicregion> map is used to guard against the case different hub-server use the map at the same time (or interleaved).
html/ext/HtmlInputHiddenTag.java is pretty simple all of the functionality is in its base class.
Do you have a different desired text change or is it dead?
gcc 4.3.1 is out since June 2008 so the workaround is no longer needed.
The KahaDBStore.getDestinations() method looks as though it would be pretty heavy performance wise.
Tag is documented and demo'd but does need a Facelets tag.
I would rather state that most Java libraries *do* care about Maven.
Shouldn't it have looked at fstime across the directories seen dir2's higher fstime and used that one?
> The modified test (below) gets stuck in an infinite loop  Your test is buggy.
I have tried using Snappy{Input/Output}Stream and they don't work (it just throws EOFException in the middle of the transfer) and causes stream to retry.
this is IMO a regression since i am pretty sure it used to throw an InvalidItemStateException in the past.
I'll take a look at where the framework should call it.
"Similarly one can use the overview of FD:OCA below and the introductory section of its respective volume and only ** refer to the details of the FD:OCA constructs as needed during implementation."""
Didn't see the selector the first time round.
This is pretty interesting.
Doh!
We do not use a framework.
doh fixed braces
I will appreciate if anyone knows of a way of doing that in junit framework.
If you are stuck with solr 4.0 alpha here's a patch which may work.
This patch also adds the new resource emulation framework to GridMix.
We need verify all the links and buttons in console to ensure the fix.
avro-mapred -- we might end up with avro-mapreduce as well for the newer api so I stuck with the package name of the hadoop api.
Sounds reasonable.
This sounds reasonable to me.
Content-wise the only part that stuck out to me was the inclusion of the many exceptions that users  will never see.
It's unfortunate that there's a lot of code duplication but it might be best that way.
Sounds reasonable to me given it's a standalone (debug) utility.
It is a little weird since the multi-core aspect would only be usable programatically but that will make it possible to easily bat around a 'core manager' and http design.
Can't be avoided though given what is going on here.
Pretty much the same stack trace.
I don't want to pull the whole patch though.
"However the NPE in the framework is still an issue even if it happens when there is a ""user error""."
Moreover there must be something going wrong in relation to the SPO index an exception is thrown:  ERROR **** Not the root: 11264 com.hp.hpl.jena.tdb.index.bplustree.BPTreeException
Back-compat Lucene-style and agile design aren't something that marries well.
We are currently stuck with this issue?
Specifically the existing framework only lets us plug-in custom compress/decompress 'streams' (e.g.
If you really don't want such a link you could always hide it with CSS.
DROP INDEX <INDEX_NAME>  making this the only option right now would suck because we don't auto-generate an index name when none is provided.
There are just too many checks there that Click is not using and we can't just use them all (many don't make sense to Click at all) :).
I'm not against fixing the CS I just want to ensure we do the 'right' fix.
That's why I'm going to use easymock as SMSC simulator.
Downgrading this as it is probably user error (rather than an explicit bug in the framework).
This was fixed Cassandra-side for 2.0 in CASSANDRA-5702 but the fix was pretty heavyweight and backporting to 1.2.x is not an option.
#1?
Nothing stops a slightly evil user to use a NoLock to wreak havoc in the index.
I pretty much have this working.
Please use git diff to generate patch because I can't easily apply git patch on a svn tree.
Does that sound like a reasonable plan going forward?
I won't intrude on the design discussion sounds good.
It can't have been this way always?
Fixed using the new injection framework and xwork configuration design.
Patrick  I can't manage to reproduce this error.
Otherwise we would be storing the bytes-per-checksum field but not actually using it which would be really confusing.
if it gets started at all.
[~jbellis] The weird thing is that although *newSSTables.size() != newSSTablesSize* the assertion doesn't actually cause an error so no stack trace but swapping the assertion (as shown below) yields the stack trace below.
J-D gave my patch a review and said fine except for some line-lengths which I've fixed.
To be quite blunt guys your log4net logger methods smell bad.
We've been passing builds pretty regular now so if this a problem it should start showing prominently.
You just need to remember to put the expected first in the assertion or it gets confusing
That can't be expressed with a static suffix but it's easily expressed with a regex.
I'm not sure this can't be achieved without messing with IR/W guts so much.
2.8.0 is no longer supported so even if there is a bug it won't be fixed in 2.x.y release series.
In TestCliDriver create_view.q and udaf_percentile_approx.q are two weird queries.
By passing in the section name {{null}} the content of the global section can be retrieved.
I would have checked it in directly but there were some aspects  that I didn't care for but couldn't seem to get around.
Submit a patch and I'll play with it.
Please play around with it.
But I'd have to play with it some to see for certain.
"This is to say that we've a pretty good coverage of IDC ""keyref"" use cases."
After moving it to a local disk I can't reproduce it anymore.
click the subversion commits tab.
I just realized I never really explained the original problem that led me to making these fixes.
So this is by design.
We can borrow some design techniques etc.
+1 For this new design.
I don't know what Lucene's position is on Maven but am interested pursuing whatever makes sense.
Marcus that sounds sensible.
I think MINA is more of a 'server framework' rather than an 'NIO framework'.. it is so close to being both.
Perhaps this should be item #0 decide whether we want to work with Maven or Gradle.
All other log frameworks do this)
Is there a way to fetch a pull request?
Please pull it up to the release branch.Thank you!
Will pull that into the 2.0.4-alpha branch.
Maybe something silly is going one.
For the first part I got stuck on two points.
You really shouldn't call one without the other.
filtering hooked up through Solr already and it would be dead simple to bring in here too but it would fit nicely in this framework.
Every framework will benefit from that!
* Framework for JMS db access can be reused.
:org.osgi.compendium-1.0.1artifact and felix-1.0.4 framework
CFS.gRS is a pretty thin layer over RowIterator.
this pretty much explains the three imports.
It is pretty close to what you are proposing.
Oh and not to forget.
Jerermy is the sole subscriber/moderator and can take care of adding other subs.
Pretty much every deserialization error shows up as an IOException.
Are you planning on doing this (or already have done this) or would you like me to do it?
Doug you defined: +public interface NamenodeProtocols +  extends ClientProtocol +          DatanodeProtocol +          NamenodeProtocol +          RefreshAuthorizationPolicyProtocol +          RefreshUserToGroupMappingsProtocol { ...}  Will this prevent us from putting the DatanodeProtocol in a different port?
But would there be other cases that the writer stuck for different reasons?
However given the factored out code is pretty trivial I decided to duplicate it in favour of not adding another set of classes along with the additional complexity introduced (e.g.
There is a single directory ~apbackup/remotelybackedup and anything symlinked to there gets backed up as documented in the README.
Jeremy are you still planning to take a stab at this?
Nothing gets broken but we have one more step towards framework independence.
We won't find the same group in a different segment.
Will commit in a few days (to the heavy committing branch) if no one objects.
This patch takes care of the nocommits in the previous patch.
I'll take care of this.
is anybody taking care of this?
There's a TODO which I'll take care of before merging to 4x (there are other renames too).
That would take care of most cases.
Yet now one must care for the relative ordering in between both icons too.
And it also will add a lot of complexity to the framework development/maintenance.
Adding a different patch.
However if the implementation of next is not capable of handling bad records then it is most likely going to throw an exception and the proposal in this jira for handling bad input starts with that assumption.
I imagine that if 7-Zip can do this decryption then the encryption mechanism I seek is not forbidden by licensing restrictions.
Also I won't give you a patch next time but just the whole new TestTransSystem.
That said I had to do something similar in configurators and I did this by adding wrappers to the getInitParameter.
Therefore it can no longer be used.
We have different disk sizes on our DNs.
I don't have a problem with a non-component directory in the framework.
Patch that changes progress computation to use position in input stream.
I do have a patch but unfortunately I am not a position to sign the CLA.
The same exception happens the I forget to sign my jars.
The squeeky wheel gets oiled!
This is where the bus gets over lapped.
OK  The problem is that the o.a.f.bundlerepository doesn't cares about R3 or R4 bundles.
Somehow I forget to move that from SOLR-2524 but it is in the updated patch now.
"Closest example that I can recall were questions about ""The Software shall be used for Good not Evil"" clause from the JSON License."
I am planning on doing a round-robin assignment for each partition.
all the key and column name comparisons should be case insensitive (and we force keyAlias to be ascii so you don't need to worry about it being some weird bytes stuff).
@Paul  Forget the previous patch it added unneeded complexity for parsing and dealing with multivalue (line) ini file  config parameters.
Fixed by updating to the new frameworks bridge 1.0.1-dev.
"{noformat} set-net-2.0-framework-configuration:   [property] Target framework changed to ""Microsoft .NET Framework 2.0""."
USE WITH CARE!
Beware all ye who venture here: the sea of XSLT is deep and wide and unforgiving.
:)  > except at much worse performance.
What's weird is that this  class is in Cocoon's normal classpath.
* This is needed since the MappingTool fails if it gets non-persistent capable classes.
This might fix broken hudson on trunk
Adding directories to removedStorageDirs in original patch is already in branch-1.
The source of this problem is master and slave allowing frameworks to register with IDs already used by completed frameworks.
This is introduced by API performance refactoring the intent is to generate the same response object for both listxxx api and other apis related to the same entity.
There may perhaps be some way to use Windows file-locking as a signalling mechanism here.
Alternatively we could build a variation avro-ipc.jar that shades in avro.jar that could be the smallest unit for OSGi.
Actually let's just fix the issue you discovered here.
we don't have an agreement that it should be active
I'm just throwing it out there.
I agree with Sami that this should be contributed to Tika and that we delegate the language identification handling in Nutch to Tika just as we are doing or planning to for the MimeType and the parsing
Also I looked at first corrupted database that I posted  and found the corruption in the  index TEST1_IDX_INDCOL2.
I could not find any documented limits but if I missed them somewhere I will gladly put them in.
However I doubt that the same is true for MapContext because it is just a wrapper around a HashMap.
"There is no RPC stuck but a lot of ""Connection reset by peer""."
With hampered performance of sstableloader after 1.0 its made the things very difficult for us.
I have retested it and I figured out that I have posted the wrong parameters in the bug description.
I fixed some bad markup in r1392857 but I can see the error afterwards on the neko stuff.
The thing about backwards-compatibility hacks is you tend to get stuck with them.
This is part of making Oak OSGi-friendly
However I'm not a lawyer.
It seems to be adding new 'functionality' to coprocessors.
Adding sleeps solves it.
We're adding endpoint as a new datatype?
Not  ideal but not too bad )
Trunk for patch pretty much.
That'll be pretty awesome to have for 0.6.0
+/** + * This class is a refactoring wrapper around the shared + * JDBC40Translation class.
Also I'm slightly confused by your description because in section 5.8 of the spec it says that members of code_handler_class_RCN can be null.
Since I screwed up the commit we voted 2.1.7 down and are working to get 2.1.8 out pretty quickly.
A user looking at the set of changes won't care :)
Or did you see some other errors or bad weinre behaviour?
Typical POST then DOH moment.
I'll see if I can find a way our systems are pretty constrained in that dimension.
Patch looks good.
Integrated to 0.94 and trunk.  Thanks for the patch Anoop.  Thanks for the review Stack.
Patch v6 integrated to trunk.  Thanks for the patch Jean-Marc.  Thanks for the review Matteo.  @Lars: Do you want this in 0.94 as well ?
Integrated to trunk.  Thanks for the patch Bryan.  Thanks for the review Chunhui.
Committed to branch and trunk thanks Gab!
Integrated to trunk.  Thanks for the patch Rajesh.  Thanks for the review Ram.
Thanks for the finding Andrey.  createDir() is recursive. Would be nice to write it in iterative manner.  Would FileSystem.mkdirs() do the job ?
Integrated to trunk.  Thanks for the patch Francis. Do you want to address Ram's comment about 0.94 patch ?  Thanks for the review Andy and Ram.
Integrated to 0.92 0.94 and trunk.  Thanks for the patch Chunhui.  Thanks for the review Lars.
Thanks for the review jimmy.  committed to trunk.
Applied to trunk.  Thanks Elliott.
Committed to trunk.  Thanks for patch Tudor and review Michael.
Thanks for the review Andrew!
Awesome. Thanks!
v3 up in PB. Thanks for the reviews
Integrated to 0.94 and trunk.  Thanks for the patch Jimmy.  Thanks for the review Andy.
Committed to trunk 0.92 and 0.94.  Thanks for the patch Shaneal.
Hi. Should this be ok to commit to trunk? Thanks.
Mind taking care of this one Doug?  This looks like a 0.92.0ism so needs dnoting so in the book as for 0.92.0.  Thanks.
Failed test is covered by HBASE-5992.  Integrated to trunk.  Thanks for the patch Matteo.  Thanks for the review Andy.
@Ted Thanks for taking a look. Sure I will make that change on commit.
Committed trunk and 0.92 branch.  Thanks for the patch N.
Committed to 92 branch and trunk.  Thanks for review stack.
Integrated to TRUNK.  Thanks for the patch Ramkrishna.  Thanks for the review Michael and J-D.
Applied to branch and TRUNK.  Thanks for the patch Ramkrishna.
Committed to TRUNK.  Thanks for the patch Doug.
Committed to TRUNK.  Thanks for the patch Doug.
Committed branch and trunk.  Thanks for the patch Mikhail.
Applied to TRUNK.  Thank you for the patch Erik.
Committed to TRUNK.  Thanks for review Prakash.
Committed to branch and trunk thanks for the review Stack.
Sounds good. We'll postpone HBASE-2993 til then. Thanks!
Committed to 0.94 (operability) and to trunk.  Thanks for the patch Jie.
Committed to 0.20 branch.  @Stack any chance of some trunk love? :)  If not I will get to it later.
Committed to trunk. Thanks for review Stack and Ryan.
Applied branch.  Resolving.  Thanks for the patch Gary.
Should the forceSync be volatile?  Otherwise patch looks good (make it volatile on commit?)
+1 as well.  Patch looks good like the javadoc we get lots of users asking what those exceptions mean.
Thanks Andrew for covering me.
Looks excellent.  Really sweet.  + Where does the jar come from and whats its license? + The REST API  looks good.  Very RESTy.  There might be little nitpicks later but for it looks great.  How different is it from current REST.  Should the two be the same?  If not how to deprecate the old? + The annotations look interesting.  Could their serializations be used in other contexts the shell say?  Or -- warning -> crazy-thought coming -- somehow producing thrift IDL? + In hadoop/hbase line lengths are < 80 chars (Your new classes are missing apache license classes are missing comments describing what class is about etc). + Is there anything we could do refactoring HTable say so your modeling was easier?  Looks like lots of code in your Cell and Database controllers.   Should our client be taking on this model? + Can you add a note on how you've changed how REST works high-level?  Thats all for now.
Committed.  Thanks for the patch J-D
I now realize hadoop-1662 is required. So i will wait. Thanks.
I just committed this.  Thanks Jim!
Thanks applied OK.
Works for me. Thank you Alexei.
Thanks Vasily - the patch was applied to BTI branch 2.0 at r615105. Please check that the patch was applied as you expected.
Thanks Vladimir - the patch was applied to BTI branch 2.0 at r602881. Please check that the patch was applied as you expected.
Thanks Imran.  Fixed in LUNI module at repo revision r568966.  Please check it was fixed as you expected.
Would you please try the patch? Thanks very much!
Please try this patch. Thanks a lot!
Apllied at revision: 545149 thanks!
Thank you Gregory Eugene. Verified on Harmony-r547521-msvc-debug.
Patch verified at r535584.  Thank you for applying it.
Yes it works ok now thanks.
Ruth patch applied at r526665 thanks a lot for this enhancement please verify thanks.
Thanks Ilya Aleksei - the patch was applied to BTI branch 2.0 at r542776.  Please check that the patch was applied as you expected.
Fixed at r504842 thanks!
Gregory thanks - the patch is applied as expected.
Well the fix looks good enough. Seems there is a lot of dead or half-broken code around st_print_frame() / exn_print_stack_trace() but those deserve good refactoring all in all.
Good ideas thanks Alexey.  I've updated the test patch.
Works fine. Thanks
Thanks Anton.  Fix applied to ACCESSIBILITY module at repo revision r598652.  Please check it is fixed as you expected.
Alexei the patch has been applied as expected. You can close the issue thanks.
Thanks for finding and fixing. The patch applied at r474719
It is OK thanks Stepan.
Spark patch applied at revision r440604 thanks a lot for this enhancement please verify that the problem is fully fixed as you expected.
Thanks Tim Patch was applied as expected!
Richard patch applied at revision r437600 thanks a lot for this enhancement please verify that the problem is fully fixed as you expected.
Hi Paulex  The fix looks good thanks!  Best regards Andrew
Richard patch applied at revision r430325 thanks a lot for this enhancement please verify that the problem is fully fixed as you expected.
Works for me.  Thank you Paulex.
Andrew patch applied at revision r428265 thanks a lot for this enhancement please verify that the problem is fully fixed as you expected.
Thanks for the patch Richard.  Applied to LUNI module at repo revision r418009.  Please check that the patch was applied as you expected.
looks fine thank you George.
Hello George  Thanks a lot. The fix works fine :-)  Best regards Richard
Looks good.  Thanks George.
Hi Oliver   Thanks for catching this problem and for your suggested fix. We can go one step in simplifying the fix by removing the need for URI altogether if we use the getResourceAsStream() method of ClassLoader rather than getResource(). Modified version of your fix committed in revision 405036. Please could you check that this version of your fix works in your environment.   Best regards  George
Thanks Richard  Patch was applied to LUNI module at repo revision 397522.  Please check that the patch was applied as you expected. (I've got strange commit notification with message: [... 31 lines stripped ...] )
Tim The fix is good. Please close this JIRA. Thanks a lot.
Tim  The fix is good. Please close this JIRA. Thanks a lot.
Oh how this warms my heart to see this...  Thanks Daniel - I know you've worked hard on this :)
okit works finethank you.
I just committed this. Thanks Samuel !!
Thanks for the work Haohui! I've committed it to branch-1.
Great [~jhsenjaliya].
+1 for the patch pending successful Jenkins run for the latest version.  The changes look good.  I verified on both Mac and Windows.  Thanks for addressing this Andrey!  I'll commit this after Jenkins responds with +1 for the latest version.
+1. Looks god. Thanks for making it better.
Great thank you for the commit Harsh!  Mitch thank you for providing the detailed bug report.
Hi Steve gentle reminder as per your instructions about reviewing this patch. Thanks!
+1 branch-2 patch looks good to me.  I'll commit this to branch-2 later today to give others a chance to comment.
Thanks Daryn I put this into trunk and branch-2
I committed the patch branch-1 also. Thank you Andy and Slavik.
That's great.  Thanks Junping.  Then I will first commit HADOOP-8820.b1.002.patch.  For the additional bug fixes let's create a new JIRA since we also need to fix the code in trunk.
I checked this into trunk branch-2 branch-2.1.0-alpha and branch-0.23.  Thanks Daryn for a quick fix to this.
+1 the new patch looks good.
I have committed this.  Thanks Junping!
Committed to branch thanks Eli.
+1 patch looks good.  Thanks a lot!
"""Thanks Tucu! You are right. I had not thought of that.  I'm on the fence because we are serializing a process which should happen fast (so parallelizing is good). Like you already mentioned  bq. 1* Add a method in the FileSystem to disable its shutdown hook. MRAppMaster would disable it and call closeAll() explicitly (it already does). This is already present (by setting conf.setBoolean(""""fs.automatic.close"""" false) ) So I'm not sure what problem this JIRA is fixing. Or are we planning for the future? I'm fine with the latter answer being yes. I'm just curious.  The patch looks good for what it intends to do although the latest one doesn't include the fixes you made in response to Nicholas' comments."""
+1 the latest patch looks good to me.
Committed to 0.20.205.1 branch-20-security 0.23.1 and trunk.  Thanks Dave!
+1 This looks good to me. Existing configurations will still work with this patch so there are no compatibility concerns.
Committed to trunk.  Thanks Bharath!  And thanks Cos for helping review.
+1 I just committed this to branch 0.22.1. Thank you Benoy.
Hey Tom  The change in your patch looks good. Latest patch just updates two tests. I'll commit this if Hudson comes back OK.  Thanks Eli
I've committed this to trunk and branch 22.  Thanks Nigel!  Tested via ant tar and ant package instead of Hudson since most of the change was done via svn remove (the patch only applies to build.xml).
+1 This patch looks reasonable to me.  Thanks Hairong!
I've just committed this. Thanks Eli!
+1 Committed. Thanks Suresh
Can anyone please review the attached patch ? Thanks
Please review the submitted patch. Thanks.
I committed this. Thanks Jay!
I have committed this.  Thanks Aaron!
This is pretty trivial just adds three asserts to TestPath#testNormalize.
I doubt that Squid is so bad at it.
It's not too bad once CacheConfig is in.
Not too bad though since clearing out commitlog segments is easily done.
ah - my bad.
My bad had a typo in the subscription e-mail.
My bad.
Yep my bad.
Pull it back in if you think different.
Tabs are evil.
I looked at our distributed pom.xml and they're pretty obscure.
once per every bad disk).
Hell UnaryFunction might even be faster than all of these calls in a row.
This was very bad documented.
Damn.
Pull it back in if you think different.
Ie both at are bad.
durrrh that sucks.
My bad.
I'm an idiot.
Splitting an existing sub-shard gets stuck up.
This is bad.
Oopps bad move !
the recommendation in the wiki is bad.
My bad.
"I'm starting to think of ""DLL Hell."""
#name
Fix bad patch
Logger on top of Log4J and its abstract base class for supporting other frameworks.
My bad.
Well either you query the database every damn time and your plugin/tool/code will be super slow.
Damn it.
Hudson seems to be stuck on this.
My bad...
which is silly -- bad package encapsulation).
My bad.
Oops had a bad comparator in the TestCLI config.
My bad.
The food is very bad.
<gmcdonald>  my bad
My bad.
My bad.
Pull it back in if you think different.
Aaarrggh how stupid of mine  to have a System.out again!
Hell UnaryFunction might even be faster than all of these calls in a row.
I always get the same stupid error.
Damn maven!
I don't care if everything is pretty or not but we should at least support basic admin functionality in IE IMO (though I have not used it for years for just about anything).
I've looked at what can be done within the NSIS framework we use to build the Windows installer.
A feature with wrong functionality is as bad as if not worse than a bad performance.
Doh!
Doh.
Sounds weird to me... Could you package a (totally!)
Am I doing something stupid here?
* Replaced TreeMap with an Array of final 'Segment' objects ** (now very slightly faster than trunk) * Rebased for trunk * Removed the patch renaming SSTableReaderTest to SegmentedFileTest  I didn't remove the builder pattern because the alternative seems pretty ugly (copying the segments array for every append).
HADOOP-2949:  - If tarball is specified HOD no longer validates for the pkgs directory in gridservice-hdfs or mapred sections as these are not going to be used anyway.
It must be lost somewhere.
Too bad we can't use position:fixed in IE6.
I think the correct resolution is to ensure that the prefix stack mechanism  gets reset each time the XMLReader is used.
This is always a really bad way to design software.
It may suck but at least *I* won't be the one bringing in a dumb dependency to a logging library.
How on earth is it buggy?
"I didn't do that because that seems bad in hive so I returned ""null"" from the operation."
This sucks.
They don't vote they whine that something (or everything) sucks.
Damn dyslexia!
Damn it !
Doing it at the hackathon you'd have a few fellas at your shoulder to give you pointers should you get stuck.
Ah that was my bad.
My bad.
it's my bad.
My bad!
Ah too bad.
Damn Chuck is scary.
Here is quite bad.
This name gets confusing with all of our prenamed *Worker* and *Master* classes.
* release of the framework.
I'm a pretty strong -0 on this.
Not pretty.
I think that would be the cleanest solution to prevent bad behaviour with wrong codecs?
Perhaps it would be easier to make ClobLocatorInputStream a wrapper around ClobLocatorReader and ClobLocatorOutputStream a wrapper around ClobLocatorWriter?
I like having consistent indenting but now it is really hard to see the diffs and it gets confusing.
> Ideally we would move to NIO framework like netty but that would much larger effort.
Once all the chunks have been uploaded the Upload Framework can returned a combined file.
You don't see '>' replaced because that's not required in this case (though some frameworks do it anyway).
Making a blocker.
I've reverted this for the moment.
and at the moment we dont have that yet.
Maybe we're stuck with it until log4j fixes its dependencies?
[~bmahler] it looks like this can be closed since the Python UI is gone.
Is that true?
seems this is a CDK issue
Mladen any progress please?
I am transfering my issues so I want to know the current status.
Thanks for reporting.
Fixed on the master (0d43528e) and the 1.6 (9affb57a).
or someother version?
ThriftServer.java and Hbase.thrift patched manually the other files generated using thrift-0.2.0
I drafted a small benchmark (so far only for android) to demonstrate the issue.
Does it solve your issue ?
I'll see if I can arrange for the manifest to contain bundle-manifestVersion.
Pushing non-critical issues to the next CR2.
If you think they are critical for CR1 please re-schedule them.
Issue is reproducible at least on Debian 32bit and SLES 10 64bit.
"So I also can say ""this is not yield keyword specific problem""."
Thank you for reviewing.
fixed in revision 478248
pushed
Supported by external project
And in that case the first argument could also be changed from OpenBitSetDISI to OpenBitSet.
Until there will be _real_ need of adding new Monoid implementations I would suggest to postpone the problem and keep *your* version of the patch.
Let's fight with the right weapons when we need I wouldn't use a katana to kill a fly (unless I am Bruce Willis and I'm in Pulp Fiction :P)  best and thanks!
No issue found after noting how it's invoked.
And interestingly suddenly this is moved from MAPREDUCE to OOZIE.
Running patch through hudson.
A new issue (https://issues.apache.org/jira/browse/RAMPARTC-140) is created for implementing SupportingToken assertion.
v1 fixes the nit
Patch committed.
Thanks Ankit!
Thanks for the patch Kim.
I will look at those remaining references to NATIVE::LOCAL in the error messages.
I tripped across one puzzling sentence but it wasn't something that you changed: In the second longer paragraph of the BUILTIN bullet we recommend LDAP and user-supplied schemes as alternatives to BUILTIN but we don't recommend NATIVE.
Did we decide to not recommend NATIVE here or should this be adjusted to include it beside the other safe schemes?
Thanks.
It could be that this has been resolved already (elsewhere).
See attached patch.
I'll add docs in Confluence if the example is integrated in Camel's codebase.
Closing all issues created in 2005 and before which are marked resolved
APIs doc is outdated - thanks
I have committed a fix for this please close if you are satisfied.
Thanks.
What % of clients do think were stuck like this?
Mind adding your test run results in here?
QA test result is recorded above.
Closing works like charm now.
Stefan: are you sure you had a clean build with my patch applied?
Please follow up on the forum: http://www.jboss.com/index.html?module=bb&op=viewtopic&p=4178893#4178893
suggested for 3.1.1
Might want to correct the misspelling: *remvoed* AllFromInodesFromMap :)
Thank you very much Les.
This patch looks pretty good.
I think we should apply it.
Moved to GA.
This appears to be a issue with Hibernate (the default JPA implementation in JBoss AS) rather than a EJB3 issue.
Fixed the MAPREDUCE-3028 commit which broke MR1.
Contributed by Hitesh Shah.
I'll close the ticket as it looks like a non-issue.
* it is impossible to name a column 1 * it is impossible to name a column alias 1  If order by supports this I do not see group by can't?
Do we want to reconsider this?I kinda like the feature.
no POM provided here
I resolved the issue via Spring.
This is a screenshot from 1.5.3 and that is how I would expect the Directory Studio to behave.
Harsh will you have a chance to work on a test for this?
Please tell me if you'd like me to help.
Misty  Can you move JBPAPP-7480 to bz and then cross reference the bz number in this Jira and close this one?
Thanks  Mike
v3 rewritten to be lots more explicit and save the vendor arch and version numbers for later code to use.
patch by johano reviewed by jbellis for
We just need a possible mentor here... volunteers...?
EAR project case is fixed.
Alexey please verify.
Resolving this issue as it is covered by JBQA-5191 and JBQA-5192.
btw MCLEAN-18-maven-clean-plugin.patch also applied the maven-clean-plugin-dcabasson-MCLEAN17.txt fix.
> DistributedHashTable is a good solution but then how are we planning  > to acheive let say a Distributed LRU Cache or Distributed Time-limited  > or a Distributed Count-Limited Cache.
I am not mandating to use DistributedHashtable directly by plugging it into Castor as it is but as the base for 'distributable' TimeLimited CountLimited et alias cache implementations.
I will re-work and submit another patch.
Will upgrade to 1.0.10 and check it.
Will re-enable assertions too.
{{TrivariateRealFunction}} renamed in r1236932.
yeah I think so.
we just used it with CBR (content based routing).
Now the JBossESB changed to SwitchYard and we need to ensure it works.
We are restructuring the trunk now from jboss as 4.2 and it's unstable.
You should check out and build revision 693 of the trunk.
pushed
It is not in the requirement.
I move it into 2.1-M1.
Attaching patch for review.
Resolved by changing the meaning of maxRows to be across all code tables with a default of 200000 introduced maxRowPerTable with a default of 10000 to take the place of maxRows and upped the default for maxCount to 200.
I switched to using the JavaSourceLoader and this appears to work fine.
Done except for pages.xml imports.
I'm not sure if this is actually a bug we had it fixed but then had to revert it.
I don't remember why right now though.
Integrated in Cassandra #63 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/63/])
Applied Andrew's patch at r676866.
Please check that it was applied as expected.
I think nothing is absolute.
This appears to have been fixed
and your comments are very welcome..
Marcus   we need a fix for this - if you can check in we will test it  thanks /sudha
Add CRC32C as another DataChecksum implementation.
Contributed by Todd Lipcon.
Not a SWF issue.
Please open with Spring Framework.
Issue is also present at JPP 6.0 ER02
Fixed with revision 164512.
Added org.jboss.test.ejb3.war.deployment.unit.Ejb3WarDeploymentTestCase in r99656.
Also see issues like SOLR-2608 that are ignored i've been running lucene tests in a loop on and off for periods and basically gave up on solr tests aswel they randomly fail all the time
This one... Really.
Alright I'll put this and HBASE-8453 in through trunk today.
"Sorry I realise now that ""does not work"" was insufficiently specific."
Seems to be working just fine.
It looks pretty bad.
[~maxandersen] please review
Corrected error in comment - please use this one.
hbase org.apache.hadoop.hbase.PerformanceEvaluation --nomapred --rows=250000 randomRead 16  10k regions 5 RS.
I tested HBASE-9609 and it does seem to make a huge difference!
I integrated the submitted patch in the new JBW but did not really test it.
Nobody has complained about it yet but I don't think it should be ported to older versions at this time.
[~saturnism] agreed.
# Javadoc in InterDatanodeProtocolServersideTranslator refers to JournalProtocl.
Shelly use JBAS-5873 for the matrix tests :)
Sanjay your patch is missing the AuthenticationFilterInitializer.java file
The pb is not in ApacheDS but in the startup delay.
Thanks Raymond
I'll try it on our Fedora.
Hi guys  I agree 100% on this.
Just a note there was a long conversation about this here: https://reviews.apache.org/r/4655/ and this is a dup of FLUME-1089.
"Based on review comment add ""respBody.hasPublishResponse()"" before returning respBody.getPublishResponse()"
using g++ (GCC) 4.1.2 20080704 (Red Hat 4.1.2-48) and cppunit 1.12.1.
Next try r1475799 in santuario.
Here's a patch that adds in support for SGD.
Fixed in trunk -r746186.
Carl can you review?
I haven't fully grok'd the entire issue yet but have a quick comment: 2.0 was released several months ago...it's not 2.0-alpha-5-SNAPSHOT.
is it essue still open?
Bozhidar: I agree with your assessment.
On 2008-04-03 15:57:26.514 laupark commented: Opened.
On 2008-04-04 13:31:57.627 jimurphy commented: fixed rev.
196883 On 2008-04-08 08:38:58.599 wichan commented: Still couldn't find the swfUtils.jar in the webtier.war ad flex-for-cf.zip On 2008-04-09 07:51:01.799 ccreanga commented: Fixed - swfutils.jar was not included in the war
The code should work.
The API need to be checked and those errors (ie PDU has been fully decoded ...) have to be fixed.
We wil investigate what's going on.
Thanks for the report !
This patch should do the trick.
A test case is needed too
I suggest that you have a closer look at the exception.
The absolut portion of the path *is always prepended*.
So any of your suggestions do not help.
Assume closed as resolved and released
PR https://github.com/SpringSource/spring-integration/pull/621
I committed patch _c with revision 697897.
Note that as indicated before JVMInfo still is part of derbyclient.jar (I assume because of the need to support sysinfo).
Committed to trunk and 0.21 branch.
Thanks Amar!
An awesome feature of Hive is the rich type system with excellent support for complex data-structures.
Solomon That is very slick.
I tested it and it works fine.
Thank you.
Chris This might be a useful technique to add to the documentation or to a HOWTO on the SJC wiki (when / if there is one)  Thanks all.
Committed after fixing the typo.
One minor point - can you make result a member variable of GenericUDFArrayContains?
This will reduce object creation.
The patch needs to be regenerated against the current trunk but otherwise looks good.
Thanks Kristian.
Committed revision 1341814.
Deleted spurious 'Added: svn:executable' from the patch.
The .md extension is used for the readme because it's in MarkDown format used by GitHub.
See the following page for an example:  https://github.com/seam/examples/tree/master/booking
It is completely expected an updatesite.zip for all of devstudio is *bigger* than the installer.jar
I presume so.
updating minor errors in pdf doc
Yup instruction are not correct.
"""That said check out setReadLength in TBinaryProtocol."""
I have committed this.
Also thanks Matt for the review.
Yup I'll get it in today.
Fixed in r544
Yeah it looks like there are conflicts with RenderingContentRewriter and one of the guice modules.
Committed to Trunk.
Atlassian 3.2 is in Central.
Missfire
closed with no testing Ian says this is a code-level change.
When the order of methods change so the rendering of fragments is bellow the definition then it works fine.
Two git commits included.
Abandoned no longer necessary.
I'm giving this up hoping someone confident in Swing would look into it.
Have you been able to work on that ?
Do you need any help?
Hi Matt  Thank you for fixing this issue.
I will use it when it will be included in a future release.
Committed the addendum in revision 1414868.
Thanks for pointing out this (rather embarrassing) oversight!
Fixed for both 2.0 M4 and 1.2.8.
Amila Could you please check my fix?
Hi Drew  I have upgraded Postage to James3 (see POSTAGE-23) and used your patch at this occasion.
Looks like we have already taken care of this.
are any changes needed to dd2spring for this?
I suggest making a tooltip with the title of the context help article (a popup)  when the user hovers the question mark icon.
We'll go with this for now thanks for the fix!
Ran some benchmarks.
When the modulus is large which means that number of collisions is small LightWeightGSet is much better than GSetByHashMap.
added fixVersion
Rolled over to 0.5.5 to get a stable JavaOne release out
Can you expand on the difference between a Prepared Statement and a Logical Prepared Statement?
And also on the difference between a Connection and a Logical Connection?
Are these already-existing concepts in the client?
Or did you introduce these concepts?
Thanks!
how does your application.cfc looks like (ormsettings)?
I think Jason has done this already?
canceling
We'll take a look for 1.21.
Thanks Matt Todd and Cos!
From https://builds.apache.org/job/PreCommit-HBASE-Build/2581/console it is not clear which test hung.
Committed the patch with thanks to Christian.
Sangwon Seo Thanks for your review.
Could you give me more comments?
fixed.
This obsoletes ~/.m2/maven.properties
The last bug I see mentioning a HSQL upgrade was JBAS-1618 which ended up with 1.8 RC9.
However this bug mentions that we were upgrading from 1.8.0.2 to 1.8.0.7.
When did 1.8 RC 9 get upgraded and where is it tracked/documented?
Done in Weld and TCK
Applied patch.
Thanks.
And many thanks to you Olivier for applying this so quickly!
quick patch.
What's the operation?
I just committed this to trunk and branch-0.23.
Thanks Sid!
Thanks for testing but please don't close it yet.
Fixed trunk r910567
For _ExternalSpecifications there is already an issue in the JIRA: MYFACES-2582
Thank you.
Issue has been resolved.
A new patch including some corrected Japanese sentences and a link to Solr Wiki.
All log statements should now be surrounded by log guards.
Horia Chiorean <hchiorea@redhat.com> made a comment on [bug 1014709|https://bugzilla.redhat.com/show_bug.cgi?id=1014709]  https://github.com/jboss-integration/modeshape/commit/d1d97180c957491bee3654743ebd4a6860314ad3
Thanks!
Used the new attribute to move ObjectStoreDir from bin to server/xxx/data/tx-object-store
Please also see Glassfish issue 13111: https://glassfish.dev.java.net/issues/show_bug.cgi?id=13111
Committed as r1382016.
Thanks Sijie.
Joram notify Jennifer Vennable when you completed this.
Never mind found it.
Changes applied in revision 1369338.
Many thanks.
Monitor no longer based on protobuf.
periodic bulk close of resolved issues.
Thank you!
It's working like a charm.
The b-1.0 patch also applies to branch-1.
I too have been busy on other projects.
I'll try and test with queryIndexesRebuiltSynchronously false today.
fancy applying this patch before 1.3.0?
applied patch (thanks) in r1055887
Hi Jaikiran   Didn't you already fix this one?
Thanks!
Dona Thiru: can one of you (or anyone else) please review this?
05 was committed in r613384
committed to TRUNK (m3)
applied.
Thanks!
Closing this issue Jacopo fixed it at r1345528.
Thanks Jacopo.
Add EnumSetWritable.
Contributed by He Yongqiang
I'm closing this for the time being as we've reopened SPR-368.
Applied Rodrigo's suggestions
This was found in via JBossWS CXF so once we've pulled it in to JBoss and tested I'll let you know if there's any issues.
Thanks for the quick response Colm.
Hullo?
Could you open a ticket for this (last comment) Tnx micha
Hi Thomas it's here:  https://svn.jboss.org/repos/common/common-old/branches/Branch_1_0/
Indeed!
This is actually the case in 2.0.5 already.
Thanks for spotting this Philipp!
It seems that using {{BufferedOutputStream}} is a better solution since it will also help other operations.
BTW do you know why there are four one-byte packets?
fixed style
Dupe of CASSANDRA-4042 (we'll include the BF fp change there)
I think this looks like the same issue as  [MFINDBUGS-55|http://jira.codehaus.org/browse/MFINDBUGS-55]  Garvin
Changed contributed by from Arpit Agarwal to Arpit Gupta.
(Revision 1450575) HADOOP-8917.
add LOCALE.US to toLowerCase in SecurityUtil.replacePattern.
Contributed by Arpit Agarwal.
Marc you're re-logging your ownn issues :)
Patch now applies cleanly against latest version of trunk.
Bounce to Doug as this is a SOA issue.
Thanks I just committed the patch.
However I cannot see errors in lucli can you make a patch for those cases too?
Look forward to seeing it resolved in 3.3.2.GA!
"On trunk apparently all use of the ""get"" ""put"" and ""delete"" histograms have already been removed from the regionserver except for the Jamon templates."
Since an approach to put back two such HRegionInterface level measurements with a query/mutate distinction has been +1ed on this issue I'm going to put them back on trunk and port back the changes to 0.94.
Calling them queryRequestLatency and mutateRequestLatency in RegionServerMetrics.
This blog post speaks briefly about colocation and how this can be achieved using AtomicMaps.
Thanks Daryn!
for taking a look.
I just committed this.
Thanks Sanjay!
After several retries seems we are unable to proceed with releng team to get supported solution.
Will setup mirroring to Brno QA lab.
Thanks Claus.
If somebody can better formulate my above fragment and place it somewhere in wiki it would be great.
I have throuroughly documented this in the XWorkConverterTest see testStringToInt testStringToInteger testStringToPrimitiveDouble and testStringToDouble.
If you feel like adding this to the showcase than go on.
cheers  ./alex -- .w( the_mindstorm )p.
I think the HttpClients service interface must be exported for it to be used.
On the other hand the BundleActivator should not be exported and can as well be moved to the osgi.impl package.
Done at revision 1582 - 1585
Closing as fixed in trunk.
Reopen if I'm mistaken.
We have an HTTP discovery agent in activemq-http no other activity on this issue so closing for now.
Although the older patch applied fine for me (patch -p1).
jBPM already works this way.
Thanks Amar!
Would your use case be covered by adding a TTL to CF?
Because that seems like the natural level of granularity for this to me.
Looks like Forge isn't used at companies having a firewall.
Attached patch for Myfaces.
the  repairment to BlueSky official website Downloads.html and MailingLists.html  thebluesky group an use the patch freely
adding --rsyncable to our build system does not seem to be a big task I am experimenting with it.
seen from my chair this is a very elegant solution until we can really split our distribution into smaller files (exe + langauge packs) and have a meta installer that combines what the user want.
Regenerated patch with trunk and with an indentation change.
Spoke too soon!
The SocetException is still there sporadically.
This adds testing of the RegionSplitsTracker.
I committed the patch to trunk.
Thank you Ivan!
Thanks Daryn for the review.
If possible let's schedule to 0.4
Whoops yeah sorry.
Agreed wildcards (* ?)
are enough for this rather than full blown regexp.
It's also easier then to tell a wildcard-based config from a normal one.
Feature verification is completed including Upgrade.
hence closing the ticket.
This issue is being tracked by other JIRAs now.
RB at https://reviews.apache.org/r/15607/
I just committed this.
Thanks Scott!
Sonar does not use JavaNCSS at all !
The 0.23 patch looks fine.
I also tried to resolve merge conflicts and ended up with pretty much the same patch as yours.
The only significant code difference seems to be in getBlockLocations().
Thanks for the patch.
It was applied with minor changes in 2.1.10-dev and 2.2-dev.
Please reopen the issue if needed.
fixed in 1.8. checking in javadoc fixes momentarily
Committed in r14473 on trunk and r14474 on 2.0.x.
I know this hasn't been looked at in quite a while.
I can't see the current GOP and GKOP being completely replaced but there may well be some areas where your code could be incorporated.
It would help considerably if you could explain what the problem was/is with finite sized pools and the current implementation.
Without an explanation of what the issue is this issue is likely to remain dormant or just be closed as won't fix.
"Same in problem in SQLServer solved with catalog=null and schema=""%"""
Done as part of Felix resolver integration
Latest commits now include filemgr and workflowmgr integration.
The work in progress archetype is now committed in the trunk (http://svn.apache.org/repos/asf/oodt/trunk/mvn/archetypes/radix/).
Bharath  Since all of this work's goal was done - can this be closed?
Thanks for Paul King's advice I tried 1.6.3 again with groovy.bat.
groovy.bat don't have this problem so the problem is from the native launcher.
Closing as dupe.
Marek Novotny <mnovotny@redhat.com> made a comment on [bug 883918|https://bugzilla.redhat.com/show_bug.cgi?id=883918]  This will be fixed/upgraded to GWT 2.5 in WFK 2.3.
Per Do??acan's comment we need to reopen this and test out his new patch for it.
Andrzej I'd be happy if you reassigned to you however I will have some time on Tuesday to look at this if you don't until then.
latest from RB
commit fix http://svn.apache.org/viewvc?rev=729857&view=rev to nmr project http://svn.apache.org/viewvc?rev=729894&view=rev to features project
Robbie  Could you please review and commit the patch?
Hmm it did work on monday for me strange I'll have a look at it right now
reviewed and committed patch.
Thanks Charitha.
Added to portletswap but not the distro until we have themed groupings.
Thanks for reporting.
Yeah classloading in JBoss is a bit tricky :)
Applied slightly modified patch in [r18180|http://fisheye.codehaus.org/changelog/mojo/?cs=18180].
Thanks!
[~ashutoshc] Thanks.
The rpc changes look good to me in the patch (smile)
I will rewrite that part.
Scratch that this fails horribly after the change to unparsing negative numbers as hex.
Fixed in next savara release.
Some more work towards full SAAJ 1.3 implementation.
Patch for 0.96 corresponding to latest trunk patch.
Thanks Amit  Your patch is in trunk at r832776 R9.04 r832779
I have committed this.  Thanks Rodrigo!
+1 for this patch. Looks good.
I just committed this. Thanks Amareshwari!
Patch looks good. +1
I just committed this. Thanks Amareshwari!
I committed this to 0.20 and above.  Thanks Corinne!
I just committed this thanks Jothi!
We ran gridmix just to be sure that the performance is not hit huge because of the locking business. And the results turned out to be in the same range of 2700-2800 secs of our previous runs of FairScheduler without this patch.  I also had a look at the patch. Looks good. +1.
I committed this. Thanks Nicholas
I just committed this. Thanks Amareshwari!
I just committed this.  Thanks JordÃ !
I just committed this to trunk and branch 0.20 as they are test cases and no change to functionality. Thanks Vinod !
+1 This looks good to me.
Patch looks good to me.
I just committed this. Thanks Alejandro!
+1 This looks good.
I just committed this. Thank you Lohit.
+1 The patch looks good.
+1 Looks good.
I just committed this.  Thanks Giri!
Release audit warning are ok. The new files don't need Apache headers.   The patch is good to go
I just committed this. Thanks Tom!
I committed the patch to trunk. Could you please submit a patch for the 0.16 branch. This one doesn't apply cleanly. Thanks!
Oh thank you lohit. Then should we close this issue?
I just committed this. Thanks Nicholas!
I have committed this.  Thanks Michele!
Thanks for the suggestion Raghu.  Let me try it.
+1. Code looks good.
+1 code review
I just committed this.  Thanks Prachi!
+1. The patch looks good.
I just committed this. Thanks Mahadev!
I just committed this. Thanks Arun!
I've just committed this.  Thanks Arun!
I just committed this.  Thanks Raghu!
Attached patch for review...   Christian could you see if this works for you? Thanks!
It looks good to me too.
I've just committed this. Thanks Espen Andrzej and Owen!
Yes I just tried it and this change works when the user's classpath variable is unset.   I've just committed this. Thanks Michael!
I just committed this.  Thanks Hairong!
I just committed this.  Thanks Sanjay!
I just committed this.  Thanks Sanjay!
I just committed this.  Thanks Sameer!
I just committed this.  Thanks Milind!
[~apresta] thanks for the fast review.
Awesome works for me. LG committing. Thanks!
+1!  Thanks!  I also ran with mvn clean install and it looks good.
Committed thanks for the review Avery!
Hi Jurgen yes that's probably a good idea but not something that can be easily and/or quickly implemented. I'd like to change this JIRA to from a Bug to an Improvement so that it can be looked at in a future release. Thanks for the suggestion.....
Merged the batch file updates to branches/2.1 (revision 743129). Thanks again!
Hi Ivan     As Donald has reassigned this JIRA to you please help to review and commit these patches after I submit the patch for TranQL localization. Thanks. BTW the first patch in the list had already been committed.
A patch for this. Thanks a lot.
This works fine.  It probably could have been done in another Valve (like the policy conetxt valve) but this is a reasonable solution as well.  Thanks.
any chance to get a patch also for this? :)
Thanks for applying the images.   Please check and apply the patch I've submitted to FOR-1008 [1]. Since the changes required to resolve this issue involves files modified there it has to be applied first.  [1] https://issues.apache.org/jira/secure/attachment/12363540/valid_xdoc.patch
thanks luis!
Patch applied to FOP trunk see r793095.  Thanks for the fix!
"""Hi Vincent  Thanks very much for taking the time to look closely at my patch.  (In reply to comment #26) > (In reply to comment #24) > > (In reply to comment #23) > > > Adrian thanks for doing this! I've looked at the patch and have a few comments > > > myself: > > > - I'd suggest to rename """"strict-configuration"""" to """"validate-configuration"""" (just > > > a personal preference). > >  > > The configuration will still be validated regardless of the setting of this > > variable.  Its just how the error is handled that makes the difference.  If > > """"strict-configuration"""" is true then FOP will immediately throw an exception and > > processing will terminate.  If """"strict-configuration"""" is set to false then FOP > > will log the error and attempt to continue parsing the configuration (if > > possible/meaningful). > >  > > > - the name of the variable """"strictFO"""" in FopFactory.configure(Configuration) > > > seems wrong. There's nothing """"FO"""" specific there. Furthermore some """"if > > > (strictFO)"""" should actually be """"if (strictConfig)"""" right? > >  > > I think you may have been looking at an older patch.  The variable > > """"strictValidation"""" is as before and the new variable is called > > """"strictUserConfigValidation"""". >  > No Jeremias is right actually. There are 'if (strictFO)' statements to test if   > an exception has to be thrown. That should be 'if  > (validateUserConfigStrictly())'. I should have better looked.  Darn it..  I didn't think Jeremias was refering to the configure method.  I missed this thanks.  > Also in the new code a HyphenationTreeResolver is no longer created and  > assigned to the hyphResolver field. That should inevitably lead to  > a NullPointerException later however the hyphenation junit tests pass. Strange.  > Are you sure of what you're doing?  Thanks for checking this I made a mistake refactoring here :-(.  > One other thing: when getting the default-page-settings parameter it's not  > necessary to test if pageConfig is null"""
Patch applied. Thanks a lot especially for the test case. http://svn.apache.org/viewcvs?rev=240459&view=rev http://svn.apache.org/viewcvs?rev=240461&view=rev  The result for 180 and -180 is definitely wrong. There's a bug that needs to be fixed eventually. But since this is low-priority for me I'm going to leave it for now. I've added a comment to the new test case block-container4a that explains what is wrong. So I think I'll close this issue. It's primarily for the patch after all and the bug won't be missed now that there is a check in our test suite. If anyone disagrees just reopen the issue or create a new one.
Applied.  Thanks Simon!
Committed rev:088067c. Thanks Roman!
I think you should get rid of the open-ended version spec [1.2) as it occasionally screws things up by bringing in snapshots or non-working code. Otherwise to my eyes it looks good.
Patch committed rev: 84f4fa61b6607d8699bb28743b568f4a716b4288. Thanks Mike!
Patch committed to trunk Thanks Hari!
Thank you Carol!
I took a quick look at D (unused locals).  Most of it looks good but I don't understand the few places where for backward compatibility there is what looks like a useless statement just to make sure a property gets read.  Won't someone else someday follow up with patch H that removes what seems to be useless code and just take those out anyway?
Re font size: sounds reasonable. Thanks.
Thanks for providing this update. WIll try to take a look at this sometime this week.
Agreed the current behavior is incorrect I just didn't want you disappointed. :-)  If you create your singleton jira patch before this one is applied then just include them together and close this bug.  I agree that commit rights would make this simpler and supplying patches is the primary way to gain commit rights so you are on the right path regardless what happens with Pax Web. :-)
Applied thanks.
Thanks Alex.  It looks like a good path to solve this issue.
Yes we need to fix it. :)
Ok thanks !  Closing the issue then.
Thank you for the source code but why don't you also attach a diff file you get by running 'svn diff'?  It will help us figure out which part is changed exactly.  Thanks in advance!  Your idea sounds very good though. :D
Thanks! Looks like a good clean-up to me. I admit I didn't check all the changed lines in detail but the approach gets my +1 :)
Thanks Rick!
Thanks for the new patch Kim. +1
Knut and Rick thanks for your feedbacks. I will take care of the comments posted by Knut. Rick thanks for adding -lint directive it will be good to see the error from the compiler.
Hi Tiago  Thanks for looking at this issue.  On visual inspection of the change your text change looks fine. However I can't apply the patch. You need to svn diff from the top of the tree show it shows the path staring with java/engine/...  You should reopen the issue until it has been committed.  Also just for good measure you should run suites.All and derbyall  in case there are any canon updates needed.  Thanks  Kathey
Changes look good. Could you run regression tests with your patch and let us know how it goes?  Thanks  Kathey
Thanks for the new patch Kristian! The changes look good to me. +1 to commit.  A tiny nit in StatementPoolingTest
Junit patch committed Revision: 634284 - Thanks John.
Thanks for double-checking the fix Knut. I am thinking of merging the fix into 10.3 tomorrow just to make sure that tests run fine on the tinderbox. All the tests ran fine on my machine.
Hi Manjula I think your approach basically looks good. Please see my comments below.  1) I think this code  +        NetworkServerControl serverControl = null
All the regression tests ran cleanly with the patch. Committed revision 1126468.  Thanks for fix Ed!
This looks like a fine approach to me. Thanks for putting the patch together. +1.
Release note looks good.
Patch 3 looks good to me too. Thanks for all the work.
Thanks Bryan for converting these tests.  In using the DERBY-2152 conversion tool can you double check that it does not omit  the fail assert method after a method call in a try-catch block when the statement is expected to fail.  e.g. instead of   try {   s.execute(command)
I was able to download and extract http://db.apache.org/derby/docs/docs-10.0.tar.gz just fine. Thanks for working on this Myrna.
Hi Ramin  The patch looks good except I think it would be good to give import/export its own  SQLState starting with XIE instead of reusing the existing store FILE_EXISTS message.  Also it would be good to have test cases for writing the lob files.  Kathey
Knut Anders thanks for the review and the good advice.  The attached patch rmTestPreparedStatementMethods_v2.diff should address your comments.
These look just fine -- thanks for your hard work on all these fixes Dag! If they are technically correct I say +1 to commit.
Thanks for the revision JÃ¸rgen! I think the release notes look good now. I will resolve the issue after a few days to allow for any feedback on Knut's suggestion.
Thanks so much Knut Anders!
Hi Anurag  Thanks for the preliminary peek at this patch. Looks good. I like the way you handled the optional inclusion of the JDBC4-specific classes in the jar file. I recommend adding class and method comments to the test classes so that reviewers will know what these tests are supposed to stress. Thanks for the explanation of why you disabled the SecurityManager and for commenting the property file accordingly. When we do commit this patch let's leave the JIRA open to remind us to re-enable the SecurityManager after mustang fixes its bug.  Cheers-Rick
The patch looks good and the tests ran cleanly. I reindented some parts of the code since they had twice as much indentation as the surrounding code (does your editor use tab width eight instead of four?).  Committed revision 396859.
Hi Anders   I agree with Knut  this issue can be marked as fixed   Thanks a lot for  working on this improvement.   -suresh
Thanks for taking the time out to review the patch Bryan. Yes the patch is ready for commit unless someone else is reviewing it too. Thanks again.
Thanks Kathey and Myrna. The release note looks good and it passes the ReleaseNoteReader's checks.
Great thanks!
Committed. Thanks for the patch :)
Sounds perfect thank you.
Hi Aki  I strongly suggest you write a persistence system test both for client and server side which can expose the problem you mentioned and also verify your fix works.  Actually we already have ServerPersistenceTest and ClientPersistenceTest in systests/ws-specs I think you can add yours in those two classes.  Btw your suggestion seems ok for me.  Thanks Freeman
Applied patch into trunk and cxf-2.2.x branch  with thanks to William.
Hi Charles thanks for the confirmation... We will chat with Willem I'm off today but will work for a couple of days this week... Sergey
I've now ported the fix to 2.0.4 and deployed  a new snapshot.   Could someone give it a try to make sure it really is fixed so that we can close this off for 2.0.4?  Thanks!
Patch applied.  Thanks!
I'm excited to see progress being made here but I have to agree with Joan the security of this concerns me even with the new sandbox.js you're putting together.
Applied to master and branch 1.2.x. Thanks Klaus.
The patch & tests look good to me.
Applied to trunk. Thanks Andrey
That looks great thanks! I replicated the database but can't figure out how to start the CouchApp can you help? Cheers Chris.
That's pretty unlikely.
Pretty much everything that FSNamesystem (ie.
Kind of sucks.
Since this patch is so damn large already I won't be making anymore changes to this other than critical/blocker bug fixes.
But please don't say that my reasoning is bad - because it is not.
> We can't let fear of back-compat prevent us from making progress.
I really don't care.
I am planning to use this framework for some new RPCs I am adding.
Don't hate me just throwing this out there.
Weird.
This is weird.
Gav...(Hoping I'm not teaching you how to suck eggs :) )
For those of us who don't care about ivy in fact lose something - a easy to build hbase.
Where on earth are you anyway?
My bad.
My bad this is already done.
That is bad.
Filling diags.log is bad mojo since it's never rotated.
How bad is it?
My bad.
My bad.
Bad IE.
Bad.
Pull it back in if you think different.
Stupid user error :)
Weird.
That's weird.
Actually I don't want to specify the encoding because I don't care how the data is transported to me.
And boy hell broke loose )  So... the biggest issue I'm facing is indeed with Random sharing across threads.
I don't care what we do as long as we don't change the attributes in JSP significantly.
The Async API in http client 4.x really sucks.
Forget the patch for the moment.
I'll just put fix in under this issue.
Please forget last comment it is incorrect.
umm ... call me crazy but why are we making this public?
[~cmccabe]: oh hell no!
Damn...
I'd say to hell with windows minority :)
Because we don't care for this in cases where we there is no node that is down.
The eclipse ui was completly stuck which was similiar to the other experinces.
My bad.
Juhani..my bad i was mistaken.
My Bad.
That's my bad.
Seeking back is pretty inefficient in DFS.
Doh!
Hudson is stuck.
Weird.
Weird.
That's weird.
If that would be the case this would be bad design.
This just seems really evil.
It may cause conflicts but if your intent is to break system security you probably don't care.
My bad uploaded the wrong patch.
I just hate having no clue what a file is really depending on )
Ah damn I thought it was fixed :/  Guillaume ?
I abbreviated the heck out of my arguments and thinking but damn it thats what I think :)
A stupid bug in a patch that is already applied.
In this case data would pretty much just silently be lost.
This whole thing is really pretty trivial.
An output connector should also have a say in what URLs it will accept.
"Turning off hints is basically intended as a ""oh shit something is broken with hints let's turn it off"" switch."
why on earth would it match anything you have not even touched the Apache install.
I've been running with -Xmx1024M -Xms1024M before and that worked in the past...Not sure what's going on now.
I too am concerned about the  DRDA 255 byte character string limit.
I almost forgot as I received the email notification on the weekend.
25 hours gone.
I'm new to Spring Integration so forgive me for the stupid things I might say.
My  apologies for the confusion.
Sorry...
Sorry again.
Crossing fingers...
Forgot ASF grant.
It was my mistake.
I'm sorry for making it confusing.
Honestly there is no way around this issue - of course we must discuss and come to some solution regarding Lucene/Solr at this point.
In the past I have enjoyed working with everyone here.
My misunderstanding.
Which API at the thrift client you are referring?
We decided not to support jdk13 jvms with 10.3 so there's no point in attempting to change anything at this point.
OK thanks for the review guys!
I'll commit soon.
Patch committed.
Thanks Hari!
Ping?
Did you manage to create a test program?
It looks like there is an unencodable character.
JMX support will be implemented with the remoting project.
As of the new log4j DSL this is not an issue
Tested by submitter.
I just committed this.
Thanks Amar - this was a long-drawn affair!
Fixed for exclusion: http://svn.apache.org/viewvc?view=revision&revision=1539249
Hi  Thanks for your help testing this stuff.
I did a review and I found the problem is not on tomahawk.
Really it is a side effect of the browser (IE 7 and IE 8).
In few words onchange event is not published until the checkbox lost focus.
I'll close this issue as won't fix but anyway I commit the fix on tomahawk examples.
89 and 90 look fine.
Yes this was fixed through Yecht.
Should be fixed with DOXIA-390 I added your test case.
Thanks!
all history lost?
for example: https://github.com/hibernate/hibernate-core/commits/3.6/hibernate-testsuite/pom.xml https://github.com/hibernate/hibernate-core/commits/3.6/testsuite/pom.xml
Review board: https://reviews.apache.org/r/15335/
I just committed this.
Thanks Amar!
patch reviewed and applied by Eoghan with r502283
Extra javadoc added
I wonder if there are any advantages to your way of doing it over mine but I think yours will definitely work to detect the error cases I had in mine.
Fair enough.
Hello Bhumir  I think Ivan is right - can you try to test your case with the complete version of log4php not only the files I have mentioned.
Thanks for you patience - this is a nasty issue.
Cheers Christian
committed r1296706.
both patches applied looks good.
This never got into Indigo - pusihing for it to go into Juno.
I'm not convinced making GrailsApplication serializable is a good idea.
"You ""workaround"" of making it transient is the correct solution."
It applies thanks.
TestMultiColumnScanner has been committed to trunk.
it is fixed by r1358 in jbossweb  2.1.x.
Unsetting Fix Version on unassigned issues.
Excellent the performance is much better.
Committed.
Bulk close for 3.1
[~lfryc] [~bleathem] [~ppitonak] Do you think guys that those page fragments methods which depend on JSON parsing - aka various {{setupFromWidget}} should throw {{UnsupportedOperationException}} till this issue is resolved ?
IMHO not as we can quite quickly implement this right ?
Applied patch with thanks to Oliver.
Bulk close for Solr 1.4
Hi Oleg  Would you be able to review this one?
thanks asankha
Yeah this was a word which somebody had looked up in Dacco.
I saw 'acusament de recepci?Â?' in the GDLC but I wasn't certain what it was in English.
Perhaps 'acknowledgement slip' would be the most commonly used term in English?
Applied the same fix as in issue 25001  http://cvs.apache.org/viewcvs.cgi/jakarta-commons/dbcp/src/java/org/apache/commons/dbcp/datasources/PerUserPoolDataSource.java?r1=1.10&r2=1.11&diff_format=h
Postponed to 2.0.0-RC1
fixed by Shane.
Steve can you clarify?
I'm not seeing how including a dependency would alter the fundamental behavior of createLink() and createLinkTo() requiring a request object.
Here are my thoughts about how it can be approached.
Fine with me.
On the recommendation of Weinan
Resolved in commit 5260 (patch by Vladimir Sizikov)
Patch submitted.
Please review.
Thank you.
I filed bug # 42715 with ASF Bugzilla about the ant.version issue.
Seems fine to me.
Thanks for looking after this.
I will be working to integrate this when I am back in CA.
Thanks Andy.
Added as a subtask [HBASE-6157] to this parent jira.
Hope that should be fine.
I added the PDType1CFont patch in revision 1425301
Clocked asyncs are fixed in r19148.
Since we will have another RC we decided to include this change in 1.5.2.
Attaching patch which incorporates Hemanth's comments.
Documentation is still a bit lacking
Don't absue the ticketing system
I committed {{BasicFunctionalityTest.testHTMLStrip()}} to branch_3x - it succeeds for me with no changes required.
The per invocation timeout facility has been added for the rmi and sslrmi transports.
Unit test: org.jboss.test.remoting.transport.rmi.timeout.RMIPerInvocationTimeoutTestCase.
Thanks for the answer Mohammad wrk.
Indeed this saved some time.
Looks like this was fixed in HDFS-3966 (For branch-1 TestFileCreation should use JUnit4 to make assumeTrue work).
I'm envisioning something like `hdfs dfsadmin -fetchimage <local path>' which would use the existing GetImageServlet to download the fsimage over HTTP saving it at <local path>.
I'm kinda dissapointed.
I'm using jruby.bat from jruby 0.9 and it works fine.
applied Marnix's patch to trunk.
url: https://192.168.124.100/sdk/vimService username: Administrator password: C******
"Note this used to work fine in 2.7 looking at ""git blame"" output it would seem the regression happened as Niels added support for 3D bboxes"
patch looks good.
+1 thanks Ivan.
Koji Noguchi I think this was fixed.
I don't see the issue on trunk.
Just realize that.
Thanks!
Can you show me which jira fixed this?
I should have tested with trunk before creating this jira.
I think I even tried with pig-0.11 to confirm the problem.
I'm sorry for the confusion.
Thanks Ittay.
Can you provide a little more context on this please...   1.
Under what runtime conditions are you seeing this.?
2
Would you have a test that we can use to reproduce?
Verified.
Thanks very much!
The -branch-18 patch does not apply to Hadoop 0.18.3.
"The only definitions of ""OutputStream baseStream"" are in run() copyBlock() and readBlock()."
Raghu am I correct here?
Resolving the issue as the patch was applied to Airavata trunk.
I see FilteringTokenFilter has a similar first token pos incr fixup so this problem is handled in at least one other filter.
I'll take a broader look tomorrow.
Jacques if you create separate issues for the erros (icluding the logs) I can take a look them.
Thanks for your vote.
Any other opinions about the patch?
+1 for the patch.
Committed to trunk.
Thanks Arpit!
I think your logic is right.
This is what I understand.
1
FileSystemView is the user file system view so it can have canDelete() and canRead() methods ..... this is fine.
2
FileSystemView can have user object to indicate whose file system view it is representing.
3
In that case caching will be at the FileSystemView level (not at FileObject level).
Logically this is also fine.
NativeFileSystemView represents the underlying OS file system view - no more no less.
Caching is a specialized function.
So if necessary we can add another FileSystemManager implementation which will support caching.
But it may not be appropriate to add this in NativeFileSystem classes.
So the action items: 1. canDelete() canRead() and canWrite() will be moved to FileSystemView from FileObject.
Please review and close out if appropriate.
This issue has been fixed by the Harmony-5834.
The testcase is helpful for ArrayList.
Thanks very much for the contribution.
The fixing is available in all builds after r658561.
solved in subversion repository as of r818942.
patch applied with slight changes thanks for the report and thanks for the patch
Thanks for the patch Gustavo.
I applied to 0.95 and trunk.
I thought there was already a (Hibernate specific?)
"Some property which points to a ""DDL file""."
It might have meant to create a DB schema but perhaps you can just use it to do whatever migration activity that you are looking for?
Please apply the attached patch by doing the following:  1.
Add the new file modules\java2wsdl\src\test\java\org\apache\tuscany\tools\java2wsdl\generate\ExampleService.java 2.
Apply the changes in patch1658 3.
Unzip ws-void-args-return under the itest directory 4.
Update the pom for itest to include ws-void-args-return  This patch fixes TUSCANY-1658 and TUSCANY-1653 and add an itest to verify the fix.
It also includes a corresponding change in java2wsdl to handle the two-way void return case correctly.
Applied patch.
Thanks!
+1 looks good.
One final note the error handling should have its own JIRA so that it is addressed one day?
Oh and that wiki page for the shell is slightly formatted wrong by the looks it changes to cursive/italics and stays until the end.
And the .irbrc sample output has no line breaks and makes it difficult to read it.
Fix race conditions in Balancer.
Contributed by Junping Du (szetszwo: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1532932) * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java
Applied the patch to implement Workspace.move
We pretty much already had the interface we wanted for this (IFixedLocationResourceStream).
I only needed to let MarkupResourceStream also implement it.
And then MarkupCache uses that interface to checkout what the actual file is  I dont think we will get IResourceStream implementation that are not File or URL and if we do then we fallback on the cachekey only approache.
It seems to work fine for me when i test locally the markup is only created once per fixed location and then access through  cachekey -> locationstring -> markup  So if locationString doesn't work we get this:  cachekey -> cachekey -> markup  But i guess that is a very small percentage if there is even 1 on this planet...  should we include this into 1.3.2?
Thanks Tatyana - the third patch was applied to BTI branch 2.0 at r552655.
Please check that the patch was applied as you expected and close the issue if everything is OK for you.
Sebastian as Sonar calls the cobertura maven plugin there is not much we can do.
Until the patch gets applied you might want to use Emma as coverage engine with the Sonar Emma Plugin (or Clover but it is free only for a month)
Hi Matt  Encrypting the serialized component tree will not solve the problem of stale view state.
The Sun JSF-RI team have implemented a fix for this issue.
It will be available in 1.2_05.
See the following link for more info:  https://javaserverfaces.dev.java.net/issues/show_bug.cgi?id=612  Thanks Ian
the configuration in the POM works fine with 2.4.3.
Patch applied on 2.1.x 2.2.x and trunk
Just an update:  # SOLR-1131: aka poly fields is almost ready to go.
Please review.
# SOLR-1297: sort by function query just needs review and then can be committed.
After that we can add in the Cartesian Tier indexing and the Cartesian Tier QParserPlugin (after a little re-write).
Then we need pseudo-fields and we likely  want to hook in a per request function cache (maybe)
It will need another round or two of adjusting but it is definitely looking solid.
I have applied the patch and am running regression tests now.
I will post my results when the tests finish.
I have some comments on the new patch:  PermDescriptor  o You should remove the permUUID field from this class since it shadows the oid field in the superclass.
In my experience this kind of shadowing always causes bugs in the future because it is so easy to lose track of which copy of the redundant field is being used.
"So the constructor for this class should call setUUID( permUUID ) rather than stuffing the shadowing field via ""this.permUUID = permUUID""."
o Once you do that you can get rid of the getUUID() setUUID() and getObjectID() methods in this class.
o I have reservations about the second constructor for this class (the one which is used to create a key descriptor rather than a full-fledged permissions descriptor).
However I see that you are following the data dictionary patterns for the other permissions descriptors.
I think you are right to follow the existing patterns.
But we should put a sticky note on our brains to remind ourselves to clean up this pattern in the future--partially initialized data is brittle.
This is something we can revisit when we collapse all of the permissions catalogs into the master catalog which you are creating.
Don't worry that's another project and not part of the sequence generator work.
SYSSEQUENCESRowFactory  o I think that you should flip the order of the keys in the second index.
That is the key order should be ( schemaID sequenceName ) rather than ( sequenceName schemaID ).
This will speed up the check in DataDictionaryImpl.isSchemaEmpty().
But maybe you have a use for an index with key order ( sequenceName schemaID ) which I'm not seeing.
If so then I would recommend adding a third index.
Note that if you swap the key order then you will need to make a corresponding change to DataDictionaryImpl.getSequenceDescriptor(String SchemaDescriptor)   o I think that buildColumnList() should return an array of 10 column descriptors one for each column in the table.
See for instance the corresponding method in SYSALIASESRowFactory.
SYSROLESRowFactory  o As with SYSSEQUENCESRowFactory I think that buildColumnList() should return a larger array one cell for each column in the table.
DataDictionary  o The header comment for getPermDescriptor() does not seem to describe the method.
EmptyDictionary  o I think that you can remove the stub for dropSequenceDescriptor(String TransactionController) because you have removed this method from the DataDictionary interface.
DataDictionaryImpl  o The header comments on getUncachedPermDescriptor() getPermDescriptor() and dropAllPermDescriptors() look like they were copied from pre-existing methods so they should be revised to describe the new methods.
Thanks -Rick
See previous comments for earlier commit.
This one fixes the end-date issue in the comment bean classes: URL: http://svn.apache.org/viewvc?rev=782269&view=rev  And that resolves this issue in the trunk.
Thanks Greg!
Hey Chrisyou just moved the files right?!
Vinicus patch only includes a single file (the failing junit) all Uwe need to do is to apply the patch to the moved file is that correct?
Also applies to the 4.x series
The behaviour still occurs in the last snapshot using both Mojarra 2.1 or MyFaces.
Attribute 'width' works fine.
Would be nice to have a batch/list functionality as well like we have for deletes/puts.
Are there any (future?)
plans to have a multi- version of the atomicMutation?
It looks like the problem originates in PDFBOX-11.
Many CID mappings are expressed as ranges which the code explodes into individual Integer->String pairs.
This causes pretty big data structures as many of our CMaps cover 65k possible mappings!
We should use a more efficient in-memory representation for such range mappings.
In the same pc I have a outlook client and It work fine the problem is with xplanner how can I  configurate xplanner for send mails ?
Thanks
ok I'm looking into this.
Okay reviewed and tested patch with sqlite3 and bdb on trunk.
One minor correction to configure without sqlite3.
$Name
Have a look at hibernate.common.xml
Hi David  This helps me understand what you were talking about - thanks!
(Things weren't as clear at 2am ))    Yes we should probably definitely be removing the ThreadLocal entirely.
I'll make those changes as soon as I can.
Hi there
Patch applied in trunk.
I have added some buttons to help put variables and patterns.
Thank you for your contribution.
applies to trunk too
"Also #3 seems to be fairly simple with a single ""around"" advice in aspectj and a couple of lines for ajc target in build.xml."
Why use an ad-hoc script that's known (according Erik himself) to have false positive that require manual inspection (which is also not part of the version controlled source tree) when you we have a built-in (aspectj*.jars are already included as build/runtime dependencies) tool specifically for such tasks?
You can do this for info messages too for more performance when that is off.
There are no extra lines of code or maintenance it is the simplest solution.
As long as the change is applied at build time I don't see any real drawback.
Ah ok ... thanks for the heads up.
"There's a note on the main page about these scripts but you're right: nothing obvious in some of hte ""sub pages""  I'll take a stab at clarifying."
Changes to the test look good and your method for finding which are right/wrong is correct.
I would advise that a good thing to do once you have fixed this issue is to then perform a similar test modification for setting a valid value and using batching.
I've started to work towards support for visualising services and cascading configuration.
The changes in this patch aren't a complete solution but they're free-standing give basic service visualisation and shared configuration support.
I've introduced a decorator framework for relationships so we can draw the service triangles (or package names).
The service handling will need to be updated slightly once we're not working from the DummyInfoProvider.
Handling of services which aren't consumed and services which are consumed by multiple bundles is pretty weak.
I've also started to add cascading configuration.
I've added a Theme class and a ComponentAppearance class which takes defaults from the theme and allows them to be over-ridden.
ComponentAppearances will also inherit from one another when we have nested components (like a bundle inside an application).
I've only switched the ComponentColorElement (which was the ComponentColorProperty) over to use the new configuration so far but I'll switch the other elements over to using the new configuration next.
In the new version elements have two methods - render() which draws them and apply() which  initialises shared properties like colour.
This allows for a two-phase rendering.
Apply isn't the best name but we're already using update() so suggestions welcome.
I've also moved RelationshipElement to the elements namespace.
These are all svn file deletions which will need to be done by hand when applying the hand.
Applied patch
Would anyone have time to put together a pull request?
Hi Preben  I just applied the your patch with a minor modification on FtpConsumerWithCharsetTest.testConsumerWithCharset to use UTF_8 charset by default.
For the CAMEL-3787 I think the key should be on the ftp client configure to find out right charset that the server is using.
> MultipartEntity entity = new MultipartEntity(HttpMultipartMode.BROWSER_COMPATIBLE)   Use strict mode instead of browser compatibility.
936-2 wasn't patching correctly against the latest flume-728 so here's an updated patchfile now that 935 is available.
Tests pass but there is some weirdness going on with rat failing on licenses because it tries to examine the surefire reports...
Thanks for supplying the patch.
I have applied it in Rev.
Please close this bug if this is ok for you.
I showed my interest on this idea in the dev-mailing list and currently started to explore the requirements and a design.
Applied Patch..
Thanks
OK will open the issue with Solr/Lucene.
This patch has been around for a while.
Does this applies to 0.22 trunk or 0.23 trunk?
Thanks.
I found the note in the manual about @Resource and that fixes it.
At this point I'd suggest  peppering the docs with references to @Resource wherever it discusses autowiring things or about collections.
"E.g. ""note: if you're trying to auto wire a collection with annotations you'll need to use @Resource instead of @Autowired""."
patch committed in rev 942983 thanks!
I looked at all the TODO's in uimaj-core and found only one Sofa-related exception that needed update.
Fixed it's foot.
Patch applied.
"boisvert@sixtine:~/svn/buildr-trunk$ svn commit -m ""BUILDR-283: Misleading documentation on env."
"variables (Alexis Midon)"" Sending        doc/settings_profiles.textile Transmitting file data ."
Committed revision 816738.
The attached patch fixes two problems:   * the first one which happen when the class is loaded from the boot classloader (in which case we now use the system classloader to load the enclosing class)   * the second one which is the fact that all kind of linkage errors can happen when loading the class and this would really disturb the classloader correct behavior
Could you please verify and/or adapt your patch against the current version at http://svn.apache.org/repos/asf/commons/proper/jcs/branches/generics-interface I was not able to apply your patch there.
A potentially legitimate use case of a {{set}} without an {{apply}} is a builder class that should not be accessed until the building is complete at which point the internal state will be accessed directly to produce the final result of some type.
Such a class does not really need an {{apply}} method.
However as a tactical decision imposed by the implementation of the typechecker we will require a matching {{apply}} method for every {{set}} method and will reassess this issue later in the context of a possible typechecker redesign.
Pull request available here:  https://github.com/SpringSource/spring-roo/pull/8  The changes are minor.
I discovered that it works fine with command line.
But it does not work when I try it using maven integration plugin for eclipse.
I have open a ticket on their side: http://jira.codehaus.org/browse/MNGECLIPSE-1054
I would rather not have specific vendor JDK names listed in there if can be avoided anyway.
Not sure when it was added.
worth investigating.
Committed to branch-2.1-beta branch-2 and trunk.
Thanks for the patch Andr?Ã¶!
+1 the branch-1 patch looks good.
This reviewable and I think also committable.
Thanks for the patch.
Committed to 0.8 after reverting the change in system_test/ (the consumer property file there is used for an 0.7 consumer).
Verified patch applied as expected.
Dyre thanks for your comments.
I have made a new patch derby-2108-v4.diff which have incorporated (most of) your commets.
I think I'm ready for commit soon.
I did a change to BULIDING.txt adding ${j14lib}/security.jar to java14compile.classpath in the description of how to build with IBM JDK.
I have not verified that it works and hope someone would confirm that it's correct.
Jacopo  Thanks for diving in a pointing Scott at the appropriate screens.
David  I completely agree that providing more intelligence in the PO creation process would be very helpful especially for larger businesses.
I can just about cope with the amount of manual intervention required but in the not too distant future I can see the need for some of the rules you suggest.
The idea of consolidating POs (or SOs I guess) came mainly because I end up having multiple small POs for some of my suppliers.
However I also run into the opposite problem.
For my larger suppliers I will often have a 100 or more requirements built up.
"Since the ""Approved Requirements"" screen paginates the requirements I'll end up generating multiple orders for the same (large) supplier simply because the application operates a page at a time."
This annoys my suppliers a bit.
I guess the obvious thing to do is to go in and reconfigure the pagination so it doesn't (paginate) but if we had the ability to add items to existing POs then you could still use the pagination mechanism and build up the PO page-by-page (quite handy if your working through  your POs during less busy times of the day).
This should have been fixed by commit 26a5b76ee6b77909a7b8d5b00924f12e1671373d.
Verified on my vmware setup extract volume is working fine now.
I did that first but then remembered that when I did that in the past people were unable to apply my patches w/o doing the svn move themselves.
Anyway for this file it's not really important I think - a very simple and tiny file w/ no history to preserve?
Please keep this open until reverse tests are configured as we might have new issues coming from them.
+1 on patch.
@Andrei: Please re-attach patch using --no-prefix option.
HadoopQA uses -p0 to apply patches.
Ok got it thanks for the explanation.
In any case the code for the toBytes method is written now so it should be a simple thing to move it to the PType implementation classes.
It would probably also be worth having optimized versions for primitive values instead of writing them to an Avro/Writable first to get the bytes representation.
But yeah that's for another JIRA.
An example patch.
Was unable to run the unit test I wrote (because my setup is being a jerk) but I'm more looking for input on the design.
Thanks for the reviews committed the patch to trunk
Zdenek  Thank you for working with these components.
I seem to be having trouble applying the patches though and I'd hate for all these corrections and improvements to be lost.
Do you think you could submit a new patch in a single file (perhaps named with the current date) based on the current SVN head ?
That would really help a lot.
Rest of Ender's patch was applied.
It is good to know that pluto has a problem here but I realy need servlets integrated pluto.
@Arnaud: My key looks exactly the same and was generated with Cygwin's ss-keygen.
The file has Unix line endings though ...  @Lukas: scpexe works for me with M2 despite the fact that it insists on a key even if an agent is running (WAGON-27).
Do the same tests work fine on 1.5.0dev?
One of the things I changed in jnr-posix was to make stat and fstat use stati64 and fstati64 respectively instead of stat64 and fstat - the last may be why its breaking with jna-posix.
Look in WindowsLibCFunctionMapper.java for the fstat mapping - it may all magically work if you change fstat to map to _fstat64 (note: no 'i' in the name - uses stat64 struct instead of stati64 ... they have different sized atime ctime mtime fields).
Otherwise the WindowsFileStat structure looks ok apart from the st_blocks and st_blksize fields at the end which are not part of the windows stat64 struct so will probably be filled with either zero or garbage.
Awesome. Thats great to hear!
{quote}  Hmm looks like the backport blew away scrubtest on 1.2 as well.
Oversight?
{quote} Yes was an oversight.
Just for clarification skipBloomFilters is old.
#1 and #2 are incorrect.
1: The skipBloomFilters gets called on the index file in RowIndexEntry line 104.
2: In 2.0 the skipBloomFilters will be called on a 1.2 sstable also the skipBloomFilters currently is called against any sstable when scrubbing.
I made a preliminary patch to share the idea how this can be fixed.
"- This fix works if I run a ""mahout"" from command line against a Hadoop cluster."
A unit test case doesn't work -Dkey=value is not parsed in ToolRunner but passed through to SparseVectorsFromSequenceFiles resulting in a parse error.
Did I make any mistake?
Maybe because a JUnit doesn't really mimic a real hadoop env?
great. awesome job sergey!
IMHO Configuration should be the first or the the last arguments.
It looks like there is something about the build scripts that is causing it because hdfs.proto where NameSpaceInfoProto is defined is more or less identical between trunk and branch-2.
Looking forward to using it.
I just committed this.
Please submit new issues for the other aspects of this that remain unfixed.
+1  Patch looks nice.
Probably worth adding a third case in testDataDirectories for a URI that includes an authority (file://localhost/path).
I've committed this.
Resolving as fixed.
I ran it on current trunk.
It successfully gets the expected exception but then fails to close FSNamesystem with NPE.
So yes it fails but not where you wanted it to.
Please open using the JIra workflow and fill in the details.
we improved hash handling in Railo but we get not the same result with non ascii characters as ACF (with ascii characters everything is fine).
we cannot find a problem in our implementation anymore perhaps there is a problem in he ACF implementation that force a other resultat.
This looks like it might be a blocker for release.
I didn't see anything in subversion?
"new version of 01 ""minifies"" type classnames for the standard o.a.c.db.marshal types"
Ok thanks.
Fixed in r1146573
"Hi Christian Jacopo  Not sure if adding ""-Duser.language=en"" is the better way to go sounds like a camouflage to me."
I just committed this.
Closing issue as per Luis.
Added support for the QueryElevationComponent and test case.
Or do we act like we always had it?
Thanks for the feedback and thanks for taking a stab at this.
won't fix.
Hi Robbie  Please could you review this patch?
cheers Keith.
<danielsh> dns done (thanks for the hostname) tb build underway
Committed to trunk.
Marco  I've fixed some minor issues in your patch (including the one you have pointed out) and I've committed it.
Hi Guys  I am pretty new to this site.
I am wondering when this patch can be accepted?
Applied to trunk revision 987457.  Thanks Bob.
Nobody has started.  If you want to submit a patch that would be great!  Thanks!
Brilliant thanks!
Hi Colm   My comments are the same see above: https://issues.apache.org/jira/browse/CODEC-161?focusedCommentId=13485148&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13485148  I've not gone through to see if you've addresses any of comments but a quick scan shows that you did not pick up 2 and 3 so I'll wait for the next version of the patch
Hi Gary I think I have a rather nice and elegant patch for this
abhi  can this be assigned to someone  thanks /sudha
"""> Have checked in this change which can be reviewed here: http://fisheye6.atlassian.com/changelog/click/?cs=766349  > Would appreciate any feedback.  I just have the feeling this entire ControlRegistry is growing over our heads. Taking a look at many older applications with older Click releases I can't stop thinking that Click was very good for years without it. I also received a few custom components for maintenance and I don't think this change (nor ControlRegistry) is of any needed in them (or maybe simply I'm not getting the entire concept).  Of course I'm a newbie but my impression is that most of the advantages you list are for ClickClick. Is it possible that the problem is with the ClickClick  approach(and that should be changed) not with Click?  Many parts of Click are simple to understand and nice but this one is not. Making it even more complex is just a guarantee that users like me will understand it even less.  From what I saw one of the most important parts of Click is backward compatibility. For this however Click should remain very simple otherwise it will get big and bloated(and it can't afford """"published"""" experiments). Tapestry on the other and can do this since with every new version they throw aboard the entire compatibility so """"experiments"""" if go wrong are not that a problem - and history shows that complex things always go wrong sooner or later (latest one: with T5.2 they'll trow maven away now :) )  My wish is when I look at the Click source code to be able to quickly understand what's up there - cause only so I have a good feeling when I use it  in applications (otherwise I have that """"black box feeling"""" - and it's not fun ).  I think like this are most users - those who prefer complex thing have a very huge choice of concurrent frameworks :). """
Hi Victor  Thanks alot for your help testing this issue!  Would be good to know if it works on Centos too.  kind regards  bob
Duplicate of  CLK-121 which is already fixed  Thanks Malcolm
Looks good patch applied -- Thanks Naoki!
Hi Mike  Do you know how to get the virtual machine file? The link to download from the rcmes web site is broken.  Thanks.   ----------------------------------------------------------------------------------------------------- Jinwon Kim Dept. Atmospheric and Oceanic Sciences and Joint Institute for Regional Earth System Science and Engineering University of California Los Angeles Los Angeles CA 90095-1565
+1 looks good.  Once it is committed I will deploy it.  Thanks Sourygna.
I just committed this to Trunk. Thanks!  NOTE: made some slight changes to patch to apply correctly to Trunk.
I just committed this thanks Ari.
+1 looks good.
+1 looks good.
Patches are welcome!
Looks good!
I did it works fine with 2.1 and iOS6.  Thanks!  Dominic
Thanks a lot for your excellent work Shazron!
Committed thanks
+1 thanks committed to cassandra-2.0 as commit 3a7093356ca032d9ce8767b2c47980aebe4bce60
Looking at DataStax's 2.0.0 Debian packages the change didn't make its way in.  Digging deeper it's only on the cassandra-1.2 branch.  Can someone merge/cherry-pick it to cassandra-2.0 and trunk.  Thanks Blair
committed to 1.2 and trunk. thanks!
Committed thanks!
Committed thanks
committed thanks!
committed thanks!
committed thanks!
ah right -- skipping the clear would be buggy.  +1 again. :)
committed thanks!
{quote} Totally didn't know we support IN for DELETE.   Would prefer to fix by offering IN for for the other statements as well. The Antlr doesn't look too daunting w/ DELETE as an example (knock on wood). Can take a stab at that tomorrow. {quote}  This is {{multiget_slice()}} / {{multiget_count()}} and was intentionally omitted (remember?).  Not saying this is decision that can't be revisited just reminding.  Oh and if we do move forward it's post-0.8 we're in a feature-freeze. :)
The v4 changes look good. Thanks
LGTM +1 (and I think I'm sufficiently convinced that it's OK for 0.7.1 as well).  I think tjake was planning to commit this so I'll leave it here for the time being until he has a chance to look it over.  Thanks Stephen.
+1 Thanks!
This looks good to me.
+1 This patch looks good. Let's remove from 0.4 and work on a better implementation on 0.5
Well if it is just a documentation bug even better. Thanks for the clarification.
Good suggestion Claus and thanks Alexander for the fast realization.
Applied the patch into trunk with thanks to Luca after minor refactoring.
Thanks guys!
Thanks for the patch.
I'll look into the namespace handler. Thanks.  Yes I should have added the different event handling support to the TODO above. I need to think more about this and look into the camel code a bit more. Using just the endpoint api I couldn't create routes on the fly hence the polling consumer approach.  I'll keep hacking away at in in my toolbox when customer needs dictate.  Am I missing something in your approach how would I do this then in the XML DSL using your approach?  I think virtual routing is a known limitation of Camel when you start to deploy large scale systems. Out of curiosity have you seen camel ever been deployed with 1000's of routes (and/or routes files)? I think this would have FD and thread overkill problems. Everything repeats itself :)
Hi Willem  I've just tried the latest version from the trunk. Now everything works like a charm. Thank you very much for such a great help.  Regards Sergey
I agree that's a good idea.
Committed r1146903 Thanks Jonas for the patch.
Yeah the patch is great. Just that it should not have been categories as a bug :)
Applied patch with thanks to Chris.
Thanks a lot Willem :-)
Xueqiang  I just committed your latest patch. Keep up the good work!  http://svn.apache.org/viewvc?rev=803128&view=rev
Applied patch with thanks to William.
Committed revision 1506513. Thanks Vinay.
Committed as r1406707. Thanks for reviewing Sijie.
ah got it. thanks Ivan for explanation.
Thanks Ivan for take a look. I'll go through and work on this weekend.
Hi Ivan  {quote} + long numberOfEntriesToReplicate = lastEntryId - firstEntryId
Thanks Ivan. I'll do the changes by just return if it matches as you suggested. I'll investigate it later.  bq.Auditor will be a top level daemon so it should own its create it's own LedgerManagers and ZooKeeper client. The only thing passed in to is should be the configuration.  Anyway we are planning to have initialization class which does starting Auditor and RW threads. Would you agree to create ledgerManagers there and pass it to the Auditor and RW daemons?  In that case shall I keep the ctor as it is. (by taking managers)  {code} public Auditor(String bookieIdentifier AbstractConfiguration conf             ZooKeeper zkc LedgerManager ledgerManager             LedgerUnderreplicationManager ledgerUnderreplicationManager) { {code}
committed as 1307725. thanks for Ivan's improvement thanks Flavio for reviewing.
new patch looks good to me +1
Committed r1185532 thanks Flavio.
Great! Thanks.  Aaron
Thanks Rahul!  I will take a look at it this evening.  Aaron
Thanks! I have not tried it yet but it looks good to me.
Committed. Thanks Stephen.
Thanks James does the rest of the build work with this patch? Seems like this should work with the old RHEL5 toolchain but I didn't look closely.
Closing based on comments from Matt.  Thank you.
Applied the patch with revision 170884.  Thanks Carlin for the detailed updates (and for all the fixed double-dashes :)).
Fixed thanks for the suggestion:  http://svn.apache.org/viewvc?view=rev&revision=498105
Looks good.  Applied thanks!    http://svn.apache.org/viewvc?rev=606891&view=rev
Please apply this patch file.  Cheers Kaushalye
Thanks Andreas I will check it
commit the patch to trunk. thanks sameera.
Hi Rich!  Thanks for taking the time to move forward on this.  I didn't have any JUnit problems at all with Maven2 when I installed it.  But clearly we're in a world of hard-to-replicate behaviors here. :)  What is the exact problem you're seeing?
Great! Thanks a lot. I suppose it will be in the next nightly build?  VBR johan
Done Thanks chatra
I just committed this. Thanks for the reviews Doug.  (BTW the last patch didn't include the removal of lang/java/mapred/src/main/java/org/apache/hadoop/io/SequenceFileBase.java so I did that manually when I committed.)
+1  Looks good to me. Thanks.
[~thiru_mg] - thanks for the patch! Looking forward to it landing in 1.7.0
I committed this.  Thanks Raymie!
I'll do it now thanks!
I committed this.  Thanks Miki!
I just committed this. Thanks Jeff.
Thanks Rick for applaying the patch.
Hi Holly  Thanks!  I have the one comment -  where is the code for InternalRecursiveBundleTracker?  I don't see it in the patch  Did I miss the source code for it?  Lin
patch committed thanks!
It's working! Thanks.
Thanks Ken!  patch applied to trunk:  r480575
Patch applied!  Thanks.
Great - thanks for the heads up Ben!
+1  Looks good to me.
+1 the patch looks good.
+1 looks good.
+1 looks good - all tests pass.
+1 the patch looks good.
+1 the patch looks good.
Committed.  Thanks John.
Committed. Thanks Hitesh!
Committed. Thanks Yusaku!
Committed. Thanks Hitesh!
Committed. Thanks!
Committed. Thanks!
Thanks
"""hi Im Sujan. I need to send proposal for gsoc  2012 for"
Now why did I not think of that... :) Great idea I'll move the files to svn!
I applied the patch thanks!
Thanks!
I think we are very close too.
Holy crap that worked!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
I think SSTableDeletingTask is a good spot I just wasn't sure if that would be appropriate.
Nice try :-)  I was Mr I dont believe in magic from the very beginning.
Are we on the good path...
hi Dan!!
Wow sure!
Thanks  Kevin
Applied the patch into trunk.
I just committed this.
Violated again in r960471.
I'll take a look at this for the next release (after 1.5.11).
Committed to trunk (0.12).
Thanks Viraj and Cheolsoo.
[~maysamyabandeh] Got time to look at this today.
There are tests in Mahout-integration that are using DummyRecordWriter  -  TestConcatenateVectorsJob and RegexMapperTest and are failing with this patch.
I'll take a stab at this again but could you verify this patch against the present trunk?
patch applied.
Thanks!
How about a slightly different take?
Please take a look at the patch and let me know if it fixes the problem for you.
Vadim you're keeping us busy :).
Hello I see Uwe has commented on SOLR-1657 that he would help convert tokenstreams to the new api but he needs this patch (the tests) applied first.
Is it possible for someone to take a look at this patch to get things moving along?
its only tests no source code changes.
Leo thanks for the patch - applied at r488522.
I did translate some Labels to german language.
fPIC rings a bell on amd64.
What are the disadvantages to adding -fPIC (a) for amd64 only (b) for all platforms?
This is fixed in V2 and an automated regression case has been created.
Closing
Thanks Sanjay for the review.
I have committed this.
(My 0.8 checkout was on the 0.8.6 tag when I reported that CliTest worked completely.)
I committed this.
Patch checked in.
Thanks for the patch.
It has been applied to the trunk.
I'll commit this soon unless there are objections.
Thanks for the patch Glen.
I'm gonna review it and apply.
Can this be closed Martijn?
Committed to trunk 1384014
Resolved fixed!
Please review this for me!
Thanks!
This is not a Camel issue but a CXF.
committed.
(looks like these were introduced in CASSANDRA-1714 fwiw so i'm going to tag affects-version to 0.7.1)
Released with GEP 221 so I close this JIRA.
Untar this to populate the position_hints table.
Change the summary to match the real issue.
You are absolutely right.
There must be some side effect in my project that circumvented this problem until 6.8.0.
Sigh ....  Do you think that is something that can be fixed in Wicket or some kind of IE8 madness that i should look to solve on my side?
Edison can you check on this issue looks like the fix is not working
Committed with rv800094
Patch for aggressive GC test
OK to close?
Oracle 7u40 runs just fine on Debian I can't imagine it wouldn't work on Ubuntu.
Tested with both JBoss EAP 5.x and Geronimo 2.2.x
Fixed in 426832.
you seem knowledgeable what not submit a patch.
Applied to trunk.
I just committed this.
Thanks Jothi!
I've just committed this.
resolved in r950171
If possible can you make sure the patch is checkstyle compliant?
Resolved in 1.0.1-SNAPSHOT.
Committed to trunk with revision 447502.
Will leave open until merged to 10.2
Committed to 3.4.6 and trunk.
This patch has been improved a lot.
What classes were changed for this fix?
Would like to pick up this fix with the stable release if possible.
My testing turns up not issues so  once again thanks for finding this Rick I'd gone blind to the problem and it was driving me mad!
Enis/Ashutosh we can try coming up with the testcase.
But in the meanwhile can I upload a patch which reverts this ?
dead branch no sure which code generator it's talking about
Patch is committed to trunk.
Btw I only get this error in LDAP Studio.
Not in LDAP Browser 2.8.2 or MS AD Explorer.
Thanks.
TestSkewedJoin finishes when you give it enough time (a mere 49 minutes) and the other tests I ran passed as well.
Milinda patch applied !
Zhenxiao I left comments on Phabricator.
Hi Sergey thanks for the patch.
But why have you added testPropertyChangePropertyChangeEvent test?
Or at least has some relation to above problem?
I will revert tomorrow if less-hacky alternative suggested above continues unaddressed.
Your explanation sounds reasonable and  the patch looks safe to me.
+1 for the patch.
After talking with Steve B. we decided that the DAS should insist that datagraph root is passed to the ApplyChanges command.
An error is now thrown if this is not the case.
The fix for test_no_extension was merged in PIG-2505.
So this is ready for merging?
This is now resolved in ActiveMQ 4.0 as we have a DemandBasedForwardingBridge which only forwards messages to another broker when there is a consumer for it
Jacques  Why concatenate the patches?
Thanks guys  Bharat your patch is in trunk at revision: 754834 .
I only review as Pranay tested successfully.
Should we close this one now that we went a different route (zero javadoc warnings?)
Thanks for the clarification.
Patch applied in SVN revision 743258
Patch applied to LUNI module at repo revision r494822.
Please check that the patch was applied as you expected.
I have applied it in Rev.
Patch applied.
+1 if the test is problematic we can always either comment out the new test or @Ignore it and add a note.
The changes look good.
+1 for commit.
I think allUserRegionsOffline should be declared inside {code} } else if (this.stopping) { {code} block.
Have you run test suite ?
The piece of cake was bigger than my mouth... Postponed.
Please look at this patch (d962_javadoc.2.diff.txt)   I ran javadoc and  did clobber and ant all ok.
Thanks.
Applied patch with thanks to Dan :)
Changed the patch as per Thejas's suggestion.
Thanks patch applied
"Added comment before ""Path getPath()"" to address Todd's comment."
Actually I thought to take a look on this weekend.
Unfortunately I may not be able to look on this weekend as came out of station.
Thanks for taking this:-)
Committed to master.
Hi Myrna  Are you recommending that we commit this patch but that additional work be put into analyzing LocalizedDisplay.sql and LocalizedConnectionAttribute.sql?
"Jason  I checked...you have karma as you are in the ""jira-administrators"" group."
Please go ahead and create it.
thanks dims
Patch applied thanks.
Vote is here: http://markmail.org/thread/yg6n646zurlpdmvy
Thanks is that everyone has to create an account and then you will link them to Pig Confluence?
I just committed this.
The patch looks fine.
I'll commit once hudson +1s it.
I just committed this.
that what WebRenederendingServices are for not sure WebDescriptionProvider could be declared to be a WebRenderingService.
OK You are correct about that.
I ran it from the base and it looks good.
Thanks +1
Adrian I cannot find the listVisualThemes.ftl file in your last patch.
From your comments I guess you changed it.
Hi  Would you please try my patch?
The patch also fixes a typo in StreamHandlerTest.java.
Thanks!
Better to have the samples run on the fly :-) .
Now it works as expected.
Looks fine Sreekanth.
Leo patch applied at revision r438040 thanks a lot for this enhancement please verify that the problem is fully fixed as you expected.
Applied patch with thanks to Christian.
Patch d1544-patch2-v2.diff committed revision 429036 - Thanks Deepa
Committed.
So returning an empty string in such case.
This irritates me on a weekly basis.
I'm often running ad-hoc analysis in grunt with snippets taken from some heavier scripts I've written and I get bitten ALL the time.
Hi Mario Ivankovits    Pls give some suggestion on above comment
HI Harry  I am new to JSPWIKI and based on the error I chose the Security issue.
You can choose to ignore or modify it.
I am still not able to run JSPWiki in Oracle Jdeveloper 10.1.3.3.0.
Hi Subramanyam  Any updates on this issue yet?
Fixed in http://svn.eu.apache.org/viewvc?view=rev&revision=707319 Many thanks to Ulhas for providing the patch!
Hi Andrus  I cleaned up for one of our projects successfully the comments and could import it into CayenneModeler - great!
Attach patch v2.
Consolidate all error handling in KafkaServerStarble.
The patch looks fine .
I was sure It's in CVS already.
Let's commit it .Thanks!
@Stack @Ted  Can u people have a look at the patch pls?
"Namit what do you by ""describe div""?"
There is a test udf_divider.q in the patch.
Do you mean more test cases?
Hi Pepijn  Can you please take a look at r1215075 ?
Thanks!
It works perfectly with the latest 2.7.8-SNAPSHOT thanks !
Done at rev.
The latest patch looks good to me.
fixed in svn r344339.
Tony in native codes I think the hymem_allocate/free_memory is preferred than malloc/free?
Patch applied to TEXT module at repo revision r450026.
Please check that it was applied as you expected.
Patch looks good to me & has been committed thanks!
Fixed at revision 936652.
Applied patch from Keheliya Gallaba.
Fix applied to master (6d1d23b7d0eba4de3c4097907adc37b09191196e).
1
+1  before committing do you mind combining the new test with one of the existing ones?
Trying to keep the test suite at under 24 hours :)
I've committed the patch.
+1  I committed this.
Leaks file handles via IW.getReader()
+1 I just committed this.
I can't apply 0002 cleanly to trunk.
Thanks
Yes we could enable junit's fork mode.
I will have a look at this.
But it would be better if you could provide a patch -) Thanks again.
Hi Uwe  I updated the patch now it uses TermRangeQuery and does not cast anymore getAttribute
On 2009-11-18 04:47:59.970 dloverin commented: Revision 11926.
I committed the patch to branch-1 as well.
+1 patch looks good to me.
I'm going to commit this momentarily.
All the regression tests ran cleanly with client-imports.diff.
Committed revision 946641.
Daryn - did you get a chance to review the latest patch for this?
Fixed in rev 1137251 and 1137252.
Patch submitted.
Please review.
PIG-1912-3.patch address Santhosh's review comments
Okay.
Anyone else want to chime in?
If no other objections let's commit Clint's patch since it works for him.
Thanks for the patch.
I polished and fixed CS as well.
I updated the wiki doc for the sqs component as well.
Thanks.
I just committed this.
This behaviour is correct as this is a test for pluto and not a generic JSR 168 test.
Hi Anil  Did you get chances to review and test?
Go for it (provided it works -) Will need to test XSP samples and make a note somewhere in the docs.
i've attached a libs.zip that contains bookkeeper and zookeeper libraries.
i suggest that we commit this ASAP and then do the work to fix the build once the code is in subversion.
I will try your suggestions
Patch was applied analyzers can now be instantiated by writing some javascript to be executed on the server side.
Patch applied thanks
New 2.0.5 and 2.1 snapshots are deployed that contain the fixes.
Can you give them a try and make sure it's all OK?
Thanks!
reviewed looks good.
Applied patch in r637925
Alejandro Abdelnur sorry I don't get it what history you want to keep?
and how can I verify the history is not lost?
Sounds like JavaScript samples are now being done in TUSCANY-287
Jacopo & Anil  Here is the patch for this issue.
I'd say last Sun JDK 1.5 as it's still the one recommended for OFBiz (even if I guess most of us use 1.6)
Committed the patch to trunk (revision 63660) and branches/2.1 (revision 636606).
Excellent!
Looks we all ran into the same issue with TCK.
Good verification.
Sorry about your signal :-(
> Attached is d1644_combined_v3.diff a patch proposal which combines the two previous code change patches.
I will try to review the latest patch sometime today...
540843 I'm leaving this issue open because a similar patch should be applied to the ecommerce/includes/language.ftl file  Jacopo
Just updated and ran through a few test runs -- haven't seen this since.
Sorry that this patch seems to have slipped through the cracks.
Hi Corinne did you attach the wrong patch?
Yes it seems that in some cases the exceptions are swallowed and e.printStackTrace() is heavily used.
Yes You are right [~jiannan] If the {{bkActiveLedgersSnapshot}} contains ledgers which are > {{lRange.end()}} then there is no chance of being gc'ed in the current gc call.
But if one more ledger is created then on next gc call all ledgers deleted ( which are having lesser ledgerId than latest created) will be gc'ed.
Committed revision 642996.
(Both the patch and the JUnit tests).
Yes that makes perfect sense to me.
Committed.
Patch looks very good.
1
@Jon  +1 the patch looks good.
Committing now..
I'll have a deeper look this evening.
:-) Cheers
I committed the patch.
I had a look at the rpm side and it looks ready to commit.
Thanks for the patch!
Hi Kevin - Any update on this.
Thanks
+1 on patch.
It looks like what was applied to trunk.
Is that so Ram?
aim to merge patches and publish for review by this coming weekend
some tests are still problematic at least on windows...
I think perhaps some of the crazier ones like DiskFull TestCrash anything that has to disable MockDirectoryWrappers's checks because they must create corrupt indexes or other scary things.
Hi Jeremias.
Looks fine to me.
Applied thanks!
Can I get a +1 from someone else.
Should be a quick review.
Thanks.
Thanks for the reviews everyone.
Jan  I looked that the logic again I think your are right.
When I did a quick glance last time I only saw the isolated patch and didn't see enough context to see the existing retry logic.
(review board is helpful).
Mind adding some comments explaining why this is ok to retry?
(We are retrying once and if we fail twice we give up).
It seems strange to me that we are retrying something that throws a DoNotRetyIOException.
Anyone else have any comments?
ok... i will change it then and keep in mind in case we run into odd issues permission wise.
Patch committed.
Patch committed to trunk.
Applied jackrabbit-core patch in revision: 949724
This newest patch applies cleanly and tests all pass.
Personally I'm +1 to commit.
I'd like for Stack or Jim to take a look at it before we commit it.
You've knocked out a really ambitious bit of work here.
Committed to master.
Everything is ok.
Thanks.
Verified at revision 473588.
Hi Uma thanks for taking a look.
h2108_20110727b.patch: fixed DatanodeManager constructor and activate(..).
Unable to apply cleanly.
Patch applied in revision 889133.
Please check that the patch was applied as you expected.
combined patch against 1.0
Screens too big to attach: https://www.dropbox.com/s/p2e2gvi87vw772n/cordova_screens.psd  Eventually this should go on the Cordova website and not the repo I think.
Verified.
All the changes are correct.
New patch looks good (nice comment on why NODELAY).
Let me test it.
Subversion dump file.
I found the same issue about unparkSuccessor with the latest snapshot also.
Please take a look and see what else needs to be fixed.
+1 the latest patch looks good to me.
I'm going to commit this momentarily.
The patch looks good.
1
Hi Claus  Please find the attached patch and zip files containing the changes you recommended.
I will check-in this version...
I will look at applying this later today.
I just committed this.
+1 the latest patch looks good to me.
Allen has Jon addressed all of your concerns?
If so I'll go ahead and commit this patch.
Please let me know.
Committed r1463555.
Alejandro we are in a middle of the release right now but I'll work on that over the weekend perhaps.
This patch fixes the issue: # Short circuits the MUST_PASS_ONE case just like it is done in the MUST_PASS_ALL case.
# filter.filterRow() is called *after* filterKeyValue in the normal flow of things.
This makes the test do the same where it matters.
I am quite skeptical about this.
+1 (subject to hudson)
verified by Paulex
Ashu  Please take a look at http://cvs.apache.org/viewcvs.cgi/xml-xerces/java/src/org/apache/xerces/dom/DeepNodeListImpl.java (mentioned by jongjin)  -- dims
This issue is fixed.
Fixing this issue led to the next issue.
(will commit after Hudson +1s that is)
HI Christian  I've applied your patches to trunk.
Looking good.
Thanks.
Could you verify the fix against the next nightly snapshot?
Updated patch to work with the latest shindig revision that is subversion revision 727075.
Are there snapshots of the volumes belonging to the VM?
Thanks  rev.
Changes looks good.
@Ivan could you please add your comments?
if you like please push this in.
When I put this patch it did not have this issueLet me update the patch.
Thanks for finding this out.
+1 for the patch.
I committed it.
I'm not very familiar with ant but i'd think it's just a wildcard for a classpath.
I misunderstood your comments.
Thanks for the clarification.
It make sense I am attaching the updated patch.
I've tried this and Can't seem to find any problem.
Can you attach the WSDL /XSD files that created the problem ?
Wow that was quick!
Thanks for the solutionhints.
Works fine and the way i (and w3c) wants it now.
No problemo Close is fine for me.
Thanks correctly applied the patch and updated working copy.
Yea this fixes the other tets.
I'll re-run the whole suite on branch-20-security for sanity before committing.
Patch applied thanks Nils
Wrong JIRA...please ignore my previous comment.
Committed revision 543344 in sandbox.
Jason thanks for the patch.
Thanks for the review Sidd.
Attaching the patch for branch-0.23.
1
OK we'll leave it with Atlassian
how about changing to ERROR for org.apache.zookeeper.server and leaving as it is for the rest?
Hi  Thanks for your comments.
It is supposed to be handled as part of http://issues.apache.org/jira/browse/OPENJPA-907  There is a patch attached to it (I was a bit liberal with the RegEx).
I will correct it and ask for commiting the patch once I am done with this.
Regards Ravi.
I have committed this.
Applied the patch with revision 169493 -- thanks.
This a patch submitted by Ankur Goyal.
I will apply and verify it.
Junit test.
I have attached DERBY-3084.diff and the corrected file rrefsqlj31580.html.
Please let me know if any further changes are needed.
Committed to trunk.
Yeah I know.
Just noticed when trying to vote for 1.3.0.
Qian  Does seem like something wrong with hudson.
Pat and myself are taking a look at it .
thanks
Committed to trunk and flume-1.4 rev: c0c4947.
Submitting it for Hudson to run the tests.
Can someone review the patch in the meanwhile?
Looks good I'll commit.
Hi Knut  I am interested in taking up this task.. Is that fine and kindly direct me on where to start off here.
I will investigate this issue and update the patch.
Thanks -Suresh
Cheers    Berin
I updated the indentation and doc/build.xml.
I think that the problems are not related.
I think that Henry Zongaro should open a new defect for the local-name problem.
Fix for the gererate-id() problem seems to be correct.
Thanks Andrew.
Patch applied to PACK200 module at repo revision r611862.
Please check it was applied as you expected.
patch to add DynamicAttributesHandler and update SimpleTheme AbstractCommonAttributesTest and AbstractTest
"re: ""should remain"" - yes I vote it should remain but be more flexible"
If someone wants to take this over that's cool - I'm unlikely to have time in the next week and a half though.
It is a nasty bug that I've seen in real life though.
Yes exactly -- sorry to be so unclear.
Currently the user passwords are too insecure.
I'm not sure what the problem is but the behavior (at least in 0.1.4) is that the _isClosed AtomicBoolean is not being set to closed in a timely manner.
Sorry for the delay and thanks for the good work works like a charm!
Sorry I thought submit patch was the way to attach a patch.
It's kind of annoying to have to use 4GB of temporary space  Nope it only writes the compressed file to disk {{gzip -1}} compresses 4GB of zeros to 18 MiB.
I don't have very much experience on branch-1 would you like to take a shot at the port?
Especially I don't know very much about the test framework differences.
I will figure out the details and do the port later this week if you don't get to it first.
sorry unassigning myself from this backport.
Another backport (for a different jira) of mine is failing and I want to see if I can figure out what might be wrong there before doing a new backport.
To upgrade to a recent version of Jackrabbit see http://wiki.apache.org/jackrabbit/BackupAndMigration I'm afraid I can't say much about the risk.
I've tried something similar (I removed the handlers and kept the readers) but the performance was not visible.
The responder seemed to be a bottleneck.
But it was not the only issue: we also want to manage priorities between the tasks but we need to read them to get enough information to make the right priorities.
While creating the DerbyTest I did notice that this problem is caused by the Database pool because it tries to connect to the database in the background.
Sorry [~fiberlijun] - havent had a chance to look at it yet.
(I could not get what KEYWORD means.)
I am checking out the dojo fixes and patching them in sorry for the delay it has been a long time
> then we don't save IO by limiting the buffer size to 1 KB  I'm confused by this.
Seems to be failing for a different reason now  testContainerLaunch(org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutorWithMocks)  Time elapsed: 0.523 sec  <<< FAILURE!
Sorry for all the issue mails I forgot to mention the second:  2) There is commented source in the test should it be enabled or removed?
I'm very sorry.
Sorry guys... :( I was the reason for this trouble.
I should have been very careful before doing this.
My bad forgot to include the test file in the patch... Resubmitting in a second.
*sorry*
Damn.
Git is tricky that way.
Sigh I don't like this overall if can't reuse everywhere.
Sorry to let you lose a wild goose chase.
You are going to kill me when I say this but it seems like we should just commit your first patch.
This half-reuse is more confusing for new devs who see this code for the first time.
Sorry probably a misunderstanding from my side....
I meet the same problem on Eclipse recently but haven't figured out how to get through.
Sorry Chris  I missed it done !
I don't think there is any sense in this who cares?
We reported this crash to Oracle in plenty of time and the *worse* wrong-results bug has been open since May 13: http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7044738 but Oracle decided not to fix that too.
One of the error is that hiddenActivations cannot be resolved.
I'm very busy for the moment so I didn't know when I 'd be able to the change myself so if you can your contribution would be one more time really appreciated.
Oh I didn't consider one flow like after the edit log conversion immediately #store failed.
I misunderstood the case and sorry for the confusion.
Aside from the job succeeding (it doesn't) what effect should I be able to measure (in order to see if this is doing anything)?
You misunderstood the response: StopFilter indeed did not change.
You would need to implement session resume thats a whole new can of worms.
And the current behavior is irritating.
Sorry for a mess in uploaded files my connection got broken during first upload.
Here are 2 patches ... one for adjustExamplePaths.bat & .sh and the other a new file holding 2 User Libraries ... UIMA_LIB & UIMA_AS_LIB Not sure where this should go ...
i found the class - so there is no bug - sorry
I was not aware of being required to call that method.
This is a silly one.
Seems we didn't enforce an exec for sh but we did for fs.
The patch should be relatively trivial but like I said I have no idea if there is other important stuff going on there or not.
Igor sorry it took me so long to get back to this.
Hey sorry I did not get back to you sooner.
I found the (sad) attempt at a script you asked about and uploaded it to MAPREDUCE-4282.
Sorry for the delay.
I don't have resources to hand to test this at mo.
Yes it is a dup thanks Mike for taking care of this (I planned to do this yesterday but didn't make it)
Forgot to add that I also tried this with and without HBASE-5864
but app developers should always explicitly set it based on their application  My guess is that a large enough group of people take the defaults that it matters.
App developers never do what they should :)
"problem - the bit to sign the source-release is being called when the source-release isn't being built because the source-release is only built at the ""executionroot""."
Excuse me for stolen assignement.
In current version the same error cause different messages because there are many commiters.
Must be: size >= 0!
Sorry for the missing class src/java/org/apache/jdo/tck/lifecycle/StateTransitionsReturnedObjects.java.
Sorry if this was not clear and yes by <blank> I meant an empty string.
Qianshi is working on the SSL session reuse but this buggy Bug system does not allow him assign this ticket sigh
Hi Guillaume  I did not have an answer right away so I sent you question to Leonard Rosenthol.
Sorry should have commented on this much sooner.
It is not yet tested - I ran into strange build problems with APR.
Done sorry for all the mess on a simple patch.
"Secondly reading this code I can see why this bug is happening after completing stage 2 above when adding the new item the code does ""ordered - cancelled = quantity"" which equates to ""1 - 1 = 0""."
Keep repeating this and the problem just gets worse.
Please ignore attachment (id=6938).
Thanks.
Looking forward to your patch.
Thanks for the info David.
That is useful advice (call me biased if you'd like).
HI Anahita  Your patch tested successfully.
I think this is pretty close to being ready.
It's great to see you've gotten the ball rolling Chris.
I'll be happy to edit the wiki when I get this resolved!
Hi John  I would agree using object notification is probably a cleaner way to do this.
> - FOUserAgent.getStream() is cool and very easy to use (now that it's properly > documented).
"i kinda like the black acctually .. but that may largely be due to my prefering the ""fire"" solr logo over the futuristic magnifying glass (the red/orange contrasts nicely with the black/grey.  to my mind the ""Solr"" ""Sun"" connection has allways been really cool ..."
Awesome discussion.
Got it thanks for the explanation.
The patch looks good.
Cool cool - thanks stack!
Awesome! This tool is cool.
Thanks guys for saving mankind!
This looks safe also
I've just tested the shiro trunk with karaf-2.2 release and everything works fine :)
Pretty sure this was fixed by the time we released JRuby 1.6.
Interesting I just replaced my src with the latest in the trunk and did a compleatly clean build and the problem seems to have gone away.
This is Awesome Stefan - thanks a million!
This is awesome - works in a one-liner.
Awesome!
Nice I did not think about using the system tables.
You're results are awesome Paul. Great work :) Looking forward to see your new JSON parser in trunk whenever you think is ready.
Awesome you rock Drew!
Very cool.
Awesome speedup! Finally all this work shows great results!!
@Subbu: Glad I was able to help.
I believe in relativity :-)
Great! It's been good to be able to contribute and I hope to be able to more for this awesome project when I have the spare time.
Thanks Uwe!
Code looks cool.
Cool - good information to have.  Thanks Lance!
Hey very cool.
Hi very cool!
I like the elegant parser!
Thanks for fixing it so quickly!
Jonathan: over to you :)
Cool looks good.
Cool. sounds good to me.
Hi Carlos This looks awesome! Lots of cool stuff.
"Waaaaay cool! I especially like the ""upload file"" option."
cool stuff! a good candidate to put on myfaces-commons-utils so the user can just add the resource handler on its webapp faces-config.
Thanks Daryn!
Thanks for the reviews..
Wow thanks Raghu that's awesome and will save me a ton of time.
So very cool and so very committed-- thanks Matthias this is awesome.
Took me a few seconds to get it but then I realized how awesome it is.
I'm happy to use it as the standard logo (i.e. website etc.).
http://svn.apache.org/viewvc?view=rev&revision=468444  Deepal Thanks!
Awesome stuff Stephen!
Thanks Alexei the patch is fine.
Awesome co-op. Thanks Robert & Mike for picking this up.
Cheers!
Thanks Galder:)
Daryn the current patch looks good.
Sounds good.
This is awesome!
v2 Looks good to me.
Hey you're right that idea was probably not the best approach to the problem - 0.6 looks really nice looking forward to it :)
Thanks for moving so quickly on this guys.
Thanks Sanjay!
Cool thanks Daniel we're actually cool with either updating it to e9ff8c4928 or flipping the write bit and we'll push the delta ourselves - whatever's the fastest to get to writeable:)  Thanks --tim
This is a great suggestion.
great thanks Joel.
Thanks for reporting solved!
it belongs to faces 1.2 :)
Ok cool this is just a matter of adding strings to the various [NotFound()|https://github.com/apache/mesos/blob/0.15.0-rc4/src/files/files.cpp#L160] returns in files.cpp  There are a number of error cases in the [browse|https://github.com/apache/mesos/blob/0.15.0-rc4/src/files/files.cpp#L204] and [read|https://github.com/apache/mesos/blob/0.15.0-rc4/src/files/files.cpp#L204] endpoints I'd imagine one could change the javascript to generate bad requests which should be sufficient to test this :)
cool no worries.
Cool feature!  Performance:  - It looks like scripts are read from the resource loader and parsed again (eval) for every update request.
Thanks Ram  That seems like a definite improvement.
Interesting that you decided not to detect the error based on finding two similarly-named operations.
It's OK.
Hi Suran  The new catalogs_b.patch looks very good.
Nice feature lars!!
:)  I've renamed ComponentColorProperty to be ComponentColorElement and RelationshipAggregation to TwistieAggregation.
Looks like this is fixed :)
Version 2.2.0RC3 is fine.
Sounds like a good idea.
Many thanks for your help.
Thank you!
I've tested them out and everything is fine.
Is that ok for this file (b/c I have no idea how to do the svn move now ... after I've made all the changes already) :)
Thanks once again.
bq. OK w/ the latest patch all tests pass for me! Great  Awesome! :)
Great! Awesome!  thanks Dims
Great! Awesome work Nick and thanks for the clarification Bill.
Great work...this is awesome stuff.
great. awesome job sergey!
Awesome - great stuff Maria.  Thanks!
Thanks for doing that Uma.
Wow!
Thanks Bryan!
Patch looks good to me.
Thanks Jeff!
It is good to have the test.
Thanks Owen!
Thanks!
Thanks Dhruba!
Patch applied with thanks!
Looks quite perfect after solving PDFBOX-490.
Thanks Ashish!
Many thanks Neeme.
Thanks for the help BJ!
thanks Tomas!
Thanks!
Applied patch with thanks to Scott.
Enjoy!
thanks to Ernst Fastl for adding this.
Thanks for the patch Mubarak.
Thanks Koji!
+1 committed thanks
The changes to LazyBoolean.java look good.
+1 to what Esteban said.
Committed.
Committed.
Thanks Edward
Fixed in http://svn.eu.apache.org/viewvc?view=rev&revision=702423
updated patch submitted at: https://reviews.facebook.net/D4455
Hi Yuval could you please attach a sample project showing the error?
Or even better do you have a patch?
Created an attachment (id=12292) corrresponding patch for LocalStrings.properties - will provide more as I encounter them
Hello Paulex      The fix is fine.
Thanks!
Best regards Jimmy
A test suite that shows the issue
> Eagerly awaiting faster mirroring with github.
See INFRA-4641 for some thoughts on how we could/should achieve that.
Input welcome!
I just committed this.
Thanks for Confluence update Francis
Svetlana  Fixed in LUNI java.net.DatagramPacket at repo revision 376009.
Please check this fully resolves your problem.
@Matt It would if I submitted the patch... Ugh  Lets see.
Patch applied.
Thanks [~bikassaha] addressed your comments.
Attaching a new patch.
The fix looks fine at r517575.
The patch looks fine.
Thank you
On 2010-03-01 15:37:55.688 jchuang commented: AIR bug is fixed.
Can you please submit a patch?
thanks dims
Thanks!
Patch applied in revision 885311.
I just committed this.
Shawn committed the fix in  rev 1159269 & rev 1164330
my xsltmark tests now run the docbook test without a problem.
Thank Morris.
Patch applied at 472664.
Please check that it was applied as expected.
why do we need normalmappedsegment as well as native?
could we get rid of normal?
Patch d2087_try2.diff  Committed revision 499100 - Thanks Julius
Patch applied in r710027.
Thanks!
The rest of the TODO has been postponed to Camel 2.0 in CAMEL-971.
Thanks Jacopo this is in SVN rev 7808.
Yup it does the trick.
I've ran the tests are both seems to be running fine now.
+1 on the patch.
As I know this is working  [I didnt not test with JSR 181]  but I have tested with POJOs and it worked fine for me   Thanks Deepal
Patch applied in rev 586231
If tablet unloads were to use the normal compaction queue may want to prioritize unload related compactions on the queue.
Thanks Ivan I commit the code according to your suggestion.
Committed revision 1298668.  thanks for the suggestion Antony
Verified at r495393 thanks Tim.
Hi Jacopo  no problem.
I agree.
Thanks for looking.
Patch committed.
Thanks for the patch Ferdy.
Lets see if someone else votes that they want it and if so then we'll commit?
Hello Ruth  The patch has been committed at revision r502513.
Thanks a lot for this enhancement.
Please verify this issue is fixed as you expected.
So i'm going to make a patch that does 1G regions and 128M flushes (with the multiplier still of x2 so we'll grow to 256M before the barrier comes down).
Any objections?
(A 2GB memstore flush is fine when lzo is on but we want something for default).
Updated patch so that the needed values from uiLabelMap can be accessible in ftls too.
I have applied your patch http://svn.apache.org/viewvc?view=rev&revision=757162 Thanks Pierre-Arnaud!
See https://issues.apache.org/jira/browse/HBASE-5151?focusedCommentId=13412056&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13412056 for one reason we need versioning in patch filenames.
When versioning is in play the above regex wouldn't pick up all the patches.
Does this belong to http://issues.apache.org/jira/browse/DAYTRADER too?
Committed revision 1058592.  thanks uwe for the review
I committed the patch.
Ilene Seeleman has agreed to review patch.
Hi Areek yes I'm reviewing your patch.
It is just taking more time than I thought it would.
Committed revision 901901 to trunk.
Fixed in LUNI module at repo revision r598681.
Please check that this resolves the issue for you.
belongs to cactus project?
+1 Committed.
Applied combined patch from Shiva and BJ at r692233.
I've just committed this to trunk and branch-2.
Looks good.
+1  Looks good to me.
As far as I understand the specification [/jcr:system/jcr:versionStorage/17/66/ea/1766eaef-f0f5-4cf6-95ef-a1d7290257f9] is not a legal name:  http://www.day.com/specs/jcr/2.0/6_Query.html#6.7.4 Name  Name ::= '[' quotedName ']' | '[' simpleName ']' | simpleName ::= /* A JCR Name that is also a legal SQL identifier 10 */ 10 See the SQL:92 rules for <regular identifier> (in ISO/IEC 9075:1992 ââ5.2 <token> and <separator>).
Too bad :( What ff we could check C-heap lock state before forking and possibly switch to an alternative way?
Because of this and HADOOP-4972 it may makes sense to acquire namesystem locks in getBlockLocationsInternal.
However it may leads to significant performance degradation
Typo:  surfix -> suffix
Who knows what evil lurks in the heart of Solr?
The shadow maven build knows...
Personally I'm more concerned with (1) than (2).
Was the fix for one issue more responsible for the performance loss than the other?
I'm no pom expert but it seems that unqualified pom variables can cause this problem.
Attached is a patch to support those variables.
correct build status capture echo's exit code
Paulex the original patch seems to be out-of-date.
Thanks
I tend to doubt there's anything we can do about this if the problem is at a lower level.
This file demonstrates the problem described in this bug.
The expected behavior would be to see the following messages on stdout.
This script renames the files like follows:  cd modules/lang-management/src/test/impl/java/org/apache/harmony/lang/management svn move DynamicMXBeanImplTest.java DynamicMXBeanImplTestBase.java svn move SingleInstanceDynamicMXBeanImplTest.java SingleInstanceDynamicMXBeanImplTestBase.java svn move MultiInstanceDynamicMXBeanImplTest.java MultiInstanceDynamicMXBeanImplTestBase.java  Run it please from classlib base directory.
I respectfully disagree with your interpretation even though in most of the cases it corresponds to what the majority of users may expect to see (including myself).
This said HttpClient is a general purpose HTTP agent and as such should follow the letter of the specification rather than make informed guesses.
We are not supposed to invent stuff.
Nowhere in the spec 'circular redirects' are mentioned.
The spec states that an HTTP agent should avoid infinite loops which is not the same thing as circular redirects in my opinion.
I saw this recently when using a custom built ActiveMQ (5.0.0.9-fuse) with ServiceMix (3.2.1).
I think the language in snowball must be English not en.
I'll have a look at mime4j and try to use it in Tika
Your patch was applied at 553723.
Please verify.
Looks fine thanks.
That was a pain!
Thanks!
svn rev 503448
Do you want me to tweak the patch to rename the alternate body header tag andif  move it to the MailConstants?
If you have any other things you don't like about it let me know and I can work on it tomorrow.
Cancelling patch until we get a consensus on Carl's comment.
Makes sense to me.
We have equalsIgnoreCase so having that ability for EqualsBuilder (and therefore HashCodeBuilder) seems apt.
Setting fix version for 3.0 - ie) won't be looking at applying this until after 2.3 gets released.
Sending to IRB.
On 2011-10-26 14:27:36.512 chiedozi commented: This would be a very simple change to make.
I would just need to retrieve the backgroundAlpha style before calling drawRect in its updateDisplayList.
The problem is that we use this draw call to also draw the thin divider.
On 2011-10-26 15:16:22.394 jsaracin commented: IRB -- opening to Chiedo priority B Ultra.
The patch is fine.
I'm almost done w/ a conversion.
[~ikelly] are you trying the right patch?
the new patch already put the #resolveConflict after checking ensemble.
oh I looked closely to see if you flipped args or did a compare w/ a presumption that hadn't been testing a statement or two earlier and it all looks right to me.
I had a quick look at the ab patch and nothing jumped at me.
Two small nits though:    * the count variable in requireAtLeastOneProperty() is never used    * couldntSetProperty() reinvents Property.toString() (or actually Hashtable.toString() which is inherited by Property)
Please check that the patch was applied as you expected.
+1 Looks good  I just committed this.
I addressed all of them in a new version of the patch.
do we really need this?
the current command is modeled after the export shell command.
export x=y sets the variable.
export with no args prints all the current settings.
we are missing an equivalent of export -n (to unset stuff).
if this is not a good model - then we should switch it with something different altogether.
Well 4762 is going nowhere fast should we push this to 2.1 as well?
This should have been fixed already.
EventListeners and CacheLoaders are documented.
Only peerProviders and some tutorial left to do.
XW-172 already talked about number conversion and localization.
Sorry I didn't see that before creating this issue.
thanks to Christian Kaltepoth for provide this patch
Hi did you check the snapshot?
+ http://tomee.apache.org/datasource-config.html explains that IgnoreDefaultValues can be used to avoid it
fixed in revision 413841 Boris please check that it was applied as expected
At the very least the exception could explain what the problem is rather than what the state of an obscure private variable is.
On 2008-02-25 15:37:45.766 jchuang commented: This is an AIR issue the AIR team has been notified.
On 2010-03-01 16:52:26.625 jchuang commented: Correctly resolved external
Thanks for updating the dependency.
Checked in the patch (see revision 1295288).
+1 thanks Henry Committed revision 779716.
I don't this its specific to HyperV as the mgmt server has to take care of reservation of IP address.
Can you verify with other hypervsisors and update .
Do more modern version of the Flash player work 11.3- 11.6?
i agree i think there is no wsdl into jms sample component.
one is could be provided isn't it?
Hmmm... it's not collisions then it was worth a try.
I still find the difference puzzling -- I can't justify your version being 3x faster.
But we know a lot about docids and extra hashing should just lead to an average-case slowdown.
Ok.
Hi Jamie  I can not access the mirror get a 404 have you moved/removed it do you still want it adding ?
Thanks.
blindly changing the oak security provider to   private SecurityProvider securityProvider = new SecurityProviderImpl()  gives Tests run: 706 Failures: 0 Errors: 122 Skipped: 13
Thx for your investigations Chet.
Have you got a link regarding the classpath exception and ASL2 compatibliity?
For the authentication issue it's recorded in JAMES-1418 (I will try to look at it this weekend).
Thank you.
So I added a comment out on UnInvertedField.
For the second issue: Consider long tailed distribution as shown on http://en.wikipedia.org/wiki/Long_tail  (In case of network traffic.
The biggest 90% data volume comes comes from less then 10% of connections.)
In this case we have extremely wide spread __important__ values with only ___single_ occurrences.
If we want to generate similar variable (for ex.
larger sample) we'll get fixed values for all this bins in the long tail.
It signifies 10% of generated values will be __fixed_ values - their respective bin means!
Last Issue: I would question usage of Gaussian Kernel at all.
Without having a mathematical prove I nevertheless suppose it could disturb parameters of generation if we have non Gaussian empirical data.
(for ex.
Pareto Tweedie ..)  Why we don't stick with triangular or uniform distribution as default for Kernel within the bean?
The apache vhost has been setup but still needs some work.
Right now it will proxy to port 80 of the httpd instance running on the current VM.
Is this sufficient?
Does nexus check for https connections for anything?
Does it care at all?
[~sershe] please bear with me - (1) could you please upload a rebased patch.
The patch is horribly out-of-date.
(2) HDFS-4979 introduced RetryCache in the NameNode.
Not sure if it is reasonable - is it possible to use the classes from HDFS-4979 here.
Committed to branch.
An email snafooey caused us to update our JIRA instance to 6.1 before we had a chance to test the import on our test instance (it didn't work properly for those who tried to whack at it).
We have a guy looking into getting it in shape now but it will require you to update your current JIRA instance to 6.1 before doing another export for us.
I'm truly sorry for this inconvenience but JIRA is really strict about the versions being equal when doing an import.
So good news is we finally have a chance of testing the exports very soon bad news is it will require an update on your side to be able to transfer the spark JIRA data.
I hope you can figure out how to upgrade and if you have any questions or need help feel free to reach out to us.
Soundararajan Ning - Yes I am planning on working on it starting next week.
I expect this to take at least upto mid to late in the week in order to get a patch available for this.
However if that schedule does not work for you please feel free to take this issue into your queue and go ahead.
Alexei this patch is quite out of date (sorry about that).
Please can you provide an updated patch?
Thanks Tim
If regionnames were made of tablename+endrow instead of tablename+startrow then in the metatables doing a search for the region that contains the wanted row we'd just have to open a scanner using passed row and the first row found by the scan would be that of the region we need (If offlined parent we'd have to scan to the next row).
If we redid the meta tables in this format we'd be using an access that is natural to hbase a scan as opposed to the perverse expensive getClosestRowBefore we currently have that has to walk backward in meta finding a containing region.
This issue is about changing the way we name regions.
If we were using scans prewarming client cache would be near costless (as opposed to what we'll currently have to do which is first a getClosestRowBefore and then a scan from the closestrowbefore forward).
Converting to the new method we'd have to run a migration on startup changing the content in meta.
Up to this the randomid component of a region name has been the timestamp of region creation.
"HBASE-2531 ""32-bit encoding of regionnames waaaaaaayyyyy too susceptible to hash clashes"" proposes changing the randomid so that it contains actual name of the directory in the filesystem that hosts the region."
If we had this in place I think it would help with the migration to this new way of doing the meta because as is the region name in fs is a hash of regionname... changing the format of the regionname would mean we generate a different hash... so we'd need hbase-2531 to be in place before we could do this change.
The two machines do not share the filesystem.
There is an embedded Tomcat running in the same JVM but according to the logs the database is not booted twice.
The only reasons for the failure I can think of were the failed network connections the OutOfMemoryError or the 4000+ simultaneous connections together with lock timeouts.
Ah and I am running 10.2 SVN 540841 as this fixes DERBY-2549.
Hmmm..
Dojo [0] and Ext.js [1] take both a different approach (which i personally don't really like).
just wanted to mention it as it's not about my personal preference but rather what is considered most useful for everybody.
[0] http://dojotoolkit.org/~dylan/dojo/tests/widget/demo_DatePicker.html [1] http://extjs.com/deploy/ext/examples/form/dynamic.html (3rd form)
Aaron: I completely agree the parser needs to be changed but I don't currently have the time available.
My goal here is a very small/quick change to revert how every FsShell command (except count) to how it used to work.
If you're ok with that I think another jira should switch the parser.
Todd: I suppose it comes down to whether hadoop should favor posix or gnu extensions?
can we apply the jarjar solution only to commons-csv?
I tested this again on Ubuntu 10.04 This works with the new Tomcat instance and then when I tried the 2nd time it is not working.
But when i delete the file which was created at <tomcat-home>/bin/search-index/write.lock  it works fine  but I'm not sure whether this creates other implications?
Further I think the tags are not saved any ware because after redeploying I found the tags I added are missing.
Is this due to the deletion of write.lock
From the logs it is clear that the reducers could not fetch the output for map attempt attempt_200905051023_1155_m_000036.
Both the reducers (and possibly everybody else) is waiting on these.
There is something mysterious going on here.
If attempt id 1 was speculative the framework would not have killed it as the original attempt was a failure.
If it was not was the attempt Id 1 killed explicitly?
In any case why did not the framework try and re execute this map somewhere else?
Could you let us know if speculative execution was turned on?
If by any chance you have the logs for the task attempts attempt_200905051023_1155_m_000036_0 and attempt_200905051023_1155_m_000036_1 could you attach them to this Jira?
Attaching the 10 findbugs reports for common and the various hadoop-tools projects and hadoop-auth.
Common has 100 or so warnings and there's a couple dozen spread out around the others.
Probably makes sense to do these like HDFS-4014 sensitive changes get their own patch and the general new classes of warning can be handled in bulk in a small set of patches.
I just committed this.
-1 overall.
Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12501038/MAPREDUCE-3209.patch   against trunk revision .
+1 @author.
The patch does not contain any @author tags.
-1 tests included.
Please justify why no new tests are needed for this patch.
Also please list what manual steps were performed to verify this patch.
+1 javadoc.
The patch passed contrib unit tests.
martin-g :i think do this in 6.0 and do 4327 in 1.5.x as its very easy to do 4327if you do 4327there will not be any need to mock AjaxRequestTarget but if you do do 4327 also add getters for prependscripts and appendscripts in ART )
I believe the notifications are now scheduled properly.
created: 2005-09-18 23:36:49.000 resolved: 2007-06-04 10:08:18.657 updated: 2011-05-10 16:51:30.000
I don't see a direct relation between this issue and the issue of simplifying the implementation of efficient map-side joins (MAPREDUCE-1183 more or less).
Am I missing the connection or is this a distinct issue?
File formats are forever.
More variations add significant long-term compatibility burdens to the project.
We badly need to add support for a higher-level object serialization system than Writable.
But I'm not convinced its wise to add such support to the exisiting Java-only container file formats.
So I'm all for a more generic serialization API that can be used by MapReduce applications.
I don't however see that it follows that we should provide implementations of file formats with a large number of different serialization systems as that invites multiplicative long-term support issues.
I'd prefer that we instead direct users towards a single preferred high-level serialization system and a single preferred container.
Historically that's been Writable and SequenceFile.
We now need to migrate from these to a more expressive language-independent serialization system and container file.
Our APIs should be of course be general enough that it's possible to incorporate different serialization systems and different file formats but we needn't provide implementations of all combinations of these but should rather direct folks towards a primary implementation.
Google benefits tremendously by having a single standard serialization system and container file format.
The Dremel paper (http://sergey.melnix.com/pub/melnik_VLDB10.pdf) argues that this is an essential enabler of their wide variety of interoperable systems.
The further we depart from this the harder we make it to build systems like Dremel that multiply the utility of stored data.
Changing serialization systems or file formats is a major imposition for many applications.
They cannot afford to do it frequently.
We should provide a clear path forward from Writable+SequenceFile to a new system that's easier to use less fragile and language-independent to better facilitate a rich ecosystem of tools.
Fixed TestCompositeService to not depend on test-order and thus made it pass on JDK7.
Contributed by Thomas Graves.
@Andy I have some thing which is similar to this and sorry if am diverting the current JIRA title.
May be i have the current PUT with me and from which i form the new PUTs to be added thro coprocessor hooks to a new region.
Assume the PUT(and related things) that i created in preBatchPut is also needed while i use the preWALWrite is there any way i can carry it in one flow?
Hi
Hope am not missing anything?
If you feel this can be done may be we can file an Improvement JIRA.
Please provide your suggestions
How about renaming leftResults as remainingResults ?
Please prepare patch for trunk.
Thanks
How about fixing WHIRR-337 before?
It should simplify things.
+1 in my mind the -ROOT- table is very important of routing the right RegionServer of .META.
The whole system could be out-of-service in case the Region of SequenceFile of the -ROOT- can NOT be opened.
Thus i quite agree that we put the .META.
directly into Zookeeper to reduce this.
We can avoid one case of failure.
Note: the attached patch does not break the existing API
Executing /stop on a running EC2 instance will terminate the instance.
There is a JIRA logged to request that 'stop' and 'terminate' be two different available actions ... see https://issues.apache.org/jira/browse/DTACLOUD-100 - also https://issues.apache.org/jira/browse/DTACLOUD-111
Fresh copy of the example configs?
{{PreTestTarget}} is a garbage I have missed out.
Removed.
The target {{inject-system-faults}} is defined in src/test/aop/build/aop.xml
What would you think of just exposing incRef() & decRef() as expert public methods?
Hi
We would just add ensureOpen() to those methods and make them public.
I don't agree to including the javadoc sources in the release.
The zip is already large and contains all code.
Using maven you can easily generate the docs yourself.
Or even better: use the maven repository to automatically attach the sources-jar to your eclipse/idea/netbeans project.
You can also download them by hand and attach them manually.
The comparison with 1.2.6 is not valid as we use a different setup now and ship everyting in one zip instead of a zip per project..
I send you a simple eclipse CXF webservice project using spring and a simple eclipse CXF client project.
With this client project i can't get connect with the webservice published in a Apache-tomcat-5.5.25.
The curious thing is that with a c#.net client i can connect with that webservice but not with a java generated client.
I don't know what i'm doing wrong!
Any help would be appreciated.
The client stubs were generated with wsdl2java command of CXF distribution.
I send you a list of all .jars at present i use in both projects there are many libraries but i test even with the minimal jars with the same results.
created: 2006-11-08 15:55:12.000 resolved: 2007-06-04 10:08:36.132 updated: 2011-05-10 16:12:23.000
Fixed in SVN now.
If it's never ok to pass null for the path (ie a bug in the code) then you can assert path != null or throw an AssertionError.
If it's OK sometimes and not others than IllegalArgumentException seems appropriate seems like it should never be legitimate to create a FileStatus w/o a path unless using the no argument constructor.
I learned much the issue progress of hadoop here.
And I think I will do more next time.
created: 2009-09-01 05:38:20.000 resolved: 2009-09-22 18:54:05.051 updated: 2011-04-15 15:06:27.000
Cleanup MR staging directory on completion.
Contributed by Mahadev Konar.
Created an attachment (id=10304) test.xml
FilterExprWalker.detach() calls  DescendantIterator.detach() and set DescendantIterator.m_traverser to null for $exceptions[1]/@name.
I would appreciate if any commiter review the patch.
I am going to repurpose HIVE-538 to clean up the implementation of the getSchema call.
Glad to see people are interested in this.
This should (and can) be automated.
Could you verify if there is still an issue with a 5.6 snapshot.
My suspicion is that it will be resolved.
Hadrian I wonder if we can add a _warning_ to the 2.0 release notes about the missing hash and sign files.
Nobody would download and use 2.0.0 release where there is 2.4.0 or better.
I would like to see this reopened.
As i wrote to the list it does not work for me.
Apache Karaf 2.1.1 is now available for download :)
I've checked the repository myself and everything seems fine to me.
I've pinged PMC group to have another eyes look around to be sure that everything is fine.
I'll ping here once this will be done.
Modified YarnRemoteException to be not backed by PB and introduced a separate SerializedException record.
Contributed by Siddharth Seth.
Updated MR App to reflect YarnRemoteException changes after YARN-634.
Contributed by Siddharth Seth.
Er sorry paste didn't work -- here's the full bug text:  =====  The following dependency relies on the sun.
* packages meaning that Xerces-J 2  is not 100% Pure Java (it depends on a specific aspect of specific versions of  *Sun* JVMs).
Found when attempting to compile Xalan-J 2 with gcj.
org/apache/xml/serialize/EncodingInfo.java:0: error: cannot find file for class  sun.io.CharToByteConverter
Fixed classpath for chukwa core jar file.
Fixed in 1.2 branch as well.
What is the right process to deprecate this behavior in 3.x?
Is it just a note in the CHANGES.txt?
We could also try to detect the behavior and log a warning... but i'm not sure that is worth much.
Changes appeared in 10.8.1 documentation so closing.
retrospectively setting fix version in preparation for SDO M3
Solution looks as follows: when doing a cluster sync whether it is for reading or writing always acquire the version manager's read lock first.
Patch adds javadoc for the newly added tests.
Right and right after that is the section I was referring to.
I think the licensing on that one might be complicated.
The source files appear to have various licenses including GPL.
Hi Andreas  as we are a bank we can't provide you with the normal documents as they are internal documents.
Therefore we will prepare a special document with a single checkbox will shows the error.
It will come eiither today or tomorrow.
The patch went in a few days ago:  http://svn.apache.org/viewvc?view=rev&revision=499521
Test code added as part of HDFS-2131
I've attached the output of dblook as db.sql and a simplified creation script as sample.sql that includes the relevant tables and columns with sample data.
I also verified that the bug still occurs in 10.6.1.0
Could you supply also a webapp sample?
It's easy look in the scratchpad/webapp/mount how it is done.
This could get many trying it and getting involved.
If I commit it as-is we could miss some help.
Patch to address the issue.
I have found the source of the problem - yet I completely don't understand why such an unrelated thing breaks navigation.
Probably it is way deeper problem.
As soon as I remove wicket enclosure or add some text node inside - the navigation toolbar starts properly counting pages (it was showing 0 of 0 before because it thinks it is not visible in hierarchy).
Thanks  Hari
This has been fixed by check in for CHUKWA-274.
This patch adds the RelativeTest.java file which is a conversion of the jdbcapi/testRelative test to the same package as the latter.
Also adds the tests to the _Suite class.
Please review and comment if anything is lacking or incorrect.
Bulk close for 3.1
Mailing Lists to be set up before this cab be done.
"I've modified the patch to name the function rep.normalize  If you think that my proposal is actually not enough ambitious for being called ""normalization"" do you prefer something like rep:toASCII which may seem less universal ?"
I really think there is an added value with this function for Jackrabbit users I personnally use it with every String-ordered query.
No - unfortunately I do not have enough knowledge to digg into OpenJPA's code
That is correct.
The build seems to have broken on 10.9 when the test was backported.
It only breaks if j14lib is not set.
The problem is that I changed the build script to use java14compile.classpath instead of java15compile.classpath for the new test when I backported it from 10.10.
I mistakenly thought 10.9 was supposed to work on Java 1.4 (in fact the 10.9.1 release notes say it is supported although we removed support for it after 10.8) and the build worked in my environment because I had j14lib set and the test ran just fine on 1.4.
I'll post a patch that makes 10.9 compile the test against 1.5 instead of 1.4.
It was attached to the wrong bug.
(back from holidays so a bit delayed but) I confirm Andrzej's suggestion -- a plain-text only summarized is ideal for clustering for example.
HTML is quite uncomfortable to work with.
Hi Marco  I changed the name of patch because it was made in Java earliar sorry for inconvenience.
Sorry for the mistake.
Not sure I want the transient inability to clean up some local log or other trivial task to kill a whole pig job?
First guess: something's getting confused about reversed-ness.
Sorry I took Martin's comment to mean the patch was committed and issue closed which certainly discarded all the patches the first time around.
I don't know what just happened.
The patch went to the wrong bug.
Sorry again!
Our caller may not notice what happend right now.
Sorry for my poor review didn't notice try/catch :(
It does not look very useful.
Oh I'm sorry I was unclear.
Sorry Jacques I'll upload another file.
I did it wrong this time.
"No idea about the issue resolution/if it's still active but I'm afraid that by no stretch of the imagination can it be termed a ""Critical"" issue."
Sorry I had attached an older version... let's try this one.
However I figured it was better to get it in the public domain sooner and let the iterative process do it's work.
This is only a problem with exceptions from java.
Incidentally if we all nag Joe Walnes enough we might be able to persuade him to release a new qdox which can ignore annotations etc (though it will still struggle with generics I think)
Sorry I kind of forget about this one.
I don't have strong opinions about it either way.
Bugs like this make me sad.
SnappyCodec check for Hadoop native lib is wrong.
Sorry for the delay I'm recreating the mirror now.
Brandon sorry.
I misread.
Ignore comment.
Seems to be a faulty mailserver setup at laguna-industries.com.
Unfortunately in these days where everyone that can put a CD the right way up in a drive is a Linux/Unix/Windows/Internet Expert this has become pretty common.
And with a network that tolerates almost everything to get a mail somehow in the right place it is only seldom noticed (or when it is it is blamed on Microsoft.
Mac looks like the tests are failing (especially TestHarFileSystem).
@Luke Sorry for the late attaching latest version patch
RMAppAttemptImpl.recoverAppAttemptTokens()  Looks like all changes in RMAppImpl are unnecessary.
Bug in existing testDelegationTokenRestoredOnRMrestart().
Sorry we are no longer using Xerces.
That is a bit too lax I think.
The cause is that the call to SpecificResponder.writeError in Responder.respond ultimately calls GenericData.resolveUnion which in turn calls GenericData.getSchemaName before the line where the UnresolvedUnionException gets thrown.
I am new to Mina and the whole environment.
I have this issue where my server and client is creating a lot of loopback threads that I believe must be related to this issue.
Any word on whether it will be fixed?
I did not have time to do it yesterday.
Sorry for being confusing.
I am not really sure what does the receive payment do before the shipment it doesn't sound as if it is doing what we expect it to do.
Incomplete example.
Unable to reproduce issue.
Oh ok.
And it will break ugly everything seems to work but data is never streamed to cassandra.
Sorry this affects 0.9.3 version too.
Phabricator is being a little buggy.
Tests no longer sleep
Sorry this should be YARN JIRA - filed YARN-350 instead
Sorry if this spam's things however it's unlikely that I'll work on these.
Forgot about these sorry.
Maurice I don't have such option or maybe I don't know where it is.
:) Sorry about that.
Sorry about the noise was trying out the precommit job.
Sorry about that...
Grumble grumble...
I was in too much of a hurry sorry hold on a sec.
Sorry I see that you say the same Mike :)
Sorry for the huge delay Fabio.
yup sorry just fixed.
I screwed up the encoding of the stopwords file (sorry).
So I also changed a test to test a non-ascii stopword :)
Sorry about the confusion before
Sorry.
Sorry folks the full fix was committed in rather a messy manner due to some platform specific issues.
Sorry didn't look at the code until commit...  Can you test making it hash to a Long or a 8-byte ByteBuffer?
Sorry - the above comes across as terse.
I really don't know how to solve this problem.
If you know of a way please describe it.
Sorry.
Ha sorry Varun thanks for the reminder I will have a look at it soon...
Sorry.
Sorry for the wild goose chase!
Long Life MyFacesssssssssss!
Looks good thanks Laura!
Thanks Ashish!
Thank you guys!
Thanks Ashutosh and Gunther for your help!
I really appreciate your comments and I really enjoy our discussions:)
Awesome work: this is a great first cut!
Thanks to Edward and Brock for the reviews.
Looks good.
Thank you Ivan!
:D
Thanks to both of you and to Deepesh for the initiative.
Cool.
Sounds like a good idea.
Cool that's all good news too! Thanks guys. One less on my radar.
cool man it looks good. we need a changes entry but from my side this looks good. we can tackle the todos on trunk
Looks pretty cool. Fresh and minimalist. Love it :-) Good work.
Cool will do!  Thanks for the review and good questions... and the whole idea! :)
This looks good to me.
Automatic location selection is very cool.
Thanks Tony.
Thanks for providing the patch.
Thank you!
Thanks Laura.
Thanks Kathey.
Thanks Adrien for looking into this nice explanation!
Super I'll commit shortly -- thanks Yonik!
Thanks a lot Jon for signing up this.
Thanks Sravya!
I like Richards update as well :) What I did was out of pure anger so it may not have been the sexiest.
Thanks [~cnauroth] & [~tomwhite] for the reviews!
Thanks for being patient Devaraj!
Hi Bruno  I like it seems to work well at 1st glance  Thanks!
Thanks Adrian!
Thanks Bob your patch is in trunk at r932317
thanks again Uwe
Best regards Andrew
Thanks Alexei!
Thanks Myrna.
Thanks Tom!
Thank you + Harit Himanshu
Thanks Arvind
Committed a slightly cleaned up version of the patch in revision: 1464605  Thanks for your help Todd.
897392 Thanks Sharan !
Thanks Karthik.
Thanks Andrew committed in rev.
thanks edgar for finding this one!
Thanks Ilya.
Thanks Jimmy.
Thank you Jing for the review.
Thanks Ravi!
Thanks Sandy.
Hello Leo  The patch has been applied at revision r479576 thanks a lot for this enhancement please verify that the problem is fully fixed as you expected.
Best regards Richard
Thank you Jing.
Thanks for your work on this Allen!
Thank you.
Forgot to mention:  Thanks to Alejandro and his team for pointing this one...
Thanks Karl.
Please contact me if you need any clarifications.
Thanks Tom!
thanks for the explanation.
Thanks Oliver that's fixed it.
Thanks a lot!
Adrian  thanks for the patch: rev.
thanks nandana
Thanks for review and test case Knut.
Thanks Suresh.
Thanks for your patience.
Hi Atul that looks good at a first glace.
Thank you Ravi.
+1 thanks Tom!
Thanks Joseph!
Thanks again for this really great patch Andrew.
Hi Edward  Thanks for spotting it.
Thanks Andrew - the patch was applied to SQL module at r525019.
Thanks for taking care of this Alejandro.
Fixed in svn revision 533916.  thanks dims
Thanks Oliver.
Thanks a lot for your help!
Cheers  Ashwin...
Thanks Martina.
Thanks Christian.
Thanks for taking a look Jiannan
@mahadev - I would love to help test a patch :) I'm currently using 3.3.1 + ZOOKEEPER-744 + ZOOKEEPER-790 applied in that order.
"Thanks Atul  Your patch is in trunk at r933169  I just added some ""mod for OFBiz layered lookups"" comments around changes as suggested Sascha"
Thanks for you work on this Jayachandra.
Thanks Areek new patch looks great I'll commit shortly.
Thanks a lot Kiran for the patch.
Thanks Mayank
Thank you Thejas for the patch and Ivan for the review.
Thanks Alejandro .
Thanks Mayank
I would love it in 3.3.2 will upload a patch for that version.
Thanks Mike!
Thanks Ravi.
Sweet! Git I love git! :)
Thanks Sijie.
Thank you for the patch Sergey.
I love a solution that's simpler and easier to understand than the original.
Hi Henry Thanks for checking it out.
In fact if you happen to know why ObserverHammerQuorumTest is failing with this latest patch I'd love to hear.
Thanks Jing!
That sounds like a plan. I love peer-reviews... :-)
Thanks for the very clear explanation of the needed change Dag.
Thanks for reminding me I agree I'll do it.
Thanks JinJie!
Thanks Brock!
Thanks Thomas!
Thanks Divesh  Your patch is commited at revision: 762863
Hi Sagara  Thank you very much for looking into this.
A pleasure :>.
The patch that broke the build on 10.9 was also backported to 10.8.
However on 10.8 it is correct to compile the test against 1.4 so that's not a problem.
Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12605661/MAPREDUCE-5538.6.patch   against trunk revision .
The patch does not contain any @author tags.
I still can't work out how to do this.
Igor suggested moving the call to Component#onBeforeRenderChildren() inside Component#onBeforeRender() but if I do that I get an NPE in the ListView#renderItem() method even for basic code.
"Hmm the suggested patch is not good for DRLVM which handles ""-version"" normally."
What is the point to print only launcher version after all?
The following had serialVersionUID set for the 2.2 release.
There have been no new ones added for 2.3.
So when 2.3 is released you shouldn't get the problems you've had from 2.1 to 2.2 for these files.
Committed patch with workaround in revisions 1054027 and 1054030
I attached our resolvers sadly we have no file system resolvers because our repository is hosted on a central server.
Only the cache is a local filesystem resolver.
"We are using this pattern ""${ivy.dir.pattern}/[revision]/[artifact]."
"[ext]"" so we should fullfil the requirements for the atomic operation."
Is it planned to add the atomic feature to all resolvers?
I did some more messing around and I was wrong about the FS stuff.
I deleted the local hbase directory and the problem went away.
Does this mean the migrate tool is busted for this version?
Problem ensuing from source files inconsistency
Patch with the fixes.
Committing shortly.
Applying both patches results in clean tests (jdbc4 tests and derbyall modulo wisconsin noise).
Yeah that one (TestAtomicOperation) has me worried a bit.
That one has not failed in a *very* long.
And new we've had two failures within a few days.
Patch integrated to trunk.
Closed after release.
Doc update to correct FQCN.
Simon I'm going to get deletes working tests passing using maps in the RT branch then we can integrate.
This'll probably be best.
"Jason I suggest you create a separate issue something like ""Integrate BytesRefHash in Realtime Branch"" and I will take care of it."
I think this issue had a clear target to factor out the hash table out of TermsHashPerField and we should close it.
lets use a new one to track the integration.
Now I can see this from the traces too.
We are re-using the prepared statement after table t1 is dropped.
Completed: At revision: 520469
I believe this will be done only when the package name has to be changed due to backward incompatibilities.
This is not new : we have the same type of behaviour when we switch languages...
Note: also change the use/effect of network connector dynamicOnly  - this flag had little effect but is now re-commissioned to indicate whether durable subs should be auto bridged (default) or whether they should be dynamic only.
There is no change to the default behavior.
Fixed long ago.
Deleting an entry now flushes the main page cache.
the link you point to is not our official javadoc  our official javadoc is in the maven repo.
if you want to view online there is a refreshed version here: wicketstuff.org/wicket13doc  for fragment see here  http://wicketstuff.org/wicket13doc/org/apache/wicket/markup/html/panel/Fragment.html
I would like to get some early comments on the changes I have made to the Namenode to support restarting within the same JVM.
There are still some debugging prints that will go away in the final submission.
Oops you're right that makes sense.
New patch that includes those changes.
In the future I could just get around the issue by using relative symlinks for those files (I think).
The patch does not contain any @author tags.
The patch doesn't appear to include any new or modified tests.
Please justify why no new tests are needed for this patch.
Also please list what manual steps were performed to verify this patch.
The javadoc tool did not generate any warning messages.
The applied patch does not increase the total number of javac compiler warnings.
The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.
The applied patch does not increase the total number of release audit warnings.
The mvn site goal succeeds with this patch.
{color:green}+1 core tests{color}.
The patch passed unit tests in .
Fxied and regression test written
These 3 crashes happen while compiling DocumentWriter.invertDocument.
It must be a bug in Sun's hotspot compiler.
Has anyone opened an issue at http://bugs.sun.com yet?
That method which inverts a single document was replaced with new code starting with 2.3 so it's possible you can workaround the bug by upgrading to 2.3 or 2.4.
But it'd still be nice to get the actual hotspot bug fixed.
"How about following the naming from Math.min() and Math.max(): public static Date min(Date date1 Date date2) public static Date max(Date date1 Date date2)  Alternatively when I read 'Returnes [sic] the least recent (oldest)"" why not call the methods oldest() and newest()"
Improve patch : when the flexibleString is resolved search fields on root context and parameters map.
Yes but even for Java 5 a relative path to a directory with flattened dependencies would be shorter wouldn't it?
Committed to 0.92 and trunk.
And compatibility test also ....
Murray that is exactly what I am proposing - having a separate org.apache.jspwiki.api -package and placing a number of existing manager classes into it as interfaces.
Or to be precise a mostly equivalent set of APIs JCR requires some modifications which would make it complicated to support the existing set of APIs - mainly because we need to carry state information in a WikiContext which the old API signatures do not mostly support.
I would also like us to start using WikiNames wherever useful.
The idea behind this is largely the same as with our existing package structure - there is nothing technical in preventing *everything* to be in the org.apache.jspwiki -namespace (with no subpackages) but both aesthetics and experience tell us that it just makes sense to add subpackages to make class management easier.
In the same vein isolating the API interfaces into a separate package makes it a lot easier to manage than having everything dispersed around - e.g.
if we want to create a jspwiki-api.jar we only need to jar up the contents of a particular subdirectory.
If the developer wants to know whether he is using any possibly unstable components all he needs to do is to glance at the set of import statements on the top - if he's using other classes than org.apache.jspwiki.api.
* he knows that the classes are potentially unstable.
In addition it makes the committer's life easier since there is an agreement by convention that the APIs inside *.api are not to be touched without common agreement whereas anything outside of this package can be treated with more liberties.
We have so many people contributing in the code that API incompatibilities can creep in without anyone noticing.
(There is no PageManager at all there is a ContentManager which will have a very similar API but it will manage both pages and attachments.
What's the purpose of this line in the patch to BaseTestCase Kristian?
Hi Julien please have a look on ./test/cpp/TestServer.cpp It's doing exactly what you what to do but TFramedTransport is explicitly specified there.
It looks like HLog main has support to invoke --split.
Does it looks like if I call that on the log that it will put split it and put the data into the right place?
We had a handful of regionservers go OOM yesterday while a MR job was doing heavy writes to a column family that doesn't usually get them.
In this case the first OOM occurred here during writing the checksum.
How about patch 2?
"I added a ""."""
to the end of the message.
So we are sure how to parse it no matter how hadoop/platform changes.
Hi Senaka  That is exactly what I'd like to do i.e. automatic download of CA cert and go ahead.
Is this possible?
Frank
Please have a look at the latest patch FELIX-3339_threaddump-as-inventoryprinter_FIXED.patch which correctly provides an InventoryPrinter implementation of Thread Dumper have a look at the attached Screen Shot 2013-05-23 at 3.02.24 PM.pdf to see it in action embedded in the WebConsole.
I hope you'll like it!
The patch looks good and the tests pass +1.
I agree there's a bug I just want to ensure any change still works correctly when CURRENT_USER name is passed in.
My assertion is that the username passed into these routines (without any modification) is equivalent to value of an username identifier in the SQL language after it has been parsed by the SQL engine.
"This is also known as ""how it is stored in the SQL engine"" [JDBC]."
Note that though how users names are represented in the derby.
* properties may or may not match how a SQL engine would store them.
For historical reasons in some (all?)
cases the values in a derby.
* property may be as a delimited identifier e.g.
"""eVe""."
Note that the Derby builtin users are not created through a SQL identifier in the SQL language e.g.
in a CREATE USER statement.
"They are created by setting properties and are thus not ""stored in the SQL engine""."
Does not appear to exist in trunk.
These blockers and critical issues are resolved but not verified.
Reporters of these issues please verify the fixes and help close these issues
I couldn't reproduce this.
Ted can you reproduce this outside of cassidy.pl?
Thanks for the response.
I like the idea of making sure that things that should be specified should be passed as method parameters in one go rather than through chaining which makes it a lot harder to see what is required.
So I took the liberty of updating the patch to use requiredX/optionalX methods like in your code.
Regarding Doug's points...  > But perhaps the preferred import should instead be 'import static SchemaBuilder.*'?
The static methods have unique-enough names that this might work well.
Sounds good.
> We can convert from Java object to JsonNode by parsing the output of GenericData.toString(Object).
Thanks.
I've updated the patch to do this but it still needs tests for this part.
I'm thinking of multiple concurrent mutateRows operation trying to lock the same set of rows.
Perhaps throwing IOException is going to prevent us from a situation where we end up with a deadlock.
But we still might want to sort it to ensure (better) progress (no livelock).
Unless I'm missing something your patch doesn't actually change how the version is checked only how it's computed.
Did you intend to combine our patches or is your patch incomplete?
[bulk update] Close all resolved issues that haven't been updated for more than one year.
- push out to 0.5
A workaround for this issue is to use entity resolver.
You may close this issue.
Also when do you plan to take up AMBARI-1843 for adding the unit tests?
We do not have to block this jira on the unit tests but we should probably try and get that in sooner than later.
Added patch to support configuration mapping
Wonky.
"Although as far as I see it the bug here is that we allow the underscore after ""_design/"" not that we can't use filters from it."
To me this is intersection.
See the below example  Employee1 Employee2 Employee3  Dept1.employess=Employee1 Dept1.employess=Employee3  Dept2.employess=Employee2  Intersection   Employee1 x Dept1.employess=Employee1 Employee3 x Dept1.employess=Employee3 Employee2 x Dept2.employess=Employee2  I don't like this sintax since it changes an obvious behaviour IMO.
I would prefer to write it as  SELECT intersect(employeethis.employees).manager.lastname FROM org.apache.jdo.tck.pc.company.Department VARIABLES Employee employee   Now it's up to up to clearify
JUnit test and Patch provided
Patch framework src/java and src/test to include JNDI adapters and unit tests.
> we have to do it for each call.
I meant for each newly added/deleted ones.
Here's another crack at this:  1) The default behavior for Derby is the current behavior with all of its security holes for java routines.
2) To get secure behavior for java routines the customer has to explicitly opt-in.
Let's be vague about what that entails right now.
3) If you do opt-in then you get the SQL standard behavior:     3a) Jar ids are mandatory.
Instead to access methods in the JRE you have to include little wrapper methods in your jar files that you loaded into the database.
3c) The search order for customer-written routines is SQL standard: First we look in the jar file where the routine lives.
Then we look in the other jar files in the order specified by SQLJ.ALTER_JAVA_PATH.
Then we defer to the system class loader.
3d) At runtime when we invoke the routine we make sure that it actually lives in the declared jar file.
MR's fix (similar to HDFS one).
Note also that an implementation goal is to not spawn threads to do writes to the underlying transport (like TFileTransport does now).
Sample implementation of CREATE DATABASE and DROP DATABASE for MySQL 5.0.2+.
Thanks.
In the wiki next to each one of these parameters it explicitly says that reducing this parameter will decrease memory usage this is why we reduced these parameters (it did not mention the filterCache at all).
They will save RAM to a certain extent for certain situations.
But not very helpful at the sizes you are working with (and not settings I would use to save RAM anyway unless the amount I need to save was pretty small).
Also the savings are largely index side - not likely a huge part of your RAM concerns which are search side.
My filterCache stats are great- you know it's set to 64K but right now with almost all the RAM used up (we're at 71.9% now) but it's only using 36290 entries at the moment and it's holding pretty steady there(even as RAM usage increased by 10%).
None of the other caches have gone up much either.
We have no cache evictions at all but a 99% hit ratio.
The sizes may be higher than you need then.
They should be adjusted to the best settings based on the wiki info.
I was originally suggesting you might sacrifice speed with the caches for RAM - but its always best to use the best settings and have the necessary RAM.
Just really wondering/hoping you'd be interested in working on these ??
I'd like to get them cleaned up because there are now so many that other more important and/or meaningful warnings are likely to be ignored.
Here's what got renamed:   .../{hadoop-hbase.postinst => hadoop.postinst}   .../deb/hbase/{hadoop-hbase-doc.dirs => hbase-doc.dirs} .../deb/hbase/{hadoop-hbase-doc.install => hbase-doc.install}  .../deb/hbase/{hadoop-hbase.dirs => hbase.dirs}      .../hbase/{hadoop-hbase.install => hbase.install}    .../{hadoop-hbase.manpages => hbase.manpages}        .../hbase/{hadoop-hbase.preinst => hbase.preinst}    .../hbase/SOURCES/{hadoop-hbase.sh => hbase.sh}      .../{hadoop-hbase.sh.suse => hbase.sh.suse}
HADOOP-5679 changed this function to return int instead of void.
Some amount of searching later...
I can't come up with a pure-CSS in-place change for this.
Another alternative might be to make a little icon + onhover that displays the non-comma'ed value and change the default display to comma'ed?
The icon may be unnecessary maybe just onhover on the text itself?
Commited a fix for this into trunk using revision 610846.
I will work on merging this into 10.3 codeline and writing a test case.
The tests ran fine on my Windows XP machine with Sun jdk1.4 The commit comments are as follows  DERBY-3302 The user was running into null pointer exception at the time of database recovery because Derby was trying to get the Collator object through database context.
But the  Collator object is already available in the territory sensitive character classes and we do not have to go to database context to get it.
I changed the code to use that collator  object rather than look into database context.
The reason for null pointer exception was  that database context was not loaded yet during database recovery.
I would like to close this issue.
Redirect handling has undergone significant changes since this issue was opened and we still need to take a hard look at redirects and possibly how scores are represented.
However the newer scoring and indexing frameworks do work around this issue.
Pushed to master.
I have changed the title and description of this JIRA to fit our current idea.
A slow hash algorithm makes offline cracking harder.
It would require changes to the API to have crypto only run on the server but it might be simpler.
Fixed the crash in this bug let's open a new one for the SPDY connection issues.
Could you attach a test case (Junit preferrably) that shows the unexpected behaviour?
This may only be related to the BlurIndexReader and may not be an issue in the BlurIndexNRT.
marking Fixed in 1.3  (I believe Ryan left this open to track any potential issues ...  if nothing else this way we'll remember to resolve it before releasing)
"I was thinking more of what happens on a Unix shell when you hit ""clear""."
With your patch the cursor might still be at the end of the grunt window after 14 new lines.
Claus I think I'm busy the next two weeks with [CAMEL-3468|https://issues.apache.org/jira/browse/CAMEL-3468] [CAMEL-3472|https://issues.apache.org/jira/browse/CAMEL-3472] [CAMEL-3471|https://issues.apache.org/jira/browse/CAMEL-3471] [CAMEL-3470|https://issues.apache.org/jira/browse/CAMEL-3470] and [CAMEL-3311|https://issues.apache.org/activemq/browse/CAMEL-3311].
Depending of the release date for Camel 2.6 and the priority of the other issues (may be I can reschedule [CAMEL-3471|https://issues.apache.org/jira/browse/CAMEL-3471] and/or other issues) I can do it in the remaining time.
Adds a new class of Exception.
Now TestBatchUpdate takes 50 sec instead of 250.
Please review.
Obviously I was testing artifacts:sources as well.
"buildr idea still produces the ""old"" format files (IDEA 6 maybe?)"
At some moments it could be upgraded to do IDEA 8 format but it's not important for now.
Is the bug really in NGramTokenFilter?
This seems to be a larger problem that would affect all tokenfilters that break larger tokens into smaller ones and recalculate offsets right?
For example: EdgeNGramTokenFilter ThaiWordFilter SmartChineseAnalyzer's WordTokenFilter etc?
I think WordDelimiterFilter has special code that might avoid the problem (line 352) so it might be ok.  Is there any better way we could solve this: for example maybe instead of the tokenizer calling correctOffset() it gets called somewhere else?
This seems to be what is causing the problem.
Uploading another patch for fixing the maven pattern to add the classifier.
I don't see the connection to .tmp files.
(Also: have you verified that the channel will actually infinite-loop returning 0?
Kind of odd behavior although I guess it's technically within-spec.)
IncomingStreamReader does clean the tmp file when there is an expection (there's an enclosing 'try catch').
The problem is that no exception is raised if the other side of the connection dies.
What will happen then is the read will infinitely read 0 bytes.
So this actually avoid the infinite loop returning 0 (and so I think answered your second question so it wasn't very clear).
Note that without this patch there is an infinite loop that will hold a socket open forever (and consume cpu though very few probably in that case).
So this is not just merely a fix of deleting the tmp files.
But it does as a consequence of correctly raising an exception when should be.
<joes4> lists will be available in 1 hour from now.
Andrew are you familiar with this code?
I've got a patch figured I'd have someone take a quick look-see before pushing it to Apache.
"Patch providing Comparable for ""api2"" project."
Patch does the following 1.
IntIdentity ShortIdentity LongIdentity CharIdentity ByteIdentity StringIdentity provide implementation of compareTo() allowing comparison with other identity *of the same type*.
2
ObjectIdentity disallows comparison
The insert_between.patch fixes the following:  1.
Changes the implementation of insertBetween in OperatorPlan.java so that the ordering of the predecessors is not changed  2.
Changed log.info in LOForeach getSchema() to log.debug.
One of my earlier patches had the log.info  3.
Removed a printPlan use in TestLogicalPlanBuilder.java  The unit tests that still fail are:      [junit] Running org.apache.pig.test.TestEvalPipeline     [junit] Tests run: 8 Failures: 0 Errors: 1 Time elapsed: 141.926 sec     [junit] Test org.apache.pig.test.TestEvalPipeline FAILED      [junit] Running org.apache.pig.test.TestFilterOpNumeric     [junit] Tests run: 8 Failures: 0 Errors: 1 Time elapsed: 56.446 sec     [junit] Test org.apache.pig.test.TestFilterOpNumeric FAILED      [junit] Running org.apache.pig.test.TestStoreOld     [junit] Tests run: 3 Failures: 0 Errors: 2 Time elapsed: 39.782 sec     [junit] Test org.apache.pig.test.TestStoreOld FAILED
xerces.apache.org has been added to DNS.
I am going to wait until it propagates before continuing.
ISOLatinAccentFilter.java again now with Unicode Latin Extended B as well.
Hi Sebb  In these cases 'this' and 'connectionPool' refer to the same object.
It just depends if the code is inside the connectionPool (uses this) or inside MTHCM (uses connectionPool).
Mike
Stopping SQL routines accessing the privileged blocks is through DERBY-2331 and DERBY-2330
If I see more occurrence of Lookup or search for find forms then shortly upload the patch for this fixes .
Can you just apply this patch file over top of the other one?
Uploading rebased patch for sathish
Maybe we should purge the Reader+queuing step and just have Handlers do the read (Nicolas Liochon what you think?)
I just committed this.
I knew they were missing and just forgot to do it before I committed it.
Cancelling current patch to upload new patch
To answer your questions in order: This problem only happens with the embedded driver because the Database is created inside the JVM and locked to prevent other processes to connect to it.
With the client you can connect to the same database many times.
I'll attach a zip with a small test project where you can reproduce the problem.
(all you need to add is your derby.jar)  I'm testing the DerbyTest from Eclipse.
I'm starting the 2nd version of DerbyTest from Eclipse as well.
When debugging it in Eclipse you can see the number of daemons grow very fast.
Still using a connection pool is pretty common.
Regards  Leon
Hey Uma I've elected not to revert the patch as-committed since it does indeed fix the bug at hand.
But in reviewing the patch I've come up with a number of ideas for how LeaseManager#changeLease and FSNamesystem#unprotectedChangeLease can be improved.
I've filed them in this JIRA: HDFS-2875.
Will try and do so within the next couple of days unless [~bikassaha] or [~sseth] beat me to it.
As KeywordTokenizer does the same thing I'll close this issue.
Some folks on the mailing list had something like this happening to them.
Added Maven Remote Resources Plugin version 1.0-alpha-5.
My assumption is that when you make a request to read 1k from a disk file that the OS reads substantially more than 1k from the disk and places it in the buffer cache.
(The cost of randomly reading 1k is nearly the same as randomly reading 100k--both are dominated by seek.)
So if you make another request to read 1k shortly thereafter you'll get it from the buffer cache and the incremental cost will be that of making a system call.
In general one should thus rely on the buffer cache and read-ahead and make input buffers only big enough so that system call overhead is insignificant.
An alternate strategy is to not trust the buffer cache and read-ahead but rather to make your buffers large enough so that transfer time dominates seeks.
This can require 1MB or larger buffers so isn't always practical.
So back to your statement a 1k buffer doesn't save physical i/o but nor should it incur extra physical i/o.
It does incur extra system calls but uses less memory which is a tradeoff.
Is that what you meant?
I will upload another patch (V5) in a couple minutes.
There are some changes about ServerName in the past few weeks so v5.patch changed this one-line {code}       return ServerName.valueOf(hostname 1234 1L) {code}
Patch uploaded.
Have passed all the local unit tests.
"//OdfDocument embedDoc = doc.getEmbeddedDocument(""Object 7"") //embedDoc.save(ResourceUtilities.newTestOutputFile(TEST_FILE_EMBEDDED_SAVE_OUT)) //embedDoc = OdfDocument.loadDocument(ResourceUtilities.getTestResourceAsStream(TEST_FILE_EMBEDDED_SAVE_OUT)) //Assert.assertEquals(embedDoc.getMediaType() OdfMediaType.TEXT.toString())  Svante"
If you want to work on it or assign it to someone else please do.
Otherwise I will try to look at it when I return to some JSF development at work later this month.
If there's a problem with items being in the cache when they shouldn't be adding a check around the Add method isn't a solution to that problem.
We should be Remove()ing items from the cache...not just overwriting them.
I applied an addendum for HBASE-3904 v6 to TRUNK.
NPE is fixed.
However HBASE-4087 is required for the new unit test to pass.
Sample client server and broker configuration.
"Watch out for ""Got InvalidDestinationException"" messages."
Kiran - I think this is a setup issue than anything else.
Hugo - if you couldnt reproduce it please close it
A quick and simple fix.
How to handle the problem with LUCENE_29 setting and the posIncr of stopwords together with QueryParser that has a default setting of ignoring posIncr?
How about adding required Version to QP ctor?
if I were to write a MyQuery class and implemen the toString method a certain way how would QueryParser know about MyQuery?
Is it possible to extend QueryParser?
"Hi  could you add the following line form.getDictionary().setItem(COSName.getPDFName(""NeedAppearances"")COSBoolean.TRUE)  right after PDAcroForm form = docCatalog.getAcroForm()  and try again?"
BR Maruan
Still about 2 hours worth of work left on this.
I changed the issue title because this issue includes the implementation of non-standard math functions.
Make scheduler heartbeat interval configurable (bikas)
+1 for making test behaves predictable.
Tests are meant to raise potential issues not to add variation of the issue.
I can see leaning toward to selects from the FLUSHER table with explicit checkpoints is a good balance for this case.
I am not aware of whether we have test to fill the page cache with new data evicting all pages currently in the cache so the row count changes are written at a known point in time.
And it will be a good test to have in turn of code coverage point of view.
This patch fixes bug for ia32.
Breakpoint handler frame is detected and processed specifically by unwinding algorithm.
Ran for almost an hour this time.
Dump looks similar to previous one.
Here's another whack at splitting the patch.
I addressed Shravan's request to add in the MRStreaming stuff into LocalLauncher and put in the jc.stop() calls after failed jobs in both Local and MapReduceLauncher.
...and it seems like having 2 APIs KV and Cell for every method other than those that take Cell-types params...
Which methods we talking about here?
There a list?
Ill go ahead and push it.
It is specific to what it is describing.
Couldn't the NodeState implementations in question (StoreNodeAsState AFAICS) just use their id to implement hashCode()?
<joes4> This is a bug in FreeBSD's ldap support- sorry there's nothing we can do about it.
In pom.xml of tajo root you forget to change the tajo version to 0.8 at Line 76.
Aaron  Go ahead.
Patch that adds the fix to WebResponse
Thanks for patch v2.
Some comments: 20.
Log.truncateTo(): The following code seems to be used just for getting the first segment.
22
ReplicaManager: 22.1 recordLeaderLogUpdate(): Could we rename it to recordLeaderLogEndOffset()?
22.2 close(): Could we rename it to shutdown to map startup()?
22.3 readCheckpointedHighWatermark(): We should just read the HW from memory.
The on-disk version is only useful on broker startup when we populate the in-memory HW using the on disk version.
23
"HighwaterMarkCheckpoint: Is it better to name the file "".highwaterMark"" so that it's hidden?"
I applied the patch at revision: 599008.
Still why do we hardcode these numbers e.g.
port/src/lil/em64t/pim/m2n_em64t_internal.h: #ifdef _WIN64 const unsigned m2n_sizeof_m2n_frame = 120 #else const unsigned m2n_sizeof_m2n_frame = 104 #endif  vmcore/src/util/em64t/base/compile_em64t.cpp: // Stack size should be (% 8 == 0) but shouldn't be (% 16 == 0) const int ALIGNMENT = 0  Doesn't it worth to let compiler calculate such values?
It would be good if folks can use standard Java i/o idioms with Hadoop.
http://www.ibm.com/developerworks/java/library/j-jtp03216.html  {noformat} OutputStream out = fs.open(...) try {   out.write(...) } finally {   out.close() } {noformat}  When multiple files are involved the best thing is to nest the try blocks.
Shouldn't we try to make this idiom work well with HDFS?
This looks good but I wish there was a good way to set up a test case.
I guess the best way would be to create a JobTracker and call the heartbeat method and observe the requested heartbeat interval.
I included the gen stamp and length in the {{cacheReport}} to handle caching newly appended data.
I guess the gen stamp is unnecessary but the DN isn't going to automatically mlock newly appended data so the NN needs to somehow realize that the cached length is shorter than the new length and ask the DN to recache at the new length.
Alternatively I guess the DN could automatically mlock appended data but there are quota implications there.
On startup I agree that we can skip cache reports until the cache is populated.
I also agree that jittering doesn't matter as much if it's ticking on such a short time scale.
I guess I could have cleaned this up rather than just changing the default cache report period like Colin asked.
However since we want to eventually have both incremental and full reports let's just ape how block reports work don't jitter the incremental reports but do jitter the start time for the full reports and afterwards tick at a regular interval.
Let's clean up all these issues in the incremental cache report JIRA (HDFS-5092) if this sounds good I'll edit the JIRA description with these todo items.
Uploaded a patch that fixes the comments and includes the test
Thanks for the review.
We used to do 'stopping'.
Smile.
Stopping' gets us closer to classic Lifecycle.
I think just needed for this at mo.
Because this neither-here-nor-there code-reuse seems not much useful.
The original reason why i asked this is so that clients can seamlessly cross over like in MR between RM and AHS.
But it seems like clients have to know which protocol they are talking to.
fully agree Christian!
I refereed to the [^v1-enable-wall-werror.patch] which contains {{-Wall -Werror}} and I thought it sould be {{-Wall -Wextra}}  However just add {{-Wall}}  to that patch makes absolutely sense and could be committed.
Hi Nicholas did you solve the issue?
Thank you for the patch Shawn can you also tell us what behavior you are seeing that is wrong?
Maybe you can paste a SOAP query example request and explain the wrong behavior and what it is you are expecting?
Commit cf6045f1aa37674f0225144ae0a1f662f955f153 in branch refs/heads/pvlan from [~jessicawang] [ https://git-wip-us.apache.org/repos/asf?p=cloudstack.gith=cf6045f ]  CLOUDSTACK-747: internalLb in VPC - UI - create network offering - system offering dropdown is for router only.
Change its variable name to be more intuitive.
you need to create system offering for system vms (ssvmcpvmrouter vm) and then you can upgrade to new system offerings it is only supported in vmware
Marking as dup of HBASE-5861 (You saw it first Andrew but Jon is going to work on this over in 5861..)
Patch applied Thank to Clay Leeds.
(BTW - The Idea to retrieve the Document Title is the best solution on my opinion.)
[~smarthi] When I apply this patch the source code cannot be compiled.
Another error is that the class Functions.NEGATE is misspelled as Function.NEGATE.
If you have the opportunity to do it go ahead.
Now again in the next trip of finalization it will throw NodeExistsException here it is really required to do the #store.
Patch looks good to me.
I'm fine w/ not naming it Consumer - I agree it does not really consume it.
But if we go with PayloadMergeProcessor we'll need PayloadMergeProcessorProvider and they become quite long names :).
I was thinking PayloadProcessor and PayloadProcessorProvider (have cool acronyms to PP and PPP) but then people might get confused that it processes all payloads (maybe before they are even written the first time) while it is actually invoked only during segment merges.I was following the *Consumer pattern I saw all over the place w/ SegmentMerger and thought that if someone ever reads SM code it will swallow easily another *Consumer one ...
So between PC PMP and PP - I prefer PP - the documentation should clarify what it does.
But I'm open for suggestions.
setting    set hive.script.auto.progress=true  seems to have no effect.
Abhinav can you also attach cloud_usage db dump.
Need it to test the fix.
I changed the ubuntu version too.
By the way getCanonicalPath() performs actual file access may take long if drive is slow (e. g. Samba) may fail with exception if path is inaccessible at the moment (e. g. on network drive) etc.
On the other hand getAbsolutePath() is purely logical and doesn't need file system access.
The change is now that in Lucene 4.0 all Analyzers are required to reuse TokenStream instances so the StopFilter is only produced only once in your application (when the Analyzer is created).
Only the first of these is a failure.
This test pukes all over the place when the first test fails.
Some of the other test cases in this class failed because that constraint error occurred?
There were most definitely 21 test case failures.
<danielsh> Yes any23.zones.apache.org is the jail's hostname so use the IP it resolves to.
ill upload a patch for 3.1 and 3.2 branch as soon as ZOOKEEPER-597 gets committed to those branches.
Yeah seems safe.
Will commit a bit later unless somebody beats me to it.
Hi Zeid  I'll go ahead and commit your changes but this one doesn't look right...I think it's a hold over  from the Mac files.
This symbol is not used for the Windows Netaccessor.
Yeah?
Please watch the source tree over the next several weeks before our next release.
There will  probably be some additional file checks so you'll need to patch the projects yet again.
Sorry db name is TPCDB user name TPC pwd whatever...
Sorry my code base is a little old looks like HADOOP-9147 add some new test in TestFileStatus.
I'm sorry.
Sorry I hit close by accident.
Sorry I've been MIA on this issue.
I haven't had any time to try re-upgrading after having to back out 2.0.8 but I will be sure to let you know how it works out as soon as I get the opportunity.
Sorry for that.
Sorry missed that select-box.
Sorry for letting you guys wait.
[~shazron]Sorry that was weird.
Missed 2 new files - sorry !
Sorry no longer working on this
Sorry patch is attached
On 2008-10-20 11:43:52.354 woflexair commented: Joan  sorry for the noise in the bugbase but it's not a bug.
Sorry once again for the wrong report Stephane
Sorry I forgot to change the priority of this one.
sorry.
Sorry that should have said fixed via HADOOP-4980 not 6980.
Sorry about the report.
Ugh sorry :(  Thanks!
Sorry about that.
Sorry I didn't use your patch but I think you'll like the end result.
Some file were missing in the last patch sorry.
The biggest problem is that we've had too many committers over the years and we'd have to get all of their permission to change it.
So sorry I can't be more obliging.
Last try.
Sorry.
sorry.. redeploy patch
Sorry for the trouble Vikram!
Sorry for the misspelling )
(sorry if that was confusing)
sorry for your time.
I must have looked at this before my morning coffee..
Sorry -) +1
Wow that was some bad copy/paste work.
sorry.
sorry....
Sorry about that...  http://bit.ly/12UiaS
Sorry Thiru I hope you haven't already started working on this!
huh ... i thought i did resolve this.
sorry.
Sorry misunderstood the issue!
Sorry for the delay.
sorry for delay
Sorry for the delay was away.
Sorry.
This was my bad.
Sorry it seems I forgot a few references to 2.0.2 in ./deps/cxf-all I'm attaching a new patch to fix these references.
Thanks Jukka for fixing this :) (sorry)
Stroooong ++++++1  I wanted to do that long time but some tests were made me afraid.
Sorry that should be  FileChannel inputChannel = input.getChannel() FileChannel outputChannel = output.getChannel()  in the try block.
Sorry [~svenkat] I just committed this.
No sorry.
Didn't got the time to try it yet.
Sorry to have not spotted this one and thanks for your care.
"Sorry I missed the ""overwrite"" keyword."
This might be a bug indeed.
Sorry everyone.
sorry last patch was missing new files
sorry for misleading attachment name.
Sorry Avdhesh I forgot to add these two files to the patch here is the new patch containing the missing files
Hey Vikram-- sorry about that I haven't paid much attention to this issue.
Sorry should ask first.
Thanks to Mathias Werlitz - sorry for the delay.
Sorry.
The problems I was talking about are mostly fixed by [HADOOP-3575] and [HADOOP-3480].
I opened [HADOOP-3607] to fix a wrong URL but appart from that I don't there's still references to the old structure.
no sorry - priorities have been shifted around...
Um sorry i thought this ticket was about something else - patch attached David would you give a try?
Sorry for the noise.
Sorry about that.
Sorry I missed some of this the first time.
Sorry  fixed.
Sorry Gav DONE Committed @revision 2487.
Sorry but look at mapSubtract and further down in the source you will find plenty.
Sorry -)
Thanks for the report and sorry its taken so long to fix it.
well sorry i didn't upload the right files
Thanks Tom.
I would love to have it right now for storm too. If you want me to sign up as a use case I am happy to.
Thanks for the patch Erik (and Jon)
Thanks for figuring this one out and providing the patch Jamie!
@Ashutosh: thanks a lot for the comments.
Thanks for the quick patching Donald!
Thanks Bilgin  it is working fine now.
Thanks henry and pat... we'll have to re submit all the PA's so trigger hudson.
Thanks senaka for the patch.
Thank you very much Richard.
Thanks for the patch Niall +1
thanks henry!
Taking it over  Thanks Mayank
:)  Best regards Fabian.
Thanks  Rishi Solanki  to review the patch.
Thanks Hari!
Best regards Richard
Hello George  The fix looks fine many thanks!
Best regards  Andrew
Thank you Brandon.
I really like what I am seeing so far.
This is quality work.
Thanks Dianne!
Thanks Elena.
Thanks Suresh
Thanks to both for these usability enhancements !
Thanks a lot for the reviews Todd.
Thanks Stepan.
Thanks Doug for the change.
Great investigations Doron!
Thanks for your patience.
Thanks Ivan.
Hi Sandy Thanks so much to give me such comments that's really helpful I will update this later.
ahhh sweet i'm guessing you have an IDE that does this? If the tests pass commit this kinda crap goes out of date too fast!
I want to integrate the sweet sweet logo Andrew crafted.
Thanks Oliver!
Thanks to Fabrizio for the patch.
Regex is your friend.
Thanks Awdesh - Done at r821748.
Thanks Richard  Patch applied to NIO_CHAR module java.nio.CharsetEncoder at repo revision 389784.
Thanks Tom
Thanks for your comments Konstantin.
Marcelo thanks for pointing out this it is now fixed in rev.
looks fine thank you.
Thanks for spotting this fixed at r487519
Thanks Areek patch looks good!
Hi James  This patch has been applied by chirino in the revision 576522  at Sep 17.
This patch really helped.
It will be great if you could confirm it either way first.
Thanks Todd glad to be on board!
Sweet I will review and apply.
Thanks Amar!
Thanks for sweet patch Erik.
Owen thanks for the slides.
I'm glad we were able to resolve this issue.
Megan Glad to be of service : )
Thanks Andi.
Hi Daniel thanks in advance for your support.
Thanks Hairong!
Thank you very much for your effort David.
Thanks in advance Jens
Aha thanks for the information. That makes sense. Glad to hear that it helped! :)
Thanks for looking into this Suresh.
Thanks a lot for sharing that Josh!
Thanks a lot Subin.
Brilliant feedback thanks! I'm glad you found the issue and the solution!
Sweet :) You got there before me
bq. First of welcome :) Thank you.
Thats sweet that you have the prior experience hacking this on top of a store already.
Excellent work Do??acan - thank you.
This looks simple and sweet to me.
Sweet kickstart for our new wiki!
Hello  Thanks a lot!  It works.  Lisa
Yes please - pull request away my friend!
I love this.
dc36d49eb3e240b3d8fd8b89ab178a7b1ec17d8b TS-1513: SPDY plugin crashes on connection close  Thanks for testing!
Thanks for looking into this Allan.
Thank you Babak!
I will be the sponsor for your beer the next time we meet.
But I'm probably not an JMX expert... -)
I tested a couple of projects with IDEA 7 and 8 and some more with IDEA 7 and everything is working.
but both IDEA 7 and IDEA 8 seem to handle them just fine.
Could you please port it to branch-1 that that we could integrate it to branch-1-win  Slavik thanks for the review!
Thanks Ashish For committing the patch.
Thanks Chandan Khandelwal
Thanks Aaron!
Please let me know if you need something else.
If you would like to take up this work that'd be great.
Otherwise hopefully someone else will improve this code when they've got some time.
[~lhofhansl] thank you so much.
Thanks Mark for taking care of this issue!
this class funny coincidence :)  What are you thoughts about QueryParser being able to know about custom Query implementations?
:)  Will have a look.
great idea those are lost to the ages but I'll try to reproduce.
However the goal of the test was very good.
Thanks Pat.
Patch looks good to me.
I like this title.
Thanks.
indeed good catch!
That's a good point we should make sure that it doesn't decrease performance for one of the database types even if it increases it for Oracle (in which case making it configurable in oozie-site.xml would be a good idea too).
(@ddlatham: thats kinda funny that you wrote the linked-to article -- it was a random google search)
This is the bridge code minus the redundant bit.
I would like that add that the problem is highly reproducible.
Looks great to me.
Thanks Doug.
These file describes my change:  derby1434-try2.diff (attached multiple times should have the same content) derby1434-try2.stat
And it's somehow connected to https://issues.apache.org/jira/browse/DIRAPI-137
"The reason of such big number of ""ByteArrayOutputStream.write"" invokes is signed ext/bcprov.jar - it's manifest is read by byte."
If we put this jar to bootclasspath we'll have at least 5% performance boost on HWA application .
btw: maybe we should add this Jira's as subtasks to HARMONY-5277 ?
As they're startup related.
Ok all done  https://issues.apache.org/jira/browse/KATO  I found sgoyal jira username to be shubham so hope thats right if so good to go!
comments on phabricator
I guessed UIMA_HOME/config but could be /lib
The only thing that the convention plugin does at request time happens in org.apache.struts2.convention.ConventionUnknownHandler the rest is just configuration loading on start up you might want to put a breakpoint in handleUnknownAction and see what is going on.
So either new-style hints are being written without versionColumn by RowMutation.hintFor or old style hints did not get cleaned out properly by SystemTable.purgeIncompatibleHints.
But both of those look fine to me.
Hi  I have changed the rsync to rsync.eu.apache.org.
Please check it now.
Regards Softaculous Team
> Stage 2 btw will involve documentation.
I'm not forgetting it :)   Did you forget it or did I miss it somewhere?
Is it possible to do this when closing the input/outputstream aswell?
Thanks Mathias.
Bringing into 0.90.0.
Marking as critical.
@Jon Thanks for committing it.
Anoop reminded me about this.
The failure of GdiplusStartup happened to because this function was called from a native method with RSP not aligned to 16 bytes.
It is not well documented in MSDN but it looks like RSP has to be always aligned to 16 bytes.
Some functions don't require it but some do.
A patch was made to the interpreter to fix this in assembly code so if a native method has an odd number of arguments additional 8 bytes are skilled by RSP to align it to 16 bytes.
Patch applied at 514264.
So I am likely going to work on this soon.
What is the exact problem?
If it would as simple as just recreating ReplicationZookeeper or using RecoverableZookeeper J-D would have probably just done it :)
You forgot to mention the part where I said that I have absolutely no idea if MinSize is important for some other part of the code )  Also it looks like if we add a check in couch_db:doc_flush_binaries/2 to see if we're not streaming an attachment of unknown length and then pass that information to couch_stream:ensure_buffer/2 so that couch_stream can decide if it wants to allocate exactly the requested amount or some extra it'd solve the issue.
The two issues mentioned by Tom has been fixed.
The new patch is the whirr-168-3.patch file.
32 bit ubuntu 12.10
Marking this for 0.18.4 and above since it's a deadlock.
The patch should work on all fair scheduler versions.
This blocker/ critcal was created before July please review and resolve we are approaching 4.2 code freeze in 7 days
Was fixed to behave as it used to.
Thanks for the patch.
It works for me both in container and in testing with WicketTester.
"As I said on that thread I'm in a compilers class right now and I thought I might make a more ""enlightened"" attempt using lex/yacc soon and see where that gets me if no one else gets this done first."
"anyway if someone is brave enough to fix the ""attempt one"" script i wish them well )"
I added a rm -rf /tmp/h* to hadoopqa... and ran this again.... seems to be working:  https://builds.apache.org/view/H-L/view/HBase/job/PreCommit-HBASE-Build/7253/console  We'll see.
by the way the only open impl of this algorithm i could find is at http://rrette.com/moman.html (ZSpell) in python.
I recently stumbled upon a C++ey STLey impl -> http://code.google.com/p/patl/  bq.
I might be on the trail of a java impl - get out the hounds!
If you do take hold of it do not hesitate to share :) The original paper and C++ code likewise melt my brain and I needed the algo in some other place.
Go ahead commit Nitay.
We can file issues with it as we find them.
Hi Deepa I looked at the transaction logs afaict this is not a bug rather the znode you mention is not deleted on session expiration because it was already explicitly deleted by another session.
I dumped the txnlog from zookeeper2 here is the section of interest.
afaict things are working properly at least on the ZK side of things.
Fixed a typo in the oritinal patch.
toString() method added.
FWIW: this issue is a regression of JCR-890 i.e.
the refactoring involved (r982520).
there might be more hidden issues like this.
+1 let's rip it out and redo it if we need it.
I wasn't talking about the perment schedule.
I was talking that the first three months of becoming a TLP we have to report monthly.
"Kevin we already have that -- the jobs are named after the file (if you run pig myfile.pig) but you can override the name with ""set job.name myname""  With this ticket that job name would essentially become a prefix."
I've saw this after completed the issue.
I've rework almost all needed messages in Preconditions.checkArgument.
And I did it according the style of each class.
I think it will be useful to standardize all the messages.
"Example for case: Preconditions.checkArgument(size >= 0 ""size must be at least 0"") we can do like Preconditions.checkArgument(size >= 0 ""Wrong size: "" + size + ""."
Marking this an incompatible change so that it gets attention.
Also setting webinterface.private.actions has other effects like enabling the 'kill job' and 'kill task' links on the Job web UI.
@Joy: I agree that not losing heartbeats is the best scenario.
In the case I mentioned in my above comment we would not lose datanode heartbeats if we implement HDFS-1392.
"On the other hand if there is a true network partition then NN will ""lose heartbeats"" from datanodes because the datanodes cannot send messages to the NN."
In that case the NN should delay creating a replication-storm in the hope that the network partition gets resolved soon.
So the heuristic that I listed above to detect a network partition should still be applicable isn't it?
Hi
Yep i am actually using just Pax Runner/Exam to start Ace stuff.. but i am not 100% sure about what the MuliFrameworkStarter is about.. what is it good for ?
Checked in with revision 388211.
After applied patch the output format just like this :  [root@infra1 bin]# ./hdfs dfs -count -header / DIR_COUNT FILE_COUNT CONTENT_SIZE FILE_NAME            7        10002           10292583 / [root@infra1 bin]# ./hdfs dfs -count -q -header / QUOTA REMAINING_QUATA SPACE_QUOTA REMAINING_SPACE_QUOTA DIR_COUNT FILE_COUNT CONTENT_SIZE FILE_NAME   2147483647      2147473638            none             inf            7        10002           10292583 /
The unit test code is for a sleep job where splits need not be different since they are empty.
"Here is his response:  ""There is no reason to change the version in the header as the version can be anything (ISO 19005-2:2005 6.1.2)."
As for the OutputIntent that is not necessary if all colors in the PDF are in a device independent colorspace (ISO 19005-1:2005 6.2.3.1).
In this case they are all in ICCBased Gray.
"Leonard""       BR  Bill Fausser"
That seems to work for me - thanks!
The preliminary patch is attached.
David  I've done a little digging and have a question around:  OrderReadHelper.getOrderItemQuantityBd()  Firstly going back a while now this log:     http://svn.ofbiz.org/viewcvs/trunk/components/order/src/org/ofbiz/order/order/OrderReadHelper.java?rev=4367&r1=4281&r2=4367 shows a change to the strings that seems to make the first if statement redundant?!
I presume either that statement can now be removed or the original intention of the statement needs reimplementing?
"scheduleAtFixedRate ""Creates and executes a periodic action that becomes enabled first after the given initial delay and subsequently with the given period that is executions will commence after initialDelay then initialDelay+period then initialDelay + 2 * period and so on."
"If any execution of the task encounters an exception subsequent executions are suppressed...""  so get() shouldn't be causing a cancel but if an exception is found then we need to re-schedule it manually."
(If you cast the Runnable in afterExecute to ScheduledFutureTask you can get access to the scheduling info.)
if get is causing the cancel even w/o any exceptions being involved then I guess you'll need to source dive in ScheduledThreadPoolExecutor to see what is going on.
Here are two thread dumps:  sling6.thread_dump.01.txt was taken about 10 seconds after a shutdown of Sling was attempted.
sling6.thread_dump.02.txt was taken a couple minutes later.
I include it because quite a few of the threads from the first dump have stopped by this point.
thx todd missed the ant/ivy stuff update patch takes care of it (hopefully mavenized gridmix goes in soon and ant/ivy will go away)
According to https://issues.apache.org/jira/browse/INFRA-6336 we can't use git for websites.
Thanks for the reference I'll look into it.
[~egli] - can you reproduce?
If you do you'd stand a better chance of making a proper fix...
I don't have the cycles today.
It would be great if you could work on it.
Thus assigning the issue to you.
Thanks.
"I have uploaded the modified patch ""PartyCommunication.patch"" and it is ready for testing."
This is the reverse side of ARIES 399 which dealt with a deadlock in ServiceRecipe.
As mentioned there to fix the issues blueprint code that calls out to any kind of client code (reference listeners service listeners initialiser methods) should not hold the full BlueprintRepository lock which allows deadlocks like the two observed.
Or at least the locks need to be much more fine-grained than locking the whole container.
I hope the first approach as the more general solution will work though :)
Yeah I had a hunch it was a shutdown hook kicking in at the same time.
i modified the test class from HADOOP-6148 (bigger is better) the result is:   ||bytes||PureJava MB/sec||Native MB/sec||Random PureJava MB/sec||Native MB/sec|| | 1
I fixed the build failures on 4.1 this morning.
I'm doing an automated test as well.
Will close this ticket if there are no issues.
FAILURE: Integrated in Hadoop-Yarn-trunk #344 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/344/]) HDFS-5041.
Add the time of last heartbeat to dead server Web UI.
The patch should be licensed to ASF for inclusion in ASF works.
Is that part of the test in question?
lifecycle management up to a separate container  Agreed and my patch also contains a TODO for that.
Similar to all other plugins the life cycle hook should be pluggable in a sense that you can deploy them e.g.
in an OSGi container as a separate bundle.
Instead of hardcoding the hooks we could introduce a LifeCycleHookProvider similar to ValidatorProvider we already have.
Subject.doAs tricks  Good point.
Though we have to review that usage in general.
I'll try the approach you suggested.
api_key and secret_key are not required.
data sync is external to cloudstack
Thank you Ole!
Committed v2 patch to trunk with revision 508383.
Committed to HDFS-3042 branch.
Thanks for the reviews Bikas and Aaron.
I said above that the earlier patches were lost but nope there they are.
"Anyway the ""-2"" patches include all the changes in the first patch files."
What platform?
Have you tried the standard maven untangling tricks like deleting ~/.m2?
We need to make this configurable before we can apply this patch to the trunk (unless i have misunderstood the effect).
The demo looks like it will be very useful.
Thanks Cyriaque.
Form#maxSize is ignored now too.
> Another thing: is the try ... catch good or bad?
I guess will be better not catch since until now was not being catched and will be good to propagate upwards.
v2 removes some dross included by misstack.
You could easily test if it is due to felix by switching to equinox if the effects are still there blame it on pax web or dosgi.
This jira is unassigned but contains a patch that needs to be reviewed for inclusion in 1.1.1.
Would a new configuration boolean 'fs.manual.shutdown' be adequate for your needs?
You could programatically set this before getting your filesystems and when set true the shutdown hook would not be added.
Vinay thanks for working on this.
Some comments:  The new method added to Namesystem is better to # pass BlockInfoUnderConstruction # call it as isInSnapshot and # do not throw IOException.
{code} //Namesystem.java public boolean isInSnapshot(BlockInfoUnderConstruction block) {code} In the implementation in FSNamesystem it should try-catch the UnresolvedLinkException and log it as an error since the full path obtained from a file should not have unresolved link.
Second question: Why adding DFSTestUtil.abortStream(..)?
I appended the information to the end of the file.
You have the ability to add users to the mina group in svn not to the group on people.a.o.
To grant new users commit access check out https://svn.apache.org/repos/asf/infrastructure/trunk/subversion/authorization and add the appropriate usernames to the mina= line in the [groups] section of the asf-authorization file.
shifted as agreed during the IRC release preparation meeting
thx to Cristi Toth  for his patch
Pull out the RopeTransport stuff fix the bug and add in the graceful degradation and I'm happy to see it pushed :)
I guess it's OK to put it in trunk.
I did not write it in a way that fits in with the rest of the implementations though.
No unit tests either yet.
* regular business exceptions work.
1
Everything timed out.
Resubmitting and making it a blocker for 0.22
I'm good with the last version of the patch of just adding a dclocal read repair chance.
Do you mind rebasing Vijay?
So I went ahead and made the change.
It's uploaded here and on review board.
Removed version-infomation znode.
More like can't fix.
Integrated in Hadoop-Mapreduce-0.23-Build #9 (See [https://builds.apache.org/job/Hadoop-Mapreduce-0.23-Build/9/])     HADOOP-7608.
Contributed by Alejandro Abdelnur.
@Matthew   If you can contribute a section on SPNEGO authentication to the HttpClient tutorial I'll commit your code to the official 4.1 branch of HttpClient  http://wiki.apache.org/HttpComponents/HttpClientTutorial  Oleg
"Note that I was seeing the dreaded ""Delta source ended unexpectedly"" error at around revision 733924 [1] so I had to set the mirror to start the mirrored version history from after that."
[1] https://svn.apache.org/viewvc?view=revision&revision=733924
Wouldn't it make sense if getFile() always checked whether the passed in fileName is an absolute file no matter of the protocol?
If this is the case this file must be returned.
Tom no problem please see WHIRR-634 and WHIRR-635.
Thanks Graham
populated see r2083
Here is the Review Board for local.py.
trivial change to make the minSize be 1 byte.
I think this should be fine for all of the cases I could find where we use this function.
If we need to specify a different minSize later we can add a {{setMinSize}} function.
The precommit failures are unrelated to the issue at hand.
[~thejas] could you please have a look at this patch?
Commit 7a977b5eaffb411cb3f1409336cdf42f6b46b18c in branch refs/heads/1.6.x from [~zack-s] [ https://git-wip-us.apache.org/repos/asf?p=incubator-jclouds.gith=7a977b5 ]  Reauthenticate on Keystone HTTP 401 (JCLOUDS-178)  The number of retries here is not the same as for 500 errors expected behavior is a quick fail while retaining some robustness.
commons.net 3.0.1 Thsi application is too big RMI is in Jconsole.I myself dont use.I just use ftp.listfiles.
here i have read that it is because of active passive mode.i will try maybe it can solve my problem but i afraid that it will produce problem with multithread application
Test case and fix.
Also adding some extra tests for newline splitting - currently this coverage is lacking.
Nothing out of the ordinary here:  % host -t mx laguna-industries.com laguna-industries.com mail is handled by 20 mail.phnx.uswest.net.
laguna-industries.com mail is handled by 10 mail.laguna-industries.com.
% host  mail.phnx.uswest.net.
mail.phnx.uswest.net has address 63.231.195.31 host -t ptr 31.195.231.63.in-addr.arpa.
31.195.231.63.in-addr.arpa domain name pointer mpls-pop.inet.qwest.net.
qmail chose the secondary MX (which is the mpls-pop.inet.qwest.net) probably because it could not reach the primary MX.
Nothing wrong everything is cool.
+1 great job pat.
+1 on intent from looking at what the patch fixes.
Haven't explicitly tested it myself.
getUserInSystemGraph usisng a priviledged block to access the system graph I don't think this should be done here as the result is no of you if the caller has no access to the system graph.
On 2008-03-06 08:00:48.178 bendalton commented: FLV video of the bug in question with Snitter On 2008-03-06 08:01:57.860 bendalton commented: This bug was first discovered in an application I'm working on.
I reproduced the bug using Snitter.
(ie this is an AIR bug not a Snitter bug)
yes they should all be using RWLock absolutely.
Where is the 'Grant license to ASF for inclusion in ASF works' for the patch
Thanks Willem.
I will update the docs (probably this weekend when I find some time)
Updated patch with new file.
Forget git add get sad...
The patch looks good to me.
Is there any particular reason on using an _ in front of the following variables?
{noformat} _harMetaCache {noformat}  Also this is meant for trunk only?
@Roger: huh this is bad as a partial integration will seg fault see BoostMonitor.cpp which does a cast of a boost::mutex.
It would be *highly* preferable instead to *not* include the BoostMonitor.cpp change as both are inter-dependent.
Alternatively I don't know what is the compiler error on Debian but it may very well be missing the include inside BoostMutex.cpp:  {code} #include <boost/thread/mutex.hpp> {code}  Thanks!
That's a very promising idea !
Will take a closer look.
Nice work [~stepinto] !
Can this ticket me marked as resolved as implementation done for portabl IP
Is the null check necessary?
the underlying protobuf handles the null properly.
{code}           ByteBuffer appAttemptTokens = attemptState.getAppAttemptTokens()           if(appAttemptTokens != null){             attemptStateData.setAppAttemptTokens(appAttemptTokens)           } {code}  New public method necessary?
The assert check should be made for rm1 and also for rm2.
{code}     // start new RM     MockRM rm2 = new TestSecurityMockRM(conf memStore)     rm2.start()      // verify tokens are properly populated back to DelegationTokenRenewer     Assert.assertEquals(tokenSet rm1.getRMContext()       .getDelegationTokenRenewer().getDelegationTokens()) {code}
try the commands  {code} git checkout cassandra-1.2 patch -p1 < 5234-3-1.2branch.txt  {code}
Fixed by making the url absolute before passing it to the web container.
Looks great Todd.
Could we keep this DataOutputBuffer for reuse?
{code} +      DataOutputBuffer buf = new DataOutputBuffer(size) {code}
If I'll come across Xerces again I'll try it.
Thanks Vijay.
"I can see why the test case just checks that the error string includes the exception text ""foo""."
"The problem is not that the exception text is lost completely it does come through just that it is wrapped in an AvroRuntimeException(""Unknown datum type: ""+datum) error."
"So instead of the expected ""java.lang.RuntimeException: foo"" text being sent you get ""org.apache.avro.AvroRuntimeException: Unknown datum type: java.lang.RuntimeException: foo""."
GenericData.getSchemaName instead throws an AvroRuntimeException so the special handling of UnresolvedUnionException in Responder.respond is skipped and the nested exception text is the result.
Another option is to create a new type for HdfsLocatedFileStatus as {code} HdfsLocatedFileStatusProto {   HdfsFileStatusProto fs = 1   LocatedBlocksProto locations = 2 } {code}
Anyway I am creating a project that began with 2.0.0-M3 and is now using 2.0.0-M4.
"that it's a huge patch is exactly why I don't want to take a ""let's just commit it and clean it up later"" approach."
Good idea especially if you have a use case for that.
You might also want to consider the induced latency though depending on the checkpoint size and frequency.
Make sure to use the hsync api for persistency guarantees.
Do you mean you checked your proposition or mine?
Mine is not thourougly thought it's just a suspenders AND belt solution.
"Also by ""working"" do you mean that you checked also for (undesired) side effects?"
> I don't see where this is limited to user-facing pages.
I need to list out all the jsp/servlet paths and check whether they are user facing or not.
I would like to show new API design first.
That's why I have posted my last patch.
Attaching patch file AVRO-957.patch:  We (Hitwise Pty Ltd) hereby assign all rights to the code contained within this patch over to the Apache Software Foundation (ASF) under the Apache Licence.
I worked with this on Danushka and the following fix worked for us.
We have done pretty intensive testing and believe this fixes the problem.
Here I am removing the subscription list impl and implementing it using ConcurrentLinkedQueue while keeping the interface intact as much as possible.
the java concurrent implementation of the queue.
QPID-3319.patch has the fix and QPID-3319.patch.2 and QPID-3319.patch.3 fix two other classes that cause a compilation error when the QPID-3319.path is applied.
Hi Carl I've made the changes JAVA_HOME and README.txt suggested by you.
I tested the JAVA_HOME change in Linux and it doesn't seem to work (unset JAVA_HOME and launch Eclipse for debugging).
Can you try it on Mac and see if it works?
Thanks Thomas committed to 3.4.0.
Another interesting message from the log  258766 (http-0.0.0.0-8443-Processor2) [                Log.java:103:WARN ] Create Payment Application: Amount to apply [36.76] is greater than the outstanding amount [0.0] of the invoice [10030].
Creating Payment Application for outstanding amount [0.0] instead.
There are a few considerations to be careful about: 1) The hostname in service could be a vip name.
The token selector is used in ipc.Client which has InetSocketAddress of the remote server.
How do we make sure we are matching the right hostnames?
One way to address it is to get the ip address from the hostname service and use that for matching but that needs a dns lookup.
2) Dns lookup in token selector would be invoked for every connection using token authentication.
Sounds good Claudio.
I used to have the subset of in-memory partitions maintained as a sliding window (i.e.
most recently computed K partitions) but I switched to the model with K-1 partitions always in memory and 1 slot for loading out-of-core partitions because it made the logic slightly simpler.
We can go back to something like that if we want to manage that subset more intelligently (2).
I would keep the hasActive field in memory (in the PartitionStore).
We start with any K in-memory partitions skip on-disk partitions that have no active vertices or incoming messages and prioritize spilling partitions that have no active vertices.
As I explained the XSL files have nothing to do with FOP so its impossible  for FOP to tell you which file/line number in XSL stylesheet(s) the problem  lies at.
FOP can only tell you which line in the single FO file the error  occurred at.
I'll focus my efforts on the compile targets then.
We'll have plenty of time to go through the bundles later.
gora-cassandra is patched to work with this prove of concept (It's just a one-liner in CassandraStore).
I meant if someone eventually uses CassandraClient directly this will break.
Yes I will open a new jira for this sometimes :)
Hei there
(HDFS-2468) - throw IllegalArgumentException if setOwner with both owner and group empty.
(HDFS-2438) - throw FileNotFoundException if getFileStatus on non-existing file.
(HDFS-2426) - fixed bugs in getBlockLocations.
"(HDFS-2508) - changed file checksum json response root from ""MD5MD5CRC32FileChecksum"" to ""FileChecksum""."
(Thanks Arpit for the suggestion.)
Seems to work reliably now.
I would prefer to get rid of the sleep again though.
Is it possible to signal from the ServiceTracker to the test that the servlet is installed?
I believe [~bikassaha] meant to resolve YARN-1068 and not this JIRA.
Working with Marek to diagnose the problem.
"v5 uses a small batch size and eagerly sends out ""incomplete"" batches if the reducer falls behind"
yep +1 to close.
I took care of this already.
Santiago and I  have removed the readExternal() writeExternal() methods (from interface Externizable) and instead rely on readObject() writeObject().
We still implement the Serializable interface.
We marked the TransformerFactoryImpl field in the TemplatesImpl class as transient to make sure it does not get serialized.
We overrode readObject(ObjectInputStream) so that we could  create a new TransformerFactory in the case of reading in a serialized translet.
"Here is a JAXP program that I wrote that shows how this fix was tested:  import javax.xml.transform.stream.StreamSource import javax.xml.transform.stream.StreamResult import javax.xml.transform.Transformer import javax.xml.transform.Templates import javax.xml.transform.TransformerFactory import java.io.FileOutputStream import java.io.ObjectOutput import java.io.ObjectOutputStream import java.io.FileInputStream import java.io.ObjectInput import java.io.ObjectInputStream import org.apache.xalan.xsltc.trax.TemplatesImpl  public class ProtoTemplates {    public static void main(String[] args){         ProtoTemplates app = new ProtoTemplates()         app.run(args)    }     public void run(String[] args){         if (args.length != 2) {             usage()         }         String inputFilename    = args[0]         String stylesheet       = args[1]          Transformer transformer         TransformerFactory factory = TransformerFactory.newInstance()          try {             FileOutputStream fout = new FileOutputStream(""MyTemplates.ser"")             ObjectOutput out = new ObjectOutputStream(fout)             Templates templates = factory.newTemplates(                 new StreamSource(stylesheet))             out.writeObject(templates)             out.flush()             out.close()              // try to use the serialized  templates object this will             // create a new Transformer Factory see TemplatesImpl.java             // readObject(...) method."
"FileInputStream fin  = new FileInputStream(""MyTemplates.ser"")             ObjectInput in = new ObjectInputStream(fin)             Templates templates2 = (Templates)in.readObject()             in.close()             transformer = templates2.newTransformer()             transformer.transform(new StreamSource(inputFilename)                                   new StreamResult(System.out))         }         catch (ClassCastException e) {             System.err.println(""CAST EXC: "" + e)             e.printStackTrace()         }         catch (Exception e) {             System.err.println(""ERROR: "" + e)             e.printStackTrace()         }         System.exit(0)    }    public void usage() {         System.err.println(             ""Usage: run <xml_file> <xsl_file>"")         System.exit(1)    } }"
This problen is there in the tests below as well   IHeaderBlockTest1 IHeaderBlockTest2 IHeaderBlockTest3 IHeaderBlockTest4 IHeaderBlockTest5 IHeaderBlockTest6 IHeaderBlockTest7 IHeaderBlockTest8 INamespaceTest1
Robin is fixing some other things w/ NB so he's going to take this.
ant tests passed on my box.
Integrated in Cassandra #446 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/446/])     use cfid instead of name in CBL.
patch by gdusbabek reviewed by jbellis.
Could we remove the setProperty() implementation in HierarchicalConfiguration and use the default implementation in AbstractConfiguration ?
Thus fetchInsertIterator could be made private.
"This will always work irrespectively of the combination of various underlying technologies and it will be a thread-safe solution :  public class GroupsResource {      private UriInfo uriInfo     private GroupResource gr1 = new GroupResource()      @Context     public void setUriInfo(UriInfo ui) {         uriInfo = ui         gr1.setUriInfo(uriInfo)      }      @Path(""group1"")     public GroupResource group1() {         return gr1     }      @Path(""group2"")     public GroupResource group2() {         GroupResource gr2 = new GroupResource()         gr2.setUriInfo(uriInfo)          return gr2     }  }  It will also work well in cases when users try to do proxy-based client invocations as client proxies do not support @Context parameters on resource methods."
In meantime I will see what I can do with respect to the context injection into subresources... Perhaps in some cases modifying the subresource classes with extra methods might not be an option.
that we have an overridable method is not by accident We want that so that we dont keep the state in the component.
But that state is only in the class not in the object
I'll upload example once again in 2 hours.
I've just seen that you attached a patch to this issue.
I'll get it right this time :)
I think it's a borderline class - architectually good for Lang but the Math guys are going to have a better idea about the complexities to be found.
Will nudge to see what people think.
> now this is really kewl )   Very 'kewl'.
I was somehow under the impression you were writing an Eclipse plugin.
The full list of commits is:  http://svn.apache.org/r1477236  http://svn.apache.org/r1477366 http://svn.apache.org/r1477698 http://svn.apache.org/r1477771 http://svn.apache.org/r1477784
16-byte BigInteger is overkill all we need is a reasonable distribution (now that Tokens don't need to be unique) and 64 or even 32 bits is plenty for that.
just uploaded.
MAPREDUCE-1813 has fixed the NPE problem.
They met the same bug with us.
You're right looks like this is already the desired behavior.
Did you forget to commit {{InvertibleRealLinearOperator}} ?
Here is the new file.
1
The background image is a 2000x2000 gif image with transparency and then the bitmap is very big.
2000*2000*4 = 16 Megas.
I change the way to draw the bacground with code like this one :
"It's not a major one but it would be a ""nice to have"" especially with the popularity of web services growing like it is."
Turns out that upgrading git (from 1.6.4 to 1.8.1) fixed this.
In the 1410e patch here are test cases that have not made into the bulkpostings branch.
I'll try and revive these first.
Uploaded new patch with no compilation errors.
Trove's licensing won't be changing.
Since the change is already included in HIVE-5741 I will mark this one resolved.
Redoing the before/after code below:  Before:        if (!msg.isPersistent() || connection.isUseAsyncSend() || txid!=null)) {             this.connection.asyncSendPacket(msg)         } else {             this.connection.syncSendPacket(msg)         }  After:       if(!msg.isResponseRequired() && (!msg.isPersistent() || connection.isUseAsyncSend() || txid!=null)) {             this.connection.asyncSendPacket(msg)         } else {             this.connection.syncSendPacket(msg)         }
Here is a url with a frame.
[~lhofhansl] Yea....
[~shwethags] Can you please fix this right away ?
Thanks Rashko  I have committed your patch in trunk revision 664697.
I'll take a closer look.
Killed it with fire.
Mike I can take this one if you want...
Thanks!
As expected thanks.
Whoops sorry.
I'll look into it.
Sorry about the trouble.
Committed Ron's patch.
Sorry in rushing to commit I forgot to add author during the commit.
Actually that's what this change did sorry for the noise.
Sorry for the noise..
Completely missed issue 614..
"Sorry typo:  ""our part"" in place of  ""one part"" above."
Sorry I thought issue (and provided patch) were about cocoon-servlet-service-impl used by either C2.2 and C3 but only now I see that instead they are related to cocoon-servlet-service-components (C2.2 only).
I'm sorry Mark  I came back here to remove my comment as after building the app from scratch I have found that by default it used  WhenExhaustedAction = GROW and not BLOCK!
Sorry  about that tiredness.
It works fine now!
Sorry about the error guys.
Yes sorry.
"It was meant to be ""super.initialize(context) line in its initialize(UimaContext)"" :)"
sorry this bug is *not* fixed.
The patch in FOP-1099 fixes it though.
Sorry.
I just saw that you commited Code that you did not write about here....
Resolving again....
Sorry yes I believe this has been resolved.
The problem here was simply that an EJBs class may not be one of its BeanTypes.
Just picking a type from one of the types was a simple enough solution.
Sorry for the spam - but which test method - can't find any related to the SnapPuller on trunk.
But may is better to have no test method than the one with 10 aspects.
[~ashutoshc] Yeah you are right.
Sorry about that.
[~appodictic] Sounds good and thanks!
Yes the patch is badly named - sorry!
Sorry attached the file to the wrong issue.
Sorry about that.
I have a really bad habit of not reading thru all the bug comments before asking questions.
Sorry I tought I could commit this patch but I did not realize that one of the files is in the framework (and I have not access to it).
Sorry on holiday last week and just catching up.
Thanks so much sorry that this isn't in the appropriate place.
Sorry guys I've been pretty busy @ work but I have been watching this.
I'll try to test out the patch this weekend and let you know how it goes.
Yes just made the same observation.
Sorry for the confusion :)
Sorry https://bugzilla.mozilla.org/show_bug.cgi?id=514760 is related but not same.
SorryI haven't tried you patch.
Sorry for the noise.
I will try to do more research on it.
Thanks.
Sorry I meant to have this patch in sooner but got quite busy I expect to have it soon.
Oh sorry.
I missed the context.
Patch in gzipped form.
For some reason I couldn't upload the straight patch.
Sorry guys
Oh signed shift!
of course!
sorry :)
Sorry - the federated build is still working out kinks...
The problem is that it builds awt and that requires some dependencies.
Please read the README.txt in   depends/libs/build  It should work after that.
I'll add a README
This is is my todo list still but hasn't risen to the top.
if someone wants to work on this please go ahead I still plan to just no time yet.
Sorry.
Yeah we can do it in 0.8.1 which should happen fairly quickly after 0.8 but we are really trying to lock down 0.8 to critical fixes and this is more of a new feature (though obviously desirable).
Sorry for the hassle.
WOW....you are right...I'm deeply sorry to bother you guys (and I'm also a bit embarrassed).
Looks like there's a code freeze for 2.1.8 happening right about now but this still hasn't been committed.
Is there a chance to get it in?
Sorry to nag.
sorry should have been getDataModel not getDataTable.
Sorry I'm wrong about #2.
Brain fart.
No unit test sorry (but there's not much deltaQuery coverage anyway).
Here is a fixed patch.
Sorry for that.
Ah sorry I misunderstood you.
Fixed then :)
I think this was a dud.
Sorry for the noise.
Aaron sorry about this.
The guy on our team that was going to do this was swamped so I re-assigned this to you.
Sorry missed this.
Will take a look at it.
I didn't 'svn up' before I looked at the code :).
Sorry for the noise.
My patch wouldn't compile.
Here is addendum to fix (Sorry about build breakage)
Sorry you're right.
Line 1664 (sorry the 0.94 codeline)
Sorry - I just didn't read the issues carefull enough.
It's the ObjectMessage that could alternatively be sent as an XStream message right?
Sorry about that fixed now.
sorry about this.
i committed GIRAPH-90 with wrong reference in commit message.
My error.
Tried some more stuff and realized I was doing it wrong.
Sorry.
Sorry here's the test that reproduces the issue.
Sorry if this spam's things however it's unlikely that I'll work on these.
Verified this with trunk.
Sorry for the trouble.
"sorry old xml here is the used one:  <?xml version=""1.0"" encoding=""UTF-8""?> <root><value>?âÂ??â??âÂ¦ und ??????"
This has been riding in 0.94 for too long.
Sorry [~kumarr] let's just do this in 0.96.
1
Sorry for the super-slow uptake on this the new job is using up all of my limited brain resources.
Well it's me that didn't get the whole point now i got more sorry for the noise.
One question if you need to override the decorated service why you need it?
Thanks Willem.
Sorry I was trying to get to the wiki but it's been a busy week.
s/[~apurtell]/[~lhofhansl]/  Sorry guys.
Sorry grabbed the wrong ticket.
Switched to @GeneratedValue(strategy = GenerationType.TABLE) for all entities rave-portal was using GenerationType.SEQUENCE (incompatible with MySQL) while rave-shindig was using GenerationType.IDENTITY (incompatible with Oracle)
Eventually I decided to include just one patch file (instead of code and test) since it was simpler after all.
Please be sure to review the following: # Collector class and documentation.
# Methods deprecation.
# New TestTopDocsCollector as well as test cases in TestSort.
One underlying cause of this issue is QPID-4731
Sorry that I think I missed some discussion in the mailing list.
The implementation and test and BreakIterator is attached.
sync'ed to trunk
I saw this behavior in a stress test just now as well on tip of 20.
Since we know meta splitting doesn't work can we put in some code that allows META's one region to grow without bound regardless of its size?
"New patch removing ""preserve holes"" option from AnalyzingCompletionLookup: you can simply tell your StopFilter whether or not holes are meaningful."
Cheers  Mick
This code is generated.
Unless we switch to Antlr 3.0 ( and even though I'm not sure this would fix the problem) there is nothing we can do.
Supplemental patch committed.
I'm leaving this issue open for further discussion.
Committed to trunk.
3.x does not have a mechanism to pass state across core reloads and that's a change I'd rather leave to 4.x.
Might simply be best to track all directories and close them on shutdown.
Shazron  I am using alert to visually notify myself.
Inferring that it might make a difference based on your comment I tried using 'console.log()' and the unexpected behavior was no longer present.
Attached is the patch running derbyall right now.
Reviews appreciated.

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from django.conf import settings\n",
    "import cPickle as pickle\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append('c:\\dev\\opinion\\opinion\\python\\opinion')\n",
    "import utils.fileutils as fileutils\n",
    "import utils.metrics as metrics\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = r\"C:\\dev\\opinion\\papers\\sentiplus\"\n",
    "indir = os.path.join(rootdir, \"Hybrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentiplus.Hybrid.DSOSE as dso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSOSE Extension Using TfIdf Parameter Change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label = p. Precision = 0.784. Recall = 0.651. F1 = 0.712\n",
      "Label = o. Precision = 0.778. Recall = 0.840. F1 = 0.808\n",
      "Label = n. Precision = 0.685. Recall = 0.681. F1 = 0.683\n",
      "F1 Macro = 0.736. Micro = 0.760\n",
      "Macro Precision = 0.749. Recall = 0.724\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "algo = \"RF\"\n",
    "learnerCol = \"DSOSE\"\n",
    "dsose = dso.DSOSE(rootdir)\n",
    "dsose.trainTestSupervisedDetector(algoName=algo)\n",
    "dsose.consolidateResults(algo)\n",
    "dsose.computePerformanceOverallOfLearner(algo, learnerCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label = p. Precision = 0.812. Recall = 0.676. F1 = 0.738\n",
      "Label = o. Precision = 0.777. Recall = 0.889. F1 = 0.829\n",
      "Label = n. Precision = 0.774. Recall = 0.641. F1 = 0.701\n",
      "F1 Macro = 0.760. Micro = 0.784\n",
      "Macro Precision = 0.787. Recall = 0.735\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "algo = \"GBTSentiCR\"\n",
    "learnerCol = \"DSOSE\"\n",
    "dsose = dso.DSOSE(rootdir)\n",
    "dsose.computePerformanceOverallOfLearner(algo, learnerCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsose = dso.DSOSE(rootdir)\n",
    "algo = \"LogisticRegression\"\n",
    "dsose.trainTestSupervisedDetector(algoName=algo)\n",
    "dsose.consolidateResults(algo)\n",
    "learnerCol = \"DSOSE\"\n",
    "dsose.computePerformanceOverallOfLearner(algo, learnerCol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label = p. Precision = 0.796. Recall = 0.651. F1 = 0.716\n",
      "Label = o. Precision = 0.783. Recall = 0.842. F1 = 0.811\n",
      "Label = n. Precision = 0.682. Recall = 0.693. F1 = 0.688\n",
      "F1 Macro = 0.741. Micro = 0.763\n",
      "Macro Precision = 0.754. Recall = 0.729\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "# bigram\n",
    "dsose = dso.DSOSE(rootdir)\n",
    "algo = \"RF\"\n",
    "dsose.trainTestSupervisedDetector(algoName=algo)\n",
    "dsose.consolidateResults(algo)\n",
    "learnerCol = \"DSOSE\"\n",
    "dsose.computePerformanceOverallOfLearner(algo, learnerCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label = p. Precision = 0.777. Recall = 0.646. F1 = 0.706\n",
      "Label = o. Precision = 0.774. Recall = 0.875. F1 = 0.821\n",
      "Label = n. Precision = 0.744. Recall = 0.641. F1 = 0.689\n",
      "F1 Macro = 0.742. Micro = 0.769\n",
      "Macro Precision = 0.765. Recall = 0.721\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "dsose = dso.DSOSE(rootdir)\n",
    "algo = \"LogisticRegression\"\n",
    "dsose.trainTestSupervisedDetector(algoName=algo)\n",
    "dsose.consolidateResults(algo)\n",
    "learnerCol = \"DSOSE\"\n",
    "dsose.computePerformanceOverallOfLearner(algo, learnerCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label = p. Precision = 0.812. Recall = 0.676. F1 = 0.738\n",
      "Label = o. Precision = 0.779. Recall = 0.889. F1 = 0.830\n",
      "Label = n. Precision = 0.773. Recall = 0.645. F1 = 0.703\n",
      "F1 Macro = 0.761. Micro = 0.784\n",
      "Macro Precision = 0.788. Recall = 0.737\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "dsose = dso.DSOSE(rootdir)\n",
    "algo =  \"GBTSentiCR\"\n",
    "dsose.trainTestSupervisedDetector(algoName=algo)\n",
    "dsose.consolidateResults(algo)\n",
    "learnerCol = \"DSOSE\"\n",
    "dsose.computePerformanceOverallOfLearner(algo, learnerCol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File DatasetLinJIRA. Label = p. Precision = 0.95. Recall = 0.89. F1 = 0.92\n",
      "File DatasetLinJIRA. Label = n. Precision = 0.95. Recall = 0.98. F1 = 0.97\n",
      "File = DatasetLinJIRA. F1 Macro = 0.94. Micro = 0.95\n",
      "-------------------------------\n",
      "File BenchmarkUddinSO. Label = p. Precision = 0.44. Recall = 0.21. F1 = 0.28\n",
      "File BenchmarkUddinSO. Label = o. Precision = 0.65. Recall = 0.80. F1 = 0.72\n",
      "File BenchmarkUddinSO. Label = n. Precision = 0.41. Recall = 0.37. F1 = 0.39\n",
      "File = BenchmarkUddinSO. F1 Macro = 0.48. Micro = 0.59\n",
      "-------------------------------\n",
      "File DatasetLinAppReviews. Label = p. Precision = 0.82. Recall = 0.90. F1 = 0.86\n",
      "File DatasetLinAppReviews. Label = o. Precision = 0.14. Recall = 0.04. F1 = 0.06\n",
      "File DatasetLinAppReviews. Label = n. Precision = 0.82. Recall = 0.82. F1 = 0.82\n",
      "File = DatasetLinAppReviews. F1 Macro = 0.59. Micro = 0.81\n",
      "-------------------------------\n",
      "File DatasetLinSO. Label = p. Precision = 0.43. Recall = 0.31. F1 = 0.36\n",
      "File DatasetLinSO. Label = o. Precision = 0.85. Recall = 0.92. F1 = 0.89\n",
      "File DatasetLinSO. Label = n. Precision = 0.49. Recall = 0.33. F1 = 0.39\n",
      "File = DatasetLinSO. F1 Macro = 0.55. Micro = 0.80\n",
      "-------------------------------\n",
      "File DatasetSenti4SDSO. Label = p. Precision = 0.88. Recall = 0.89. F1 = 0.89\n",
      "File DatasetSenti4SDSO. Label = o. Precision = 0.79. Recall = 0.76. F1 = 0.77\n",
      "File DatasetSenti4SDSO. Label = n. Precision = 0.74. Recall = 0.77. F1 = 0.75\n",
      "File = DatasetSenti4SDSO. F1 Macro = 0.80. Micro = 0.81\n",
      "-------------------------------\n",
      "File OrtuJIRA. Label = p. Precision = 0.78. Recall = 0.67. F1 = 0.72\n",
      "File OrtuJIRA. Label = o. Precision = 0.85. Recall = 0.88. F1 = 0.87\n",
      "File OrtuJIRA. Label = n. Precision = 0.66. Recall = 0.70. F1 = 0.68\n",
      "File = OrtuJIRA. F1 Macro = 0.76. Micro = 0.81\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "algo = \"RF\"\n",
    "learnerCol = \"DSOSE\"\n",
    "dsose = dso.DSOSE(rootdir)\n",
    "dsose.computePerformancOfLearner(algo, learnerCol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsose = dso.DSOSE(rootdir)\n",
    "algo = \"RF\"\n",
    "dsose.consolidateResults(algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label = p. Precision = 0.785. Recall = 0.655. F1 = 0.714\n",
      "Label = o. Precision = 0.782. Recall = 0.837. F1 = 0.809\n",
      "Label = n. Precision = 0.672. Recall = 0.683. F1 = 0.677\n",
      "F1 Macro = 0.735. Micro = 0.759\n",
      "Macro Precision = 0.747. Recall = 0.725\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "algo = \"RF\"\n",
    "learnerCol = \"DSOSE\"\n",
    "dsose = dso.DSOSE(rootdir)\n",
    "dsose.computePerformanceOverallOfLearner(algo, learnerCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File DatasetLinJIRA. Label = p. Precision = 0.96. Recall = 0.87. F1 = 0.91\n",
      "File DatasetLinJIRA. Label = n. Precision = 0.94. Recall = 0.98. F1 = 0.96\n",
      "File = DatasetLinJIRA. F1 Macro = 0.94. Micro = 0.95\n",
      "-------------------------------\n",
      "File BenchmarkUddinSO. Label = p. Precision = 0.46. Recall = 0.23. F1 = 0.30\n",
      "File BenchmarkUddinSO. Label = o. Precision = 0.66. Recall = 0.80. F1 = 0.72\n",
      "File BenchmarkUddinSO. Label = n. Precision = 0.41. Recall = 0.40. F1 = 0.41\n",
      "File = BenchmarkUddinSO. F1 Macro = 0.49. Micro = 0.59\n",
      "-------------------------------\n",
      "File DatasetLinAppReviews. Label = p. Precision = 0.82. Recall = 0.89. F1 = 0.85\n",
      "File DatasetLinAppReviews. Label = o. Precision = 0.33. Recall = 0.08. F1 = 0.13\n",
      "File DatasetLinAppReviews. Label = n. Precision = 0.80. Recall = 0.82. F1 = 0.81\n",
      "File = DatasetLinAppReviews. F1 Macro = 0.62. Micro = 0.80\n",
      "-------------------------------\n",
      "File DatasetLinSO. Label = p. Precision = 0.42. Recall = 0.29. F1 = 0.34\n",
      "File DatasetLinSO. Label = o. Precision = 0.85. Recall = 0.92. F1 = 0.89\n",
      "File DatasetLinSO. Label = n. Precision = 0.54. Recall = 0.37. F1 = 0.44\n",
      "File = DatasetLinSO. F1 Macro = 0.56. Micro = 0.80\n",
      "-------------------------------\n",
      "File DatasetSenti4SDSO. Label = p. Precision = 0.89. Recall = 0.90. F1 = 0.89\n",
      "File DatasetSenti4SDSO. Label = o. Precision = 0.78. Recall = 0.76. F1 = 0.77\n",
      "File DatasetSenti4SDSO. Label = n. Precision = 0.72. Recall = 0.74. F1 = 0.73\n",
      "File = DatasetSenti4SDSO. F1 Macro = 0.80. Micro = 0.80\n",
      "-------------------------------\n",
      "File OrtuJIRA. Label = p. Precision = 0.78. Recall = 0.67. F1 = 0.72\n",
      "File OrtuJIRA. Label = o. Precision = 0.86. Recall = 0.87. F1 = 0.86\n",
      "File OrtuJIRA. Label = n. Precision = 0.64. Recall = 0.70. F1 = 0.67\n",
      "File = OrtuJIRA. F1 Macro = 0.75. Micro = 0.81\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "algo = \"RF\"\n",
    "learnerCol = \"DSOSE\"\n",
    "dsose = dso.DSOSE(rootdir)\n",
    "dsose.computePerformancOfLearner(algo, learnerCol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with text preprocessing but no handling of negations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsose = dso.DSOSE(rootdir)\n",
    "algo = \"RF\"\n",
    "dsose.consolidateResults(algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label = p. Precision = 0.792. Recall = 0.651. F1 = 0.714\n",
      "Label = o. Precision = 0.779. Recall = 0.849. F1 = 0.813\n",
      "Label = n. Precision = 0.688. Recall = 0.672. F1 = 0.680\n",
      "F1 Macro = 0.738. Micro = 0.763\n",
      "Macro Precision = 0.753. Recall = 0.724\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "algo = \"RF\"\n",
    "learnerCol = \"DSOSE\"\n",
    "dsose = dso.DSOSE(rootdir)\n",
    "dsose.computePerformanceOverallOfLearner(algo, learnerCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsose = dso.DSOSE(rootdir)\n",
    "algo = \"MLPC\"\n",
    "dsose.consolidateResults(algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label = p. Precision = 0.670. Recall = 0.676. F1 = 0.673\n",
      "Label = o. Precision = 0.778. Recall = 0.743. F1 = 0.760\n",
      "Label = n. Precision = 0.602. Recall = 0.663. F1 = 0.631\n",
      "F1 Macro = 0.689. Micro = 0.710\n",
      "Macro Precision = 0.683. Recall = 0.694\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "algo = \"MLPC\"\n",
    "learnerCol = \"DSOSE\"\n",
    "dsose = dso.DSOSE(rootdir)\n",
    "dsose.computePerformanceOverallOfLearner(algo, learnerCol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentiCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsose = dso.DSOSE(rootdir)\n",
    "algo = \"GBTSentiCR\"\n",
    "dsose.consolidateResults(algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label = p. Precision = 0.798. Recall = 0.669. F1 = 0.728\n",
      "Label = o. Precision = 0.764. Recall = 0.905. F1 = 0.829\n",
      "Label = n. Precision = 0.815. Recall = 0.587. F1 = 0.682\n",
      "F1 Macro = 0.755. Micro = 0.779\n",
      "Macro Precision = 0.792. Recall = 0.720\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "algo = \"GBTSentiCR\"\n",
    "learnerCol = \"DSOSE\"\n",
    "dsose = dso.DSOSE(rootdir)\n",
    "dsose.computePerformanceOverallOfLearner(algo, learnerCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File DatasetLinJIRA. Label = p. Precision = 0.99. Recall = 0.83. F1 = 0.90\n",
      "File DatasetLinJIRA. Label = n. Precision = 0.93. Recall = 1.00. F1 = 0.96\n",
      "File = DatasetLinJIRA. F1 Macro = 0.94. Micro = 0.94\n",
      "-------------------------------\n",
      "File BenchmarkUddinSO. Label = p. Precision = 0.49. Recall = 0.19. F1 = 0.27\n",
      "File BenchmarkUddinSO. Label = o. Precision = 0.64. Recall = 0.88. F1 = 0.74\n",
      "File BenchmarkUddinSO. Label = n. Precision = 0.45. Recall = 0.27. F1 = 0.34\n",
      "File = BenchmarkUddinSO. F1 Macro = 0.48. Micro = 0.60\n",
      "-------------------------------\n",
      "File DatasetLinAppReviews. Label = p. Precision = 0.84. Recall = 0.93. F1 = 0.88\n",
      "File DatasetLinAppReviews. Label = o. Precision = 0.18. Recall = 0.12. F1 = 0.14\n",
      "File DatasetLinAppReviews. Label = n. Precision = 0.87. Recall = 0.79. F1 = 0.83\n",
      "File = DatasetLinAppReviews. F1 Macro = 0.62. Micro = 0.82\n",
      "-------------------------------\n",
      "File DatasetLinSO. Label = p. Precision = 0.54. Recall = 0.40. F1 = 0.46\n",
      "File DatasetLinSO. Label = o. Precision = 0.86. Recall = 0.94. F1 = 0.90\n",
      "File DatasetLinSO. Label = n. Precision = 0.65. Recall = 0.37. F1 = 0.47\n",
      "File = DatasetLinSO. F1 Macro = 0.62. Micro = 0.83\n",
      "-------------------------------\n",
      "File DatasetSenti4SDSO. Label = p. Precision = 0.89. Recall = 0.91. F1 = 0.90\n",
      "File DatasetSenti4SDSO. Label = o. Precision = 0.79. Recall = 0.82. F1 = 0.80\n",
      "File DatasetSenti4SDSO. Label = n. Precision = 0.79. Recall = 0.74. F1 = 0.76\n",
      "File = DatasetSenti4SDSO. F1 Macro = 0.82. Micro = 0.83\n",
      "-------------------------------\n",
      "File OrtuJIRA. Label = p. Precision = 0.79. Recall = 0.77. F1 = 0.78\n",
      "File OrtuJIRA. Label = o. Precision = 0.87. Recall = 0.92. F1 = 0.89\n",
      "File OrtuJIRA. Label = n. Precision = 0.82. Recall = 0.64. F1 = 0.72\n",
      "File = OrtuJIRA. F1 Macro = 0.80. Micro = 0.85\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "algo = \"GBTSentiCR\"\n",
    "learnerCol = \"DSOSE\"\n",
    "dsose = dso.DSOSE(rootdir)\n",
    "dsose.computePerformancOfLearner(algo, learnerCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label = p. Precision = 0.810. Recall = 0.677. F1 = 0.737\n",
      "Label = o. Precision = 0.789. Recall = 0.877. F1 = 0.831\n",
      "Label = n. Precision = 0.747. Recall = 0.676. F1 = 0.710\n",
      "F1 Macro = 0.762. Micro = 0.785\n",
      "Macro Precision = 0.782. Recall = 0.743\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "# DSOSE with p and n scores + dso label\n",
    "algo = \"GBTSentiCR\"\n",
    "learnerCol = \"DSOSE\"\n",
    "dsose = dso.DSOSE(rootdir)\n",
    "dsose.computePerformanceOverallOfLearner(algo, learnerCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File DatasetLinJIRA. Label = p. Precision = 0.99. Recall = 0.84. F1 = 0.91\n",
      "File DatasetLinJIRA. Label = n. Precision = 0.93. Recall = 1.00. F1 = 0.96\n",
      "File = DatasetLinJIRA. F1 Macro = 0.94. Micro = 0.95\n",
      "-------------------------------\n",
      "File BenchmarkUddinSO. Label = p. Precision = 0.50. Recall = 0.19. F1 = 0.28\n",
      "File BenchmarkUddinSO. Label = o. Precision = 0.65. Recall = 0.85. F1 = 0.74\n",
      "File BenchmarkUddinSO. Label = n. Precision = 0.43. Recall = 0.33. F1 = 0.38\n",
      "File = BenchmarkUddinSO. F1 Macro = 0.49. Micro = 0.60\n",
      "-------------------------------\n",
      "File DatasetLinAppReviews. Label = p. Precision = 0.83. Recall = 0.89. F1 = 0.86\n",
      "File DatasetLinAppReviews. Label = o. Precision = 0.08. Recall = 0.04. F1 = 0.05\n",
      "File DatasetLinAppReviews. Label = n. Precision = 0.79. Recall = 0.78. F1 = 0.79\n",
      "File = DatasetLinAppReviews. F1 Macro = 0.57. Micro = 0.79\n",
      "-------------------------------\n",
      "File DatasetLinSO. Label = p. Precision = 0.50. Recall = 0.39. F1 = 0.44\n",
      "File DatasetLinSO. Label = o. Precision = 0.86. Recall = 0.94. F1 = 0.90\n",
      "File DatasetLinSO. Label = n. Precision = 0.64. Recall = 0.38. F1 = 0.47\n",
      "File = DatasetLinSO. F1 Macro = 0.61. Micro = 0.82\n",
      "-------------------------------\n",
      "File DatasetSenti4SDSO. Label = p. Precision = 0.90. Recall = 0.91. F1 = 0.90\n",
      "File DatasetSenti4SDSO. Label = o. Precision = 0.82. Recall = 0.80. F1 = 0.81\n",
      "File DatasetSenti4SDSO. Label = n. Precision = 0.78. Recall = 0.79. F1 = 0.78\n",
      "File = DatasetSenti4SDSO. F1 Macro = 0.83. Micro = 0.83\n",
      "-------------------------------\n",
      "File OrtuJIRA. Label = p. Precision = 0.79. Recall = 0.77. F1 = 0.78\n",
      "File OrtuJIRA. Label = o. Precision = 0.87. Recall = 0.92. F1 = 0.89\n",
      "File OrtuJIRA. Label = n. Precision = 0.83. Recall = 0.66. F1 = 0.73\n",
      "File = OrtuJIRA. F1 Macro = 0.80. Micro = 0.85\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "algo = \"GBTSentiCR\"\n",
    "learnerCol = \"DSOSE\"\n",
    "dsose = dso.DSOSE(rootdir)\n",
    "dsose.computePerformancOfLearner(algo, learnerCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of imblearn.over_sampling failed: Traceback (most recent call last):\n",
      "  File \"C:\\anaconda3\\envs\\opinion2\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "ImportError: cannot import name BaseSampler\n",
      "]\n",
      "[autoreload of imblearn failed: Traceback (most recent call last):\n",
      "  File \"C:\\anaconda3\\envs\\opinion2\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "ImportError: cannot import name FunctionSampler\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File DatasetLinJIRA. Label = p. Precision = 0.97. Recall = 0.84. F1 = 0.90\n",
      "File DatasetLinJIRA. Label = n. Precision = 0.93. Recall = 0.99. F1 = 0.96\n",
      "File = DatasetLinJIRA. F1 Macro = 0.93. Micro = 0.94\n",
      "-------------------------------\n",
      "File BenchmarkUddinSO. Label = p. Precision = 0.47. Recall = 0.15. F1 = 0.23\n",
      "File BenchmarkUddinSO. Label = o. Precision = 0.62. Recall = 0.91. F1 = 0.74\n",
      "File BenchmarkUddinSO. Label = n. Precision = 0.42. Recall = 0.17. F1 = 0.24\n",
      "File = BenchmarkUddinSO. F1 Macro = 0.45. Micro = 0.59\n",
      "-------------------------------\n",
      "File DatasetLinAppReviews. Label = p. Precision = 0.80. Recall = 0.91. F1 = 0.85\n",
      "File DatasetLinAppReviews. Label = o. Precision = 0.07. Recall = 0.04. F1 = 0.05\n",
      "File DatasetLinAppReviews. Label = n. Precision = 0.82. Recall = 0.73. F1 = 0.77\n",
      "File = DatasetLinAppReviews. F1 Macro = 0.56. Micro = 0.78\n",
      "-------------------------------\n",
      "File DatasetLinSO. Label = p. Precision = 0.53. Recall = 0.31. F1 = 0.39\n",
      "File DatasetLinSO. Label = o. Precision = 0.85. Recall = 0.95. F1 = 0.90\n",
      "File DatasetLinSO. Label = n. Precision = 0.61. Recall = 0.35. F1 = 0.45\n",
      "File = DatasetLinSO. F1 Macro = 0.59. Micro = 0.82\n",
      "-------------------------------\n",
      "File DatasetSenti4SDSO. Label = p. Precision = 0.87. Recall = 0.91. F1 = 0.89\n",
      "File DatasetSenti4SDSO. Label = o. Precision = 0.78. Recall = 0.84. F1 = 0.81\n",
      "File DatasetSenti4SDSO. Label = n. Precision = 0.83. Recall = 0.69. F1 = 0.76\n",
      "File = DatasetSenti4SDSO. F1 Macro = 0.82. Micro = 0.83\n",
      "-------------------------------\n",
      "File OrtuJIRA. Label = p. Precision = 0.79. Recall = 0.77. F1 = 0.78\n",
      "File OrtuJIRA. Label = o. Precision = 0.87. Recall = 0.92. F1 = 0.90\n",
      "File OrtuJIRA. Label = n. Precision = 0.85. Recall = 0.64. F1 = 0.73\n",
      "File = OrtuJIRA. F1 Macro = 0.81. Micro = 0.85\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "algo = \"GBTSentiCR\"\n",
    "learnerCol = \"SentiCR\"\n",
    "dsose = dso.DSOSE(rootdir)\n",
    "dsose.computePerformancOfLearner(algo, learnerCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label = p. Precision = 0.803. Recall = 0.667. F1 = 0.728\n",
      "Label = o. Precision = 0.767. Recall = 0.903. F1 = 0.830\n",
      "Label = n. Precision = 0.804. Recall = 0.599. F1 = 0.687\n",
      "F1 Macro = 0.756. Micro = 0.780\n",
      "Macro Precision = 0.791. Recall = 0.723\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "algo = \"GBTSentiCR\"\n",
    "learnerCol = \"SentiCR\"\n",
    "dsose = dso.DSOSE(rootdir)\n",
    "dsose.computePerformanceOverallOfLearner(algo, learnerCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
